# Migrating from v0.1.x to v0.2.0

**Date**: October 28, 2025
**Status**: Official Migration Guide for CascadeFlow v0.2.0

---

## ðŸŽ¯ TL;DR: Zero Breaking Changes!

**Your v0.1.x code will run unchanged in v0.2.0.** But you'll want to upgrade to use the new features!

**What you get:**
- âœ… 95% less code (20+ lines â†’ 1 line)
- âœ… Automatic provider detection (zero config)
- âœ… 85-99% cost savings validated
- âœ… 1.8x faster cache performance
- âœ… Quality scores 0.7+ across all presets

**Migration time**: <10 minutes for most projects

---

## ðŸš€ Quick Start with v0.2.0

### Before (v0.1.x) - Manual Configuration:

```python
from cascadeflow import CascadeAgent, ModelConfig

# Manual model configuration (20+ lines)
models = [
    ModelConfig(
        name="llama-3.1-8b-instant",
        provider="groq",
        cost=0.00005,
        quality_tier=1
    ),
    ModelConfig(
        name="gpt-4o-mini",
        provider="openai",
        cost=0.00015,
        quality_tier=2
    ),
    ModelConfig(
        name="gpt-4o",
        provider="openai",
        cost=0.005,
        quality_tier=3
    ),
]

agent = CascadeAgent(
    models=models,
    validation_threshold=0.7,
    max_attempts=3
)

result = await agent.run("What is 2+2?")
```

### After (v0.2.0) - Presets 2.0:

```python
from cascadeflow import get_balanced_agent

# One line to create an agent!
agent = get_balanced_agent()

result = await agent.run("What is 2+2?")
```

**Result**: 95% code reduction, automatic provider detection, production-ready defaults.

---

## ðŸ“‹ Migration Checklist

### Step 1: Update CascadeFlow (30 seconds)

```bash
pip install --upgrade cascadeflow
```

### Step 2: Choose Your Migration Path

#### Option A: Keep v0.1.x Code (Zero Changes Required) âœ…

Your existing code will continue to work with deprecation warnings:

```python
# v0.1.x code - still works!
from cascadeflow import CascadeAgent, ModelConfig

models = [ModelConfig(name="gpt-4o-mini", provider="openai", cost=0.00015)]
agent = CascadeAgent(models=models)

# You'll see helpful deprecation warnings:
# DeprecationWarning: Manual model configuration is deprecated.
# Consider using Presets 2.0 for simpler setup: get_balanced_agent()
```

#### Option B: Migrate to Presets 2.0 (Recommended) â­

Replace manual configuration with one-line presets:

```python
# v0.2.0 - Presets 2.0
from cascadeflow import get_balanced_agent

agent = get_balanced_agent()
```

### Step 3: Update Environment Variables

Presets 2.0 automatically detect providers from environment variables:

```bash
# .env file
OPENAI_API_KEY=sk-...
GROQ_API_KEY=gsk_...
ANTHROPIC_API_KEY=sk-ant-...
TOGETHER_API_KEY=...
```

No code changes needed - detection is automatic!

### Step 4: Test Your Migration

Run your existing tests - everything should pass:

```bash
pytest tests/
```

---

## ðŸŽ¨ Choosing the Right Preset

v0.2.0 includes 5 production-ready presets optimized for different use cases:

### 1. `get_cost_optimized_agent()` - Minimize Costs

**Best for**: High-volume applications, budget-conscious production

```python
from cascadeflow import get_cost_optimized_agent

agent = get_cost_optimized_agent()
```

**Performance**:
- Cost savings: 85-95% vs GPT-4
- Quality: 0.70-0.85
- Latency: 1-2 seconds
- Models: Groq Llama â†’ GPT-4o-mini â†’ GPT-4o

**Use cases**:
- Chatbots with high message volume
- Content moderation at scale
- Internal tools and automation

---

### 2. `get_balanced_agent()` - Best Overall (Default) â­

**Best for**: Most production applications

```python
from cascadeflow import get_balanced_agent

agent = get_balanced_agent()  # Recommended default
```

**Performance**:
- Cost savings: 80-90% vs GPT-4
- Quality: 0.75-0.90
- Latency: 1-3 seconds
- Models: GPT-4o-mini â†’ GPT-4o â†’ Claude 3.5

**Use cases**:
- General-purpose applications
- Customer support
- Content generation
- Most production workloads

---

### 3. `get_speed_optimized_agent()` - Minimize Latency

**Best for**: Real-time applications, user-facing interfaces

```python
from cascadeflow import get_speed_optimized_agent

agent = get_speed_optimized_agent()
```

**Performance**:
- Cost savings: 70-85% vs GPT-4
- Quality: 0.70-0.85
- Latency: <800ms (validated)
- Models: Fast providers prioritized

**Use cases**:
- Real-time chat interfaces
- Interactive assistants
- Low-latency requirements

---

### 4. `get_quality_optimized_agent()` - Maximize Quality

**Best for**: High-stakes applications, complex reasoning

```python
from cascadeflow import get_quality_optimized_agent

agent = get_quality_optimized_agent()
```

**Performance**:
- Cost savings: 60-80% vs GPT-4
- Quality: 0.90-0.98 (highest)
- Latency: 2-5 seconds
- Models: GPT-4o â†’ Claude 3.5 Sonnet â†’ o1

**Use cases**:
- Medical/legal applications
- Complex analysis
- High-stakes decisions
- Research and education

---

### 5. `get_development_agent()` - Fast Iteration

**Best for**: Development, testing, debugging

```python
from cascadeflow import get_development_agent

agent = get_development_agent()
```

**Performance**:
- Cost savings: 90-99% vs GPT-4
- Quality: 0.65-0.80
- Latency: <1 second
- Logging: Verbose mode enabled

**Use cases**:
- Local development
- Unit testing
- Prototyping
- Debugging

---

### 6. `auto_agent(preset: str)` - Dynamic Selection

**Best for**: Runtime preset selection, multi-tenant apps

```python
from cascadeflow import auto_agent

# Select preset at runtime
preset = "balanced"  # or "cost_optimized", "speed_optimized", etc.
agent = auto_agent(preset)
```

**Use cases**:
- Multi-tenant applications with different tiers
- User-configurable performance profiles
- A/B testing different configurations

---

## ðŸ“Š Detailed Migration Examples

### Example 1: Basic Application

#### Before (v0.1.x):

```python
import asyncio
from cascadeflow import CascadeAgent, ModelConfig

async def main():
    models = [
        ModelConfig(name="llama-3.1-8b-instant", provider="groq", cost=0.00005),
        ModelConfig(name="gpt-4o-mini", provider="openai", cost=0.00015),
    ]

    agent = CascadeAgent(
        models=models,
        validation_threshold=0.7,
        max_attempts=2
    )

    result = await agent.run("Explain quantum computing")
    print(result.content)

if __name__ == "__main__":
    asyncio.run(main())
```

#### After (v0.2.0):

```python
import asyncio
from cascadeflow import get_balanced_agent

async def main():
    agent = get_balanced_agent()

    result = await agent.run("Explain quantum computing")
    print(result.content)

if __name__ == "__main__":
    asyncio.run(main())
```

**Changes**: 14 lines â†’ 8 lines (43% reduction)

---

### Example 2: Cost-Sensitive Application

#### Before (v0.1.x):

```python
from cascadeflow import CascadeAgent, ModelConfig

# Manually configure for cost optimization
models = [
    ModelConfig(
        name="llama-3.1-8b-instant",
        provider="groq",
        cost=0.00005,
        quality_tier=1,
        max_attempts=2
    ),
    ModelConfig(
        name="llama-3.1-70b-versatile",
        provider="groq",
        cost=0.00059,
        quality_tier=2,
        max_attempts=1
    ),
    ModelConfig(
        name="gpt-4o-mini",
        provider="openai",
        cost=0.00015,
        quality_tier=3,
        max_attempts=1
    ),
]

agent = CascadeAgent(
    models=models,
    validation_threshold=0.65,
    max_attempts=3,
    cache_enabled=True
)
```

#### After (v0.2.0):

```python
from cascadeflow import get_cost_optimized_agent

agent = get_cost_optimized_agent()
```

**Changes**: 32 lines â†’ 3 lines (91% reduction)

---

### Example 3: Multi-Tenant Application with Tier Routing

#### Before (v0.1.x):

```python
from cascadeflow import CascadeAgent, ModelConfig

def create_agent_for_tier(tier: str):
    if tier == "free":
        models = [
            ModelConfig(name="llama-3.1-8b-instant", provider="groq", cost=0.00005),
        ]
    elif tier == "pro":
        models = [
            ModelConfig(name="gpt-4o-mini", provider="openai", cost=0.00015),
            ModelConfig(name="gpt-4o", provider="openai", cost=0.005),
        ]
    else:
        raise ValueError(f"Unknown tier: {tier}")

    return CascadeAgent(models=models, validation_threshold=0.7)

# Usage
free_agent = create_agent_for_tier("free")
pro_agent = create_agent_for_tier("pro")
```

#### After (v0.2.0):

```python
from cascadeflow import auto_agent

# Simple tier mapping
def create_agent_for_tier(tier: str):
    tier_map = {
        "free": "cost_optimized",
        "pro": "balanced",
        "enterprise": "quality_optimized"
    }
    return auto_agent(tier_map[tier])

# Usage
free_agent = create_agent_for_tier("free")
pro_agent = create_agent_for_tier("pro")
```

**Changes**: More maintainable, production-ready presets

---

### Example 4: Streaming Responses

#### Before (v0.1.x):

```python
from cascadeflow import CascadeAgent, ModelConfig

models = [ModelConfig(name="gpt-4o-mini", provider="openai", cost=0.00015)]
agent = CascadeAgent(models=models)

async for chunk in agent.run_stream("Write a story"):
    print(chunk.content, end="", flush=True)
```

#### After (v0.2.0):

```python
from cascadeflow import get_balanced_agent

agent = get_balanced_agent()

async for chunk in agent.run_stream("Write a story"):
    print(chunk.content, end="", flush=True)
```

**Changes**: No changes to streaming API - works exactly the same!

---

## ðŸ”§ Advanced Migration Scenarios

### Scenario 1: Custom Model Configuration

If you need custom models not in presets:

```python
from cascadeflow import CascadeAgent, ModelConfig

# You can still use manual configuration
models = [
    ModelConfig(name="my-custom-model", provider="openai", cost=0.001),
]

agent = CascadeAgent(models=models)
```

**Note**: Manual configuration is still fully supported for advanced use cases.

---

### Scenario 2: Customizing Presets

If you need preset-like behavior with custom settings:

```python
from cascadeflow.utils.presets import get_balanced_agent

# Start with preset
agent = get_balanced_agent()

# Override specific settings if needed
agent.validation_threshold = 0.8
agent.max_attempts = 5
```

---

### Scenario 3: Domain-Specific Strategies

v0.2.0 includes domain-specific cascade strategies:

```python
from cascadeflow import get_balanced_agent
from cascadeflow.strategies import CODE_CASCADE, MEDICAL_CASCADE

# For code-related queries
code_agent = get_balanced_agent()
# Future: code_agent.set_strategy(CODE_CASCADE)

# For medical/high-stakes queries
medical_agent = get_quality_optimized_agent()
# Future: medical_agent.set_strategy(MEDICAL_CASCADE)
```

**Note**: Domain strategies are implemented but not yet exposed in public API. Coming in v0.2.1.

---

## ðŸš¨ Deprecated Features & Warnings

### Deprecated in v0.2.0:

#### 1. Manual Model Configuration (Soft Deprecation)

**Status**: Still works, shows deprecation warning

```python
# Deprecated (but still works)
from cascadeflow import CascadeAgent, ModelConfig

models = [ModelConfig(name="gpt-4o-mini", provider="openai", cost=0.00015)]
agent = CascadeAgent(models=models)

# Warning: Manual model configuration is deprecated.
# Consider using Presets 2.0: get_balanced_agent()
```

**Recommendation**: Migrate to Presets 2.0

---

#### 2. Direct Provider Configuration

**Status**: Still works, automatic detection preferred

```python
# Old way (still works)
from cascadeflow.providers import OpenAIProvider

provider = OpenAIProvider(api_key="sk-...")

# New way (automatic)
# Just set environment variable:
# OPENAI_API_KEY=sk-...
# Presets detect automatically!
```

**Recommendation**: Use environment variables for automatic detection

---

### Not Deprecated:

- âœ… `CascadeAgent` class (core API unchanged)
- âœ… `ModelConfig` dataclass (still valid)
- âœ… Streaming API (`run_stream()`)
- âœ… Tool execution
- âœ… Custom validation
- âœ… All provider integrations

---

## ðŸ“ˆ Performance Comparison

### Real-World Benchmark Results (v0.2.0):

| Metric | v0.1.x Manual Config | v0.2.0 Balanced Preset | Improvement |
|--------|---------------------|------------------------|-------------|
| Setup code | 20-30 lines | 1 line | 95% reduction |
| Cost vs GPT-4 | 60-70% savings | 80-90% savings | +20-30% better |
| Cache speedup | 1.2x | 1.8x | 50% faster |
| Quality score | 0.70-0.80 | 0.75-0.90 | Higher & consistent |
| Time to first result | ~10 min setup | <2 min setup | 80% faster |

### Cost Savings by Preset (vs GPT-4 baseline):

| Preset | Cost Savings | Quality | Best For |
|--------|--------------|---------|----------|
| Development | 90-99% | 0.65-0.80 | Testing, prototyping |
| Cost Optimized | 85-95% | 0.70-0.85 | High volume |
| Balanced â­ | 80-90% | 0.75-0.90 | General production |
| Speed Optimized | 70-85% | 0.70-0.85 | Real-time apps |
| Quality Optimized | 60-80% | 0.90-0.98 | High-stakes |

---

## ðŸ› Troubleshooting

### Issue 1: "No providers detected" error

**Problem**: Presets can't find any API keys

**Solution**:
```bash
# Check environment variables are set
echo $OPENAI_API_KEY
echo $GROQ_API_KEY

# Or create .env file:
cat > .env << EOF
OPENAI_API_KEY=sk-...
GROQ_API_KEY=gsk_...
EOF
```

---

### Issue 2: Deprecation warnings

**Problem**: Seeing warnings about deprecated features

**Solution**: These are informational only - your code still works!

```python
# To suppress warnings (not recommended):
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Better: Migrate to Presets 2.0 to remove warnings
from cascadeflow import get_balanced_agent
agent = get_balanced_agent()
```

---

### Issue 3: Custom models not working with presets

**Problem**: Need to use custom model configuration

**Solution**: You can still use manual configuration:

```python
from cascadeflow import CascadeAgent, ModelConfig

# Manual config still fully supported
models = [
    ModelConfig(name="my-custom-model", provider="openai", cost=0.001),
]
agent = CascadeAgent(models=models)
```

---

### Issue 4: Import errors after upgrade

**Problem**: `ImportError: cannot import name 'get_balanced_agent'`

**Solution**: Ensure you've upgraded to v0.2.0:

```bash
pip install --upgrade cascadeflow
python -c "import cascadeflow; print(cascadeflow.__version__)"
# Should show: 0.2.0
```

---

### Issue 5: Tests failing after migration

**Problem**: Existing tests break with presets

**Solution**: Presets use environment variables - mock or set them in tests:

```python
import os
import pytest

@pytest.fixture
def mock_env():
    os.environ["OPENAI_API_KEY"] = "test-key"
    os.environ["GROQ_API_KEY"] = "test-key"
    yield
    del os.environ["OPENAI_API_KEY"]
    del os.environ["GROQ_API_KEY"]

def test_my_agent(mock_env):
    from cascadeflow import get_balanced_agent
    agent = get_balanced_agent()
    # Your test...
```

---

## âœ… Migration Validation Checklist

After migrating, verify everything works:

### Functionality:
- [ ] Agent initialization works
- [ ] Basic queries return results
- [ ] Streaming works (if used)
- [ ] Tool execution works (if used)
- [ ] Error handling works as expected

### Performance:
- [ ] Response times are acceptable
- [ ] Quality meets requirements
- [ ] Costs are within budget
- [ ] Cache is working (faster repeated queries)

### Testing:
- [ ] All existing tests pass
- [ ] Integration tests pass
- [ ] End-to-end tests pass
- [ ] No unexpected warnings (except deprecation)

### Production:
- [ ] Staging environment tested
- [ ] Monitoring/logging works
- [ ] Error tracking configured
- [ ] Rollback plan ready

---

## ðŸ“š Additional Resources

### Documentation:
- [Presets 2.0 Guide](./guides/presets.md) - Complete preset reference
- [Quickstart Guide](./guides/quickstart.md) - Updated for v0.2.0
- [Production Guide](./guides/production.md) - Production best practices
- [API Reference](./api/python/agent.md) - Full API documentation

### Examples:
- [examples/quickstart_v2.py](../examples/quickstart_v2.py) - Simple one-line example
- [examples/migration_example.py](../examples/migration_example.py) - Side-by-side comparison
- [examples/preset_comparison.py](../examples/preset_comparison.py) - Compare all presets
- [examples/presets_v2_usage.py](../examples/presets_v2_usage.py) - Advanced preset usage

### Benchmarks:
- [benchmark_results/V0.2.0_FINAL_VALIDATION_REPORT.md](../benchmark_results/V0.2.0_FINAL_VALIDATION_REPORT.md) - Validation results
- [benchmarks/v0_2_0_realworld_benchmark.py](../benchmarks/v0_2_0_realworld_benchmark.py) - Real-world test suite

---

## ðŸ†˜ Getting Help

### Common Questions:

**Q: Do I need to migrate right away?**
A: No! Your v0.1.x code will continue working. Migrate when convenient.

**Q: Can I mix v0.1.x and v0.2.0 APIs?**
A: Yes! You can use manual configuration and presets in the same application.

**Q: What if I need features not in presets?**
A: Manual configuration is still fully supported for advanced use cases.

**Q: Are there breaking changes?**
A: No! v0.2.0 is 100% backwards compatible with v0.1.x.

**Q: How long until v0.1.x is deprecated?**
A: No timeline yet. Manual configuration will be supported for foreseeable future.

### Support Channels:

- **GitHub Issues**: [github.com/lemony-ai/CascadeFlow/issues](https://github.com/lemony-ai/CascadeFlow/issues)
- **Documentation**: [Full docs available in /docs](../docs/)
- **Examples**: [See /examples directory](../examples/)

---

## ðŸŽ‰ What's Next?

### Coming in v0.2.1 (WEEK 4-6):

- ðŸ”§ Enhanced error messages with hints
- ðŸ“Š Built-in cost tracking and analytics
- ðŸŽ¯ Domain-specific strategies (public API)
- ðŸ§ª Test utilities and mock providers
- ðŸ“ˆ Performance monitoring helpers

### Coming in v0.2.2 (WEEK 7-9):

- ðŸŒ Advanced caching strategies
- ðŸ” Enhanced security features
- ðŸ“ Structured output validation
- ðŸš€ Performance optimizations
- ðŸŽ¨ Custom preset builder

### Coming in v0.3.0:

- ðŸŸ¦ Full TypeScript/JavaScript support
- ðŸŒ Multi-language SDKs
- ðŸ”Œ Plugin system
- ðŸ“± Mobile SDK support

---

**Migration Status**: âœ… COMPLETE
**Your v0.1.x code**: âœ… STILL WORKS
**Time to migrate**: â±ï¸ <10 minutes
**Benefits**: ðŸš€ 95% code reduction, automatic detection, better performance

**Welcome to CascadeFlow v0.2.0!** ðŸŽ‰

---

**Last Updated**: October 28, 2025
**Version**: v0.2.0
**Status**: Official Release
