# CascadeFlow v0.2.2 Implementation Plan
## ML Parity + Reasoning Models Support

**Version:** 0.2.2
**Branch:** `feature/cost-control-quality-v2` (existing)
**Target Release:** Q2 2025
**Estimated Effort:** 20-24 hours
**Status:** Ready for implementation

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Research Findings](#research-findings)
3. [LiteLLM Integration Strategy](#litellm-integration-strategy)
4. [Branch Strategy](#branch-strategy)
5. [Implementation Milestones](#implementation-milestones)
6. [Testing Strategy](#testing-strategy)
7. [Documentation Updates](#documentation-updates)
8. [Success Criteria](#success-criteria)

---

## Executive Summary

This plan adds TWO major features to CascadeFlow v0.2.2, building on the existing `feature/cost-control-quality-v2` branch:

### Feature 1: TypeScript ML Parity (12-16 hours)
- Brings TypeScript to feature parity with Python's ML capabilities
- Uses Transformers.js with `Xenova/bge-small-en-v1.5`
- 84-87% domain detection confidence (matches Python)
- Works in Node.js, browser, and edge environments
- **LiteLLM integration:** Leverages LiteLLM's model metadata when available

### Feature 2: Reasoning Models Support (8-10 hours)
- Full support for OpenAI o1/o3-mini reasoning models
- Full support for Anthropic extended thinking mode
- Handles model-specific limitations (streaming, tools, system messages)
- Special cascade strategies optimized for reasoning tasks
- **LiteLLM integration:** Uses LiteLLM pricing for reasoning tokens

### Key Requirements
- ✅ Build on existing `feature/cost-control-quality-v2` branch
- ✅ Leverage LiteLLM when available (optional dependency)
- ✅ Graceful fallback when LiteLLM not installed
- ✅ Fully backward compatible
- ✅ No AI assistant mentions in code or commit messages

---

## Research Findings

### 1. OpenAI Reasoning Models (o1, o1-mini, o3-mini)

**Key Parameters:**
- `max_completion_tokens`: Replaces `max_tokens`, controls total output (200K context, 100K max output for o1)
- `reasoning_effort`: "low", "medium", "high" (o1/o3-mini only, not o1-mini)
  - Higher effort = more reasoning tokens, longer processing, better quality
  - Can adjust between speed and thoroughness

**Limitations by Model:**

| Feature | o1-preview/o1-mini (Original) | o1 (2024-12-17) | o3-mini | o4-mini |
|---------|------------------------------|-----------------|---------|---------|
| Streaming | ✅ Added | ❌ Not supported | ✅ Supported | ✅ Supported |
| Function Calling | ❌ Not supported | ❌ Not supported | ✅ Supported | ✅ Supported |
| System Messages | ❌ Not supported | ❌ Not supported | ❌ Not supported | ✅ Supported |
| Vision | ❌ Not supported | ✅ Supported | ❌ Not supported | ✅ Supported |
| Structured Outputs | ❌ Not supported | ❌ Not supported | ✅ Supported | ✅ Supported |

**Reasoning Tokens:**
- Hidden tokens not returned in response content
- Used by model to "think" before answering
- Part of `completion_tokens_details.reasoning_tokens`
- Billed but not visible to user

### 2. Anthropic Extended Thinking

**Key Parameters:**
- `thinking`: Dictionary with `type: "enabled"` and `budget_tokens`
- `budget_tokens`: Minimum 1024, max < max_tokens
- `beta: "interleaved-thinking-2025-05-14"`: Header for interleaved thinking

**Supported Models:**
- `claude-sonnet-4-5` (Claude 3.7 Sonnet)
- `claude-opus-4` (Claude 4 Opus)

**Response Format:**
```python
{
  "content": [
    {"type": "thinking", "thinking": "Internal reasoning...", "signature": "..."},
    {"type": "text", "text": "Final answer..."}
  ]
}
```

### 3. Transformers.js for TypeScript ML

**Library:** `@xenova/transformers` v2.17.0+
**Model:** `Xenova/bge-small-en-v1.5` (ONNX-converted BAAI/bge-small-en-v1.5)

**Specifications:**
- Model size: ~40MB
- Embedding dimensions: 384
- Inference time: ~20-50ms per embedding
- Works: Node.js, browser, edge functions
- No server required (offline after first download)

---

## LiteLLM Integration Strategy

### Overview

CascadeFlow already has LiteLLM integration in `cascadeflow/integrations/litellm.py`. This plan extends that integration to support:
1. Reasoning model pricing (reasoning tokens)
2. ML model metadata
3. Provider-specific parameter handling
4. Graceful fallback when LiteLLM not installed

### Current LiteLLM Integration

**Existing Features:**
- `LiteLLMCostProvider`: Accurate cost calculations
- `LITELLM_AVAILABLE`: Flag for availability check
- `BUDGET_MANAGER_AVAILABLE`: Advanced budget features
- Provider validation with `SUPPORTED_PROVIDERS`

**File:** `cascadeflow/integrations/litellm.py`

### Extended LiteLLM Integration

#### 1. Reasoning Token Cost Calculation

**With LiteLLM:**
```python
# cascadeflow/integrations/litellm.py

def calculate_cost_with_reasoning(
    model: str,
    input_tokens: int,
    output_tokens: int,
    reasoning_tokens: Optional[int] = None
) -> float:
    """
    Calculate cost including reasoning tokens using LiteLLM.

    LiteLLM automatically handles reasoning token pricing for o1/o3 models.
    """
    if not LITELLM_AVAILABLE:
        return fallback_cost_estimate(model, input_tokens, output_tokens, reasoning_tokens)

    try:
        # LiteLLM's completion_cost handles reasoning tokens automatically
        cost = completion_cost(
            model=model,
            prompt_tokens=input_tokens,
            completion_tokens=output_tokens
        )

        # Add reasoning tokens if applicable (o1/o3 models)
        if reasoning_tokens and reasoning_tokens > 0:
            # LiteLLM includes reasoning tokens in completion_tokens for o1/o3
            # Already accounted for in cost calculation
            pass

        return cost
    except Exception as e:
        logger.warning(f"LiteLLM cost calculation failed: {e}, using fallback")
        return fallback_cost_estimate(model, input_tokens, output_tokens, reasoning_tokens)


def fallback_cost_estimate(
    model: str,
    input_tokens: int,
    output_tokens: int,
    reasoning_tokens: Optional[int] = None
) -> float:
    """Fallback cost estimation when LiteLLM unavailable."""
    # Use built-in pricing table as fallback
    FALLBACK_PRICING = {
        "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
        "gpt-4o": {"input": 0.0025, "output": 0.01},
        "o1-mini": {"input": 0.003, "output": 0.012},
        "o3-mini": {"input": 0.001, "output": 0.005},
        "claude-sonnet-4-5": {"input": 0.003, "output": 0.015},
        # ... add more models
    }

    pricing = FALLBACK_PRICING.get(model, {"input": 0.001, "output": 0.003})

    cost = (input_tokens / 1000 * pricing["input"]) + \
           (output_tokens / 1000 * pricing["output"])

    # Reasoning tokens typically priced same as output tokens
    if reasoning_tokens:
        cost += (reasoning_tokens / 1000 * pricing["output"])

    return cost
```

#### 2. Model Capabilities Detection

**With LiteLLM:**
```python
# cascadeflow/integrations/litellm.py

def get_model_capabilities(model: str) -> dict:
    """
    Get model capabilities using LiteLLM metadata.

    Returns capabilities like streaming, tools, system_messages, etc.
    Falls back to built-in detection if LiteLLM unavailable.
    """
    if LITELLM_AVAILABLE:
        try:
            # LiteLLM provides model info including capabilities
            model_info = litellm.model_cost.get(model, {})

            return {
                "supports_streaming": model_info.get("supports_streaming", True),
                "supports_tools": model_info.get("supports_function_calling", False),
                "supports_system_messages": model_info.get("supports_system_message", True),
                "supports_vision": model_info.get("supports_vision", False),
                "is_reasoning_model": "o1" in model or "o3" in model or "gpt-5" in model,
                "max_tokens": model_info.get("max_tokens", 4096),
                "max_input_tokens": model_info.get("max_input_tokens", 128000),
            }
        except Exception as e:
            logger.warning(f"LiteLLM model info failed: {e}, using fallback")

    # Fallback to manual detection
    return fallback_model_capabilities(model)


def fallback_model_capabilities(model: str) -> dict:
    """Fallback capabilities detection when LiteLLM unavailable."""
    model_lower = model.lower()

    # Reasoning models
    is_reasoning = any(rm in model_lower for rm in ["o1", "o3", "gpt-5"])

    # o1-2024-12-17 doesn't support streaming
    supports_streaming = "o1-2024-12-17" not in model_lower

    # o3-mini, o4-mini support tools
    supports_tools = any(m in model_lower for m in ["o3-mini", "o4-mini", "gpt-4"])

    return {
        "supports_streaming": supports_streaming,
        "supports_tools": supports_tools,
        "supports_system_messages": not is_reasoning or "o4-mini" in model_lower,
        "supports_vision": "vision" in model_lower or "gpt-4o" in model_lower,
        "is_reasoning_model": is_reasoning,
        "max_tokens": 4096,
        "max_input_tokens": 128000,
    }
```

#### 3. Provider Parameter Mapping

**With LiteLLM:**
```python
# cascadeflow/integrations/litellm.py

def map_provider_parameters(provider: str, model: str, **kwargs) -> dict:
    """
    Map parameters to provider-specific format using LiteLLM.

    LiteLLM handles parameter mapping across providers automatically.
    """
    if LITELLM_AVAILABLE:
        try:
            # LiteLLM's completion() handles parameter mapping
            # Just need to pass reasoning-specific params
            mapped = kwargs.copy()

            # Handle reasoning parameters
            if "reasoning" in kwargs:
                reasoning_config = kwargs["reasoning"]

                # OpenAI reasoning
                if reasoning_config.get("effort"):
                    mapped["reasoning_effort"] = reasoning_config["effort"]
                if reasoning_config.get("max_completion_tokens"):
                    mapped["max_completion_tokens"] = reasoning_config["max_completion_tokens"]

                # Anthropic thinking
                if reasoning_config.get("thinking"):
                    mapped["thinking"] = reasoning_config["thinking"]

                del mapped["reasoning"]

            return mapped
        except Exception as e:
            logger.warning(f"LiteLLM parameter mapping failed: {e}, using fallback")

    # Fallback to manual mapping
    return fallback_parameter_mapping(provider, model, **kwargs)
```

### Integration Points

**1. CascadeAgent Initialization**
```python
# cascadeflow/agent.py

class CascadeAgent:
    def __init__(self, ...):
        # Try to use LiteLLM cost provider
        if LITELLM_AVAILABLE:
            from cascadeflow.integrations.litellm import LiteLLMCostProvider
            self.cost_provider = LiteLLMCostProvider()
        else:
            self.cost_provider = FallbackCostProvider()
```

**2. Provider Initialization**
```python
# cascadeflow/providers/openai.py

class OpenAIProvider:
    def __init__(self, ...):
        # Get model capabilities from LiteLLM if available
        if LITELLM_AVAILABLE:
            from cascadeflow.integrations.litellm import get_model_capabilities
            self.get_capabilities = get_model_capabilities
        else:
            self.get_capabilities = self._fallback_capabilities
```

**3. Cost Calculation**
```python
# cascadeflow/providers/base.py

class BaseProvider:
    def calculate_cost(self, model, input_tokens, output_tokens, reasoning_tokens=None):
        if LITELLM_AVAILABLE:
            from cascadeflow.integrations.litellm import calculate_cost_with_reasoning
            return calculate_cost_with_reasoning(
                model, input_tokens, output_tokens, reasoning_tokens
            )
        else:
            return self._fallback_cost(model, input_tokens, output_tokens, reasoning_tokens)
```

### Benefits of LiteLLM Integration

**When LiteLLM Available:**
- ✅ Always up-to-date pricing (LiteLLM team maintains it)
- ✅ Covers 100+ models across 10+ providers
- ✅ Accurate reasoning token pricing
- ✅ Automatic parameter mapping
- ✅ Model capability detection
- ✅ Special pricing (batch, cached tokens)

**When LiteLLM Not Available:**
- ✅ Graceful fallback to built-in pricing
- ✅ Manual model detection
- ✅ Manual parameter mapping
- ✅ All features still work (slightly less accurate costs)

---

## Branch Strategy

### Current Branch: `feature/cost-control-quality-v2`

**Recent commits show:**
- v0.2.1 features (rate limiting, guardrails, multi-tenant)
- LiteLLM integration already present
- Comprehensive benchmarking

**Strategy:** Build directly on this branch, no new branch needed.

### Commit Conventions

**NO mentions of AI assistants** - Use standard professional commit messages:

```bash
# Good commits
git commit -m "feat(ml): add TypeScript embedding service with Transformers.js"
git commit -m "feat(reasoning): add OpenAI o1/o3 reasoning model support"
git commit -m "feat(litellm): extend integration for reasoning token pricing"

# Bad commits (avoid these)
git commit -m "feat: implement with Claude assistance"  # ❌
git commit -m "Co-Authored-By: Claude <...>"  # ❌
git commit -m "Generated with Claude Code"  # ❌
```

### Git Workflow

```bash
# Work directly on existing branch
git checkout feature/cost-control-quality-v2
git pull origin feature/cost-control-quality-v2

# Create milestone feature branches as needed
git checkout -b ml-typescript-parity
# ... implement ...
git add .
git commit -m "feat(ml): add TypeScript ML parity with 84-87% confidence"
git push -u origin ml-typescript-parity

# Create PR: ml-typescript-parity → feature/cost-control-quality-v2
gh pr create --base feature/cost-control-quality-v2 --head ml-typescript-parity

# Merge when ready
gh pr merge <PR-NUMBER> --squash

# Continue with next milestone
git checkout feature/cost-control-quality-v2
git pull
git checkout -b reasoning-models-support
# ...
```

---

## Implementation Milestones

### Milestone 1: TypeScript ML Infrastructure (4-5 hours)

**Branch:** `ml-typescript-parity`
**Goal:** Create `@cascadeflow/ml` package with embedding service
**LiteLLM Integration:** Use LiteLLM model metadata if available

#### Tasks

**1.1: Package Setup (1 hour)**
- [ ] Create `packages/ml/` directory structure
- [ ] Create `package.json` with dependencies
- [ ] Add `@xenova/transformers` dependency
- [ ] Create TypeScript config
- [ ] Create package README

**1.2: UnifiedEmbeddingService Implementation (2-3 hours)**
- [ ] Port Python `UnifiedEmbeddingService` to TypeScript
- [ ] Implement lazy initialization
- [ ] Add cosine similarity calculation
- [ ] Add EmbeddingCache
- [ ] Add graceful fallback handling

**1.3: LiteLLM Integration (Optional Enhancement)**
```typescript
// packages/ml/src/embedding.ts

export class UnifiedEmbeddingService {
  // ... existing code ...

  /**
   * Check if LiteLLM model metadata is available
   * (for future integration with LiteLLM TypeScript bindings)
   */
  private async checkLiteLLMAvailability(): Promise<boolean> {
    try {
      // Future: Check for @litellm/client package
      // const litellm = await import('@litellm/client');
      // return litellm.isAvailable();
      return false;  // Not available yet in TypeScript
    } catch {
      return false;
    }
  }
}
```

**1.4: Testing (1 hour)**
- [ ] Unit tests for embedding service
- [ ] Test model loading (Node.js)
- [ ] Test embedding generation (384 dimensions)
- [ ] Test similarity calculation
- [ ] Test caching behavior

**Validation:**
- [ ] Package builds successfully
- [ ] Model loads correctly
- [ ] Embeddings are 384 dimensions
- [ ] All tests pass

**Git Commands:**
```bash
git checkout feature/cost-control-quality-v2
git checkout -b ml-typescript-parity
# ... implement ...
git add packages/ml/
git commit -m "feat(ml): create @cascadeflow/ml package with Transformers.js

- Add UnifiedEmbeddingService with lazy initialization
- Add EmbeddingCache for request-scoped caching
- Add cosine similarity calculation
- Add graceful fallback handling
- Add comprehensive unit tests
- 84-87% confidence on complex domains (matches Python)"
git push -u origin ml-typescript-parity
```

---

### Milestone 2: Core ML Integration (3-4 hours)

**Branch:** `ml-typescript-parity` (continue)
**Goal:** Integrate ML into core package

#### Tasks

**2.1: Add Semantic Validation (1-2 hours)**
- [ ] Update `ValidationMethod` enum with `SEMANTIC`
- [ ] Create `SemanticValidator` class
- [ ] Implement semantic validation with fallback
- [ ] Add tests

**2.2: Add Semantic Domain Detection (2 hours)**
- [ ] Create `SemanticDomainDetector` class
- [ ] Port domain exemplars from Python
- [ ] Integrate with existing domain detection
- [ ] Add tests

**2.3: Update CascadeAgent (1 hour)**
- [ ] Add `enableSemanticDetection` config option
- [ ] Initialize ML components when enabled
- [ ] Add metadata for detection method
- [ ] Add tests

**Validation:**
- [ ] ML detection works (84-87% confidence)
- [ ] Falls back to rule-based when unavailable
- [ ] No breaking changes
- [ ] All tests pass

---

### Milestone 3: Reasoning Models - OpenAI (4-5 hours)

**Branch:** `reasoning-models-openai`
**Goal:** Full OpenAI o1/o3-mini support with LiteLLM integration

#### Tasks

**3.1: Add ReasoningConfig Types (1 hour)**

**Python:**
```python
# cascadeflow/schema/config.py

from enum import Enum
from typing import Optional
from pydantic import BaseModel, Field

class ReasoningEffort(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"

class ReasoningConfig(BaseModel):
    """Configuration for reasoning models."""
    effort: Optional[ReasoningEffort] = None
    max_completion_tokens: Optional[int] = None
```

**TypeScript:**
```typescript
// packages/core/src/types.ts

export enum ReasoningEffort {
  LOW = 'low',
  MEDIUM = 'medium',
  HIGH = 'high',
}

export interface ReasoningConfig {
  effort?: ReasoningEffort;
  maxCompletionTokens?: number;
}
```

**3.2: Update OpenAI Provider (2-3 hours)**

**Python:**
```python
# cascadeflow/providers/openai.py

class OpenAIProvider(BaseProvider):
    def __init__(self, ...):
        super().__init__(...)

        # Use LiteLLM capabilities if available
        if LITELLM_AVAILABLE:
            from cascadeflow.integrations.litellm import get_model_capabilities
            self.get_capabilities = get_model_capabilities
        else:
            self.get_capabilities = self._fallback_capabilities

    def _is_reasoning_model(self, model: str) -> bool:
        """Detect reasoning model using LiteLLM or fallback."""
        capabilities = self.get_capabilities(model)
        return capabilities.get("is_reasoning_model", False)

    async def complete(
        self,
        prompt: str,
        model: str = "gpt-4o-mini",
        reasoning: Optional[ReasoningConfig] = None,
        **kwargs
    ) -> ModelResponse:
        """Complete with reasoning model support."""

        # Get capabilities
        caps = self.get_capabilities(model)
        is_reasoning = caps["is_reasoning_model"]

        request_data = {"model": model, "messages": [...]}

        # Reasoning models use max_completion_tokens
        if is_reasoning and reasoning and reasoning.max_completion_tokens:
            request_data["max_completion_tokens"] = reasoning.max_completion_tokens
        else:
            request_data["max_tokens"] = kwargs.get("max_tokens", 4096)

        # Add reasoning_effort if supported
        if is_reasoning and reasoning and reasoning.effort:
            if "o1-mini" not in model.lower():  # o1-mini doesn't support effort
                request_data["reasoning_effort"] = reasoning.effort.value

        # ... rest of implementation ...
```

**3.3: Update Cost Calculation with LiteLLM (1 hour)**

```python
# cascadeflow/providers/openai.py

def _calculate_cost(self, response: dict) -> float:
    """Calculate cost using LiteLLM if available."""
    usage = response.get("usage", {})
    model = response.get("model", "")

    input_tokens = usage.get("prompt_tokens", 0)
    output_tokens = usage.get("completion_tokens", 0)

    # Get reasoning tokens if present
    reasoning_tokens = usage.get("completion_tokens_details", {}).get("reasoning_tokens", 0)

    # Use LiteLLM for accurate pricing
    if LITELLM_AVAILABLE:
        from cascadeflow.integrations.litellm import calculate_cost_with_reasoning
        return calculate_cost_with_reasoning(
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            reasoning_tokens=reasoning_tokens
        )

    # Fallback to manual calculation
    return self._fallback_cost(model, input_tokens, output_tokens, reasoning_tokens)
```

**3.4: Testing (1 hour)**
- [ ] Test o1/o3-mini detection
- [ ] Test reasoning_effort parameter
- [ ] Test max_completion_tokens parameter
- [ ] Test cost calculation with reasoning tokens
- [ ] Test LiteLLM integration
- [ ] Test fallback when LiteLLM unavailable

**Validation:**
- [ ] Reasoning models detected correctly
- [ ] Parameters applied correctly
- [ ] Cost includes reasoning tokens
- [ ] Works with and without LiteLLM
- [ ] All tests pass

---

### Milestone 4: Reasoning Models - Anthropic (3-4 hours)

**Branch:** `reasoning-models-anthropic`
**Goal:** Anthropic extended thinking support with LiteLLM integration

#### Tasks

**4.1: Add ThinkingConfig (1 hour)**

```python
# cascadeflow/schema/config.py

class ThinkingConfig(BaseModel):
    """Configuration for Anthropic extended thinking."""
    enabled: bool = True
    budget_tokens: int = Field(default=1024, ge=1024)
    interleaved: bool = False

class ReasoningConfig(BaseModel):
    """Combined reasoning config."""
    # OpenAI
    effort: Optional[ReasoningEffort] = None
    max_completion_tokens: Optional[int] = None
    # Anthropic
    thinking: Optional[ThinkingConfig] = None
```

**4.2: Update Anthropic Provider (2 hours)**

```python
# cascadeflow/providers/anthropic.py

class AnthropicProvider(BaseProvider):
    def _supports_thinking(self, model: str) -> bool:
        """Check extended thinking support using LiteLLM."""
        if LITELLM_AVAILABLE:
            from cascadeflow.integrations.litellm import get_model_capabilities
            caps = get_model_capabilities(model)
            return caps.get("supports_extended_thinking", False)

        # Fallback
        return any(m in model.lower() for m in ["claude-sonnet-4-5", "claude-opus-4"])

    async def complete(
        self,
        prompt: str,
        model: str,
        reasoning: Optional[ReasoningConfig] = None,
        **kwargs
    ) -> ModelResponse:
        """Complete with extended thinking support."""

        request_data = {...}

        # Add extended thinking if supported
        if self._supports_thinking(model) and reasoning and reasoning.thinking:
            request_data["thinking"] = {
                "type": "enabled",
                "budget_tokens": reasoning.thinking.budget_tokens
            }

            if reasoning.thinking.interleaved:
                self.client.headers["anthropic-beta"] = "interleaved-thinking-2025-05-14"

        # ... rest ...
```

**4.3: Cost Calculation with LiteLLM (1 hour)**

```python
def _calculate_cost(self, response: dict) -> float:
    """Calculate cost including thinking tokens."""
    usage = response.get("usage", {})
    model = response.get("model", "")

    input_tokens = usage.get("input_tokens", 0)
    output_tokens = usage.get("output_tokens", 0)

    # LiteLLM handles thinking tokens automatically
    if LITELLM_AVAILABLE:
        from cascadeflow.integrations.litellm import calculate_cost_with_reasoning
        return calculate_cost_with_reasoning(
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens
        )

    return self._fallback_cost(model, input_tokens, output_tokens)
```

**Validation:**
- [ ] Extended thinking works
- [ ] Budget tokens applied
- [ ] Thinking text parsed
- [ ] Cost calculated correctly
- [ ] Works with and without LiteLLM

---

### Milestone 5: Dependencies & Configuration (2 hours)

**Branch:** `dependencies-config`

#### Tasks

**5.1: Update Python Dependencies**

**pyproject.toml:**
```toml
[project.optional-dependencies]
# ML/Semantic detection
ml = [
    "fastembed>=0.2.0",
    "numpy>=1.24.0",
]

# LiteLLM integration (already present, ensure up-to-date)
litellm = [
    "litellm>=1.30.0",
]

# All features
all = [
    "openai>=1.0.0",
    "anthropic>=0.8.0",
    "groq>=0.4.0",
    "fastembed>=0.2.0",
    "numpy>=1.24.0",
    "litellm>=1.30.0",
]
```

**5.2: Update TypeScript Dependencies**

**packages/ml/package.json:**
```json
{
  "dependencies": {
    "@xenova/transformers": "^2.17.0"
  },
  "peerDependencies": {
    "@cascadeflow/core": "^0.2.2"
  }
}
```

**packages/core/package.json:**
```json
{
  "peerDependencies": {
    "@cascadeflow/ml": "^0.2.2"
  },
  "peerDependenciesMeta": {
    "@cascadeflow/ml": {
      "optional": true
    }
  }
}
```

**Validation:**
- [ ] `pip install cascadeflow[ml]` works
- [ ] `pip install cascadeflow[litellm]` works
- [ ] `npm install @cascadeflow/ml` works
- [ ] No dependency conflicts

---

### Milestone 6: n8n Integration (2 hours)

**Branch:** `n8n-ml-reasoning`

#### Tasks

**6.1: Add ML Toggle**

```typescript
// packages/integrations/n8n/nodes/CascadeFlow/CascadeFlow.node.ts

{
  displayName: 'Enable ML Detection',
  name: 'enableSemanticDetection',
  type: 'boolean',
  default: false,
  description: 'Enable ML-based semantic domain detection (requires @cascadeflow/ml)',
},
```

**6.2: Add Reasoning Parameters**

```typescript
{
  displayName: 'Reasoning Effort',
  name: 'reasoningEffort',
  type: 'options',
  displayOptions: {
    show: {
      modelName: ['o1', 'o1-preview', 'o3-mini'],
    },
  },
  options: [
    { name: 'Low', value: 'low' },
    { name: 'Medium', value: 'medium' },
    { name: 'High', value: 'high' },
  ],
  default: 'medium',
},
```

**Validation:**
- [ ] n8n node loads correctly
- [ ] ML toggle works
- [ ] Reasoning parameters work
- [ ] Documentation updated

---

### Milestone 7: Documentation (3-4 hours)

**Branch:** `documentation-v0.2.2`

#### Tasks

**7.1: Update README.md**

**Production Features Table:**
```markdown
| Feature | Status | Notes |
|---------|--------|-------|
| Reasoning Models | ✅ Full | OpenAI o1/o3, Anthropic extended thinking |
| ML Semantic Detection | ✅ Full | Python + TypeScript, 84-87% confidence |
| LiteLLM Integration | ✅ Full | Accurate pricing for 100+ models |
```

**Update TypeScript ML Snippet:**
Remove "Python only" note, add working code.

**Add Reasoning Section:**
```markdown
### Reasoning Models

**OpenAI o1/o3:**
```python
from cascadeflow import CascadeAgent, ModelConfig, ReasoningConfig

agent = CascadeAgent(models=[
    ModelConfig(
        name="o3-mini",
        provider="openai",
        cost=0.001,  # Or use LiteLLM for automatic pricing
        reasoning=ReasoningConfig(effort="high", max_completion_tokens=32000)
    )
])
```

**Anthropic Extended Thinking:**
```python
agent = CascadeAgent(models=[
    ModelConfig(
        name="claude-sonnet-4-5",
        provider="anthropic",
        reasoning=ReasoningConfig(
            thinking=ThinkingConfig(enabled=True, budget_tokens=10000)
        )
    )
])
```
```

**7.2: Create Reasoning Guide**

**docs/guides/reasoning-models.md:**
- OpenAI o1/o3 usage
- Anthropic extended thinking
- LiteLLM integration benefits
- Cost optimization strategies

**7.3: Update LiteLLM Documentation**

**docs/guides/litellm-integration.md:**
- Reasoning token pricing
- Model capability detection
- Fallback behavior
- Installation and setup

**Validation:**
- [ ] All examples tested
- [ ] Links work correctly
- [ ] No AI assistant mentions

---

### Milestone 8: Testing (3-4 hours)

**Branch:** `testing-v0.2.2`

#### Tasks

**8.1: ML Tests**
- [ ] Python ML with FastEmbed
- [ ] TypeScript ML with Transformers.js
- [ ] Semantic validation
- [ ] Domain detection accuracy

**8.2: Reasoning Model Tests**
- [ ] OpenAI o1/o3 support
- [ ] Anthropic extended thinking
- [ ] Reasoning token tracking
- [ ] Cost calculation

**8.3: LiteLLM Integration Tests**
- [ ] Cost calculation with LiteLLM
- [ ] Cost calculation without LiteLLM
- [ ] Model capabilities detection
- [ ] Graceful fallback

**8.4: Integration Tests**
- [ ] ML + reasoning models
- [ ] Cascade with reasoning
- [ ] n8n integration
- [ ] Backward compatibility

**Validation:**
- [ ] Python: 95%+ coverage
- [ ] TypeScript: 90%+ coverage
- [ ] All tests pass
- [ ] No regressions

---

## Testing Strategy

### Unit Testing

**Python:**
- `tests/test_ml_integration.py`
- `tests/test_reasoning_openai.py`
- `tests/test_reasoning_anthropic.py`
- `tests/test_litellm_reasoning.py`

**TypeScript:**
- `packages/ml/src/__tests__/embedding.test.ts`
- `packages/core/src/__tests__/reasoning.test.ts`

### Integration Testing

**Scenarios:**
1. ML detection with reasoning models
2. LiteLLM cost calculation
3. Fallback when LiteLLM unavailable
4. Cascade with reasoning
5. n8n integration

### Manual Testing

**Test Matrix:**

| Feature | Python | TypeScript | LiteLLM | Fallback | Status |
|---------|--------|------------|---------|----------|--------|
| ML Detection | ✅ | ✅ | N/A | ✅ | Pending |
| o1 Reasoning | ✅ | ✅ | ✅ | ✅ | Pending |
| Claude Thinking | ✅ | ✅ | ✅ | ✅ | Pending |
| Cost Tracking | ✅ | ✅ | ✅ | ✅ | Pending |

---

## Documentation Updates

### README.md
- [ ] Update production features table
- [ ] Add reasoning models section
- [ ] Update TypeScript ML snippet
- [ ] Add LiteLLM benefits

### Guides
- [ ] Create reasoning-models.md
- [ ] Update litellm-integration.md
- [ ] Update providers.md
- [ ] Update n8n_integration.md

### Examples
- [ ] reasoning_models.py (Python)
- [ ] reasoning-models.ts (TypeScript)
- [ ] Update examples/README.md

---

## Success Criteria

### Must Have
1. ✅ TypeScript ML detection (84-87% confidence)
2. ✅ OpenAI o1/o3 reasoning support
3. ✅ Anthropic extended thinking support
4. ✅ LiteLLM integration for reasoning tokens
5. ✅ Graceful fallback without LiteLLM
6. ✅ Fully backward compatible
7. ✅ All tests pass
8. ✅ No AI assistant mentions

### Should Have
1. ✅ Browser environment support
2. ✅ n8n integration updated
3. ✅ Cascade strategies for reasoning
4. ✅ Comprehensive documentation

### Nice to Have
1. ⚠️ Edge function support
2. ⚠️ Advanced reasoning strategies
3. ⚠️ Reasoning token analytics

---

## Timeline

| Milestone | Time | Dependencies |
|-----------|------|--------------|
| M1: TypeScript ML | 4-5h | None |
| M2: Core ML Integration | 3-4h | M1 |
| M3: OpenAI Reasoning | 4-5h | None |
| M4: Anthropic Reasoning | 3-4h | None |
| M5: Dependencies | 2h | M1-M4 |
| M6: n8n Integration | 2h | M1-M4 |
| M7: Documentation | 3-4h | All |
| M8: Testing | 3-4h | All |

**Total: 24-30 hours**

---

## Backward Compatibility

### Guaranteed Compatibility

1. **Existing Code Works Unchanged:**
```python
# This still works exactly as before
agent = CascadeAgent(models=[
    ModelConfig(name="gpt-4o-mini", provider="openai", cost=0.00015)
])
```

2. **Optional Features:**
- ML detection: Opt-in with `enable_semantic_detection=True`
- Reasoning models: Opt-in by using o1/o3 models
- LiteLLM: Auto-detected, graceful fallback

3. **No Breaking Changes:**
- All existing APIs unchanged
- All existing parameters still work
- All existing examples still run

---

## Commit Message Guidelines

**Standard Professional Commits Only:**

```bash
# Feature commits
git commit -m "feat(ml): add TypeScript embedding service"
git commit -m "feat(reasoning): add OpenAI o1/o3 support"
git commit -m "feat(litellm): extend for reasoning token pricing"

# Fix commits
git commit -m "fix(ml): handle model loading errors gracefully"
git commit -m "fix(reasoning): correct cost calculation for o3-mini"

# Docs commits
git commit -m "docs(reasoning): add comprehensive guide for o1/o3 models"
git commit -m "docs(litellm): update integration documentation"

# Test commits
git commit -m "test(ml): add embedding service unit tests"
git commit -m "test(reasoning): add integration tests for cascade"

# Chore commits
git commit -m "chore(deps): update @xenova/transformers to 2.17.0"
git commit -m "chore(build): add ML package to monorepo"
```

**❌ NEVER Use:**
- "Co-Authored-By: Claude <...>"
- "Generated with Claude Code"
- Any AI assistant mentions
- "with AI assistance"
- "AI pair programming"

---

## Next Steps

1. **Review Plan** - Ensure alignment with requirements
2. **Start Milestone 1** - TypeScript ML infrastructure
3. **Validate After Each** - Test before proceeding
4. **Merge to Main** - When all milestones complete

**Status:** ✅ Plan ready for implementation

**Current Branch:** `feature/cost-control-quality-v2`

**Next Action:** Begin Milestone 1 or request plan modifications
