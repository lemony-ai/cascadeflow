# CascadeFlow v0.2.1 - Detailed Milestones

**Date**: October 28, 2025
**Plan**: See `V0.2.1_IMPLEMENTATION_PLAN.md`
**Status**: ðŸ”¨ READY TO IMPLEMENT

---

## ðŸ“‹ Milestone Overview

| Milestone | Duration | Priority | Dependencies | Risk |
|-----------|----------|----------|--------------|------|
| **M1: Batch Completion** | 3-4 days | P0 | None | Low |
| **M2: Async Streaming** | 2-3 days | P0 | None | Low |
| **M3: User Profiles** | 4-5 days | P0 | None | Medium |
| **M4: Rate Limiting** | 2-3 days | P1 | M3 | Low |
| **M5: Guardrails** | 3-4 days | P1 | M3 | Medium |

**Total Estimated Time**: 14-19 days (3-4 weeks)

---

## ðŸŽ¯ Milestone 1: Batch Completion

### Goal
Process multiple queries efficiently with LiteLLM native batch + fallback to sequential processing.

### Duration
3-4 days (Days 1-4 of implementation)

### Dependencies
- None (can start immediately)

### Files to Create

#### 1. `cascadeflow/batch_config.py`
**Purpose**: Configuration for batch processing
**Lines**: ~80 lines
**Priority**: P0

```python
"""Batch processing configuration for CascadeFlow."""

from dataclasses import dataclass
from typing import Optional, List, Dict, Any
from enum import Enum


class BatchStrategy(str, Enum):
    """Batch processing strategy"""
    LITELLM_NATIVE = "litellm_native"  # Use LiteLLM batch API (preferred)
    SEQUENTIAL = "sequential"          # Sequential with concurrency control
    AUTO = "auto"                       # Auto-detect best strategy


@dataclass
class BatchConfig:
    """
    Configuration for batch processing.

    Example:
        config = BatchConfig(
            batch_size=10,
            max_parallel=3,
            timeout_per_query=30.0,
            strategy=BatchStrategy.AUTO
        )
    """

    # Batch settings
    batch_size: int = 10
    """Maximum number of queries in a single batch"""

    max_parallel: int = 3
    """Maximum number of parallel requests (fallback mode)"""

    timeout_per_query: float = 30.0
    """Timeout per query in seconds"""

    total_timeout: Optional[float] = None
    """Total timeout for entire batch (default: timeout_per_query * batch_size)"""

    # Strategy
    strategy: BatchStrategy = BatchStrategy.AUTO
    """Batch processing strategy"""

    # Error handling
    stop_on_error: bool = False
    """Stop processing batch if any query fails"""

    retry_failed: bool = True
    """Retry failed queries once"""

    # Cost & quality
    track_cost: bool = True
    """Track cost for each query in batch"""

    validate_quality: bool = True
    """Validate quality for each query in batch"""

    # Advanced
    preserve_order: bool = True
    """Preserve query order in results"""

    metadata: Dict[str, Any] = None
    """Custom metadata for batch"""

    def __post_init__(self):
        if self.total_timeout is None:
            self.total_timeout = self.timeout_per_query * self.batch_size
        if self.metadata is None:
            self.metadata = {}
```

#### 2. `cascadeflow/batch.py`
**Purpose**: Core batch processing logic
**Lines**: ~350 lines
**Priority**: P0

```python
"""Batch processing for CascadeFlow."""

import asyncio
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass
import time
from enum import Enum

from .batch_config import BatchConfig, BatchStrategy
from .result import CascadeResult


# Check if LiteLLM batch is available
try:
    from litellm import batch_completion
    HAS_LITELLM_BATCH = True
except (ImportError, AttributeError):
    HAS_LITELLM_BATCH = False


@dataclass
class BatchResult:
    """Result from batch processing"""

    results: List[Optional[CascadeResult]]
    """Results for each query (None if failed)"""

    success_count: int
    """Number of successful queries"""

    failure_count: int
    """Number of failed queries"""

    total_cost: float
    """Total cost for all queries"""

    total_time: float
    """Total processing time in seconds"""

    strategy_used: str
    """Strategy used (litellm_native or sequential)"""

    errors: List[Optional[str]]
    """Error messages for failed queries"""

    metadata: Dict[str, Any]
    """Custom metadata"""

    @property
    def success_rate(self) -> float:
        """Success rate (0.0 to 1.0)"""
        total = self.success_count + self.failure_count
        return self.success_count / total if total > 0 else 0.0

    @property
    def average_cost(self) -> float:
        """Average cost per query"""
        total = self.success_count + self.failure_count
        return self.total_cost / total if total > 0 else 0.0

    @property
    def average_time(self) -> float:
        """Average time per query in seconds"""
        total = self.success_count + self.failure_count
        return self.total_time / total if total > 0 else 0.0


class BatchProcessor:
    """
    Batch processor with LiteLLM + fallback.

    Features:
    - LiteLLM native batch (preferred, most efficient)
    - Sequential fallback with concurrency control
    - Cost tracking per query
    - Quality validation per query
    - Error handling and retry logic
    """

    def __init__(self, agent: 'CascadeAgent'):
        """
        Initialize batch processor.

        Args:
            agent: CascadeAgent instance to use for processing
        """
        self.agent = agent

    async def process_batch(
        self,
        queries: List[str],
        config: Optional[BatchConfig] = None,
        **kwargs
    ) -> BatchResult:
        """
        Process a batch of queries.

        Args:
            queries: List of query strings
            config: Batch configuration (default: BatchConfig())
            **kwargs: Additional arguments passed to agent.run()

        Returns:
            BatchResult with all results and metadata

        Example:
            queries = ["What is Python?", "What is JavaScript?", "What is Rust?"]
            result = await processor.process_batch(queries)

            for i, cascade_result in enumerate(result.results):
                print(f"Query {i}: {cascade_result.content}")

            print(f"Success rate: {result.success_rate:.1%}")
            print(f"Total cost: ${result.total_cost:.4f}")
        """
        if config is None:
            config = BatchConfig()

        start_time = time.time()

        # Choose strategy
        strategy = self._choose_strategy(config.strategy)

        # Process batch using chosen strategy
        if strategy == BatchStrategy.LITELLM_NATIVE:
            results, errors = await self._process_litellm_batch(queries, config, **kwargs)
        else:
            results, errors = await self._process_sequential_batch(queries, config, **kwargs)

        # Calculate statistics
        success_count = sum(1 for r in results if r is not None)
        failure_count = len(results) - success_count
        total_cost = sum(r.cost for r in results if r is not None)
        total_time = time.time() - start_time

        return BatchResult(
            results=results,
            success_count=success_count,
            failure_count=failure_count,
            total_cost=total_cost,
            total_time=total_time,
            strategy_used=strategy.value,
            errors=errors,
            metadata=config.metadata
        )

    def _choose_strategy(self, strategy: BatchStrategy) -> BatchStrategy:
        """Choose batch processing strategy"""
        if strategy == BatchStrategy.AUTO:
            return BatchStrategy.LITELLM_NATIVE if HAS_LITELLM_BATCH else BatchStrategy.SEQUENTIAL
        return strategy

    async def _process_litellm_batch(
        self,
        queries: List[str],
        config: BatchConfig,
        **kwargs
    ) -> tuple[List[Optional[CascadeResult]], List[Optional[str]]]:
        """Process batch using LiteLLM native batch API"""
        if not HAS_LITELLM_BATCH:
            raise RuntimeError("LiteLLM batch not available, use sequential strategy")

        results: List[Optional[CascadeResult]] = []
        errors: List[Optional[str]] = []

        # Split into batches of batch_size
        for i in range(0, len(queries), config.batch_size):
            batch_queries = queries[i:i + config.batch_size]

            try:
                # Use LiteLLM batch_completion
                # Note: This is a simplified implementation
                # Real implementation would integrate with cascade routing
                batch_results = await asyncio.wait_for(
                    self._litellm_batch_helper(batch_queries, **kwargs),
                    timeout=config.total_timeout
                )

                results.extend(batch_results)
                errors.extend([None] * len(batch_results))

            except asyncio.TimeoutError:
                # Timeout: mark all queries in this batch as failed
                results.extend([None] * len(batch_queries))
                errors.extend(["Timeout"] * len(batch_queries))

                if config.stop_on_error:
                    break

            except Exception as e:
                # Error: mark all queries in this batch as failed
                results.extend([None] * len(batch_queries))
                errors.extend([str(e)] * len(batch_queries))

                if config.stop_on_error:
                    break

        return results, errors

    async def _litellm_batch_helper(
        self,
        queries: List[str],
        **kwargs
    ) -> List[CascadeResult]:
        """Helper to process batch with LiteLLM"""
        # This would integrate with CascadeAgent's routing logic
        # For now, process sequentially with concurrency
        tasks = [self.agent.run(query, **kwargs) for query in queries]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Convert exceptions to None
        return [r if not isinstance(r, Exception) else None for r in results]

    async def _process_sequential_batch(
        self,
        queries: List[str],
        config: BatchConfig,
        **kwargs
    ) -> tuple[List[Optional[CascadeResult]], List[Optional[str]]]:
        """Process batch sequentially with concurrency control"""
        results: List[Optional[CascadeResult]] = []
        errors: List[Optional[str]] = []

        # Semaphore for concurrency control
        semaphore = asyncio.Semaphore(config.max_parallel)

        async def process_one(query: str, index: int) -> tuple[int, Optional[CascadeResult], Optional[str]]:
            """Process single query with semaphore"""
            async with semaphore:
                try:
                    result = await asyncio.wait_for(
                        self.agent.run(query, **kwargs),
                        timeout=config.timeout_per_query
                    )
                    return index, result, None

                except asyncio.TimeoutError:
                    if config.retry_failed:
                        # Retry once
                        try:
                            result = await asyncio.wait_for(
                                self.agent.run(query, **kwargs),
                                timeout=config.timeout_per_query
                            )
                            return index, result, None
                        except Exception as e:
                            return index, None, f"Timeout (retry failed: {str(e)})"
                    else:
                        return index, None, "Timeout"

                except Exception as e:
                    if config.retry_failed:
                        # Retry once
                        try:
                            result = await asyncio.wait_for(
                                self.agent.run(query, **kwargs),
                                timeout=config.timeout_per_query
                            )
                            return index, result, None
                        except Exception as retry_e:
                            return index, None, f"Error (retry failed: {str(retry_e)})"
                    else:
                        return index, None, str(e)

        # Create tasks for all queries
        tasks = [process_one(query, i) for i, query in enumerate(queries)]

        # Process with timeout
        try:
            completed = await asyncio.wait_for(
                asyncio.gather(*tasks),
                timeout=config.total_timeout
            )
        except asyncio.TimeoutError:
            # Total timeout exceeded
            completed = []
            for i in range(len(queries)):
                completed.append((i, None, "Total timeout exceeded"))

        # Sort by index to preserve order (if requested)
        if config.preserve_order:
            completed.sort(key=lambda x: x[0])

        # Extract results and errors
        results = [r for _, r, _ in completed]
        errors = [e for _, _, e in completed]

        return results, errors
```

#### 3. `cascadeflow/agent.py` (modifications)
**Purpose**: Add `run_batch()` method to CascadeAgent
**Lines**: ~50 lines added
**Priority**: P0

```python
# Add to CascadeAgent class

from .batch import BatchProcessor, BatchResult
from .batch_config import BatchConfig

class CascadeAgent:
    # ... existing code ...

    def __init__(self, ...):
        # ... existing init ...
        self._batch_processor = None

    async def run_batch(
        self,
        queries: List[str],
        batch_config: Optional[BatchConfig] = None,
        **kwargs
    ) -> BatchResult:
        """
        Process multiple queries in batch.

        Features:
        - LiteLLM native batch (preferred, automatic)
        - Sequential fallback with concurrency control
        - Cost tracking per query
        - Quality validation per query
        - Automatic retry on failures

        Args:
            queries: List of query strings
            batch_config: Batch configuration (default: BatchConfig())
            **kwargs: Additional arguments passed to run()

        Returns:
            BatchResult with all results and statistics

        Example:
            queries = ["What is Python?", "What is JS?", "What is Rust?"]
            result = await agent.run_batch(queries)

            print(f"Success: {result.success_count}/{len(queries)}")
            print(f"Total cost: ${result.total_cost:.4f}")
            print(f"Strategy: {result.strategy_used}")

            for i, cascade_result in enumerate(result.results):
                if cascade_result:
                    print(f"{i}: {cascade_result.content[:100]}...")
        """
        if self._batch_processor is None:
            self._batch_processor = BatchProcessor(self)

        return await self._batch_processor.process_batch(queries, batch_config, **kwargs)
```

### Tests to Create

#### 4. `tests/test_batch.py`
**Purpose**: Comprehensive batch testing
**Lines**: ~600 lines
**Priority**: P0
**Tests**: 15+ unit tests, 5+ integration tests, 2+ E2E tests

**Test Categories**:
1. **Unit Tests** (15+):
   - BatchConfig creation
   - BatchConfig validation
   - Strategy selection (auto, litellm, sequential)
   - Error handling
   - Retry logic
   - Timeout handling
   - Cost tracking
   - Quality validation
   - Order preservation
   - Batch splitting
   - Result aggregation
   - Success rate calculation
   - Average cost calculation
   - Average time calculation
   - Metadata handling

2. **Integration Tests** (5+):
   - Batch with LiteLLM (10 queries)
   - Batch without LiteLLM (10 queries, fallback)
   - Batch with mixed success/failure
   - Batch with cost tracking
   - Batch with quality validation

3. **E2E Tests** (2+):
   - Real-world batch: 20 queries, measure performance
   - Real-world batch: 50 queries with errors, test resilience

### Example to Create

#### 5. `examples/batch_processing.py`
**Purpose**: Working example of batch processing
**Lines**: ~120 lines
**Priority**: P0

```python
"""
Example: Batch Processing with CascadeFlow

This example demonstrates:
1. Simple batch processing
2. Batch with custom configuration
3. Handling batch results
4. Cost tracking for batches
5. Error handling
"""

import asyncio
from cascadeflow import CascadeAgent
from cascadeflow.batch_config import BatchConfig, BatchStrategy


async def example_simple_batch():
    """Simple batch processing"""
    print("\n=== Example 1: Simple Batch ===\n")

    agent = CascadeAgent()

    queries = [
        "What is Python?",
        "What is JavaScript?",
        "What is Rust?",
        "What is Go?",
        "What is TypeScript?"
    ]

    result = await agent.run_batch(queries)

    print(f"Success: {result.success_count}/{len(queries)}")
    print(f"Total cost: ${result.total_cost:.4f}")
    print(f"Average cost: ${result.average_cost:.4f}")
    print(f"Total time: {result.total_time:.2f}s")
    print(f"Average time: {result.average_time:.2f}s")
    print(f"Strategy: {result.strategy_used}")


async def example_custom_config():
    """Batch with custom configuration"""
    print("\n=== Example 2: Custom Configuration ===\n")

    agent = CascadeAgent()

    config = BatchConfig(
        batch_size=10,
        max_parallel=5,
        timeout_per_query=30.0,
        strategy=BatchStrategy.AUTO,
        stop_on_error=False,
        retry_failed=True
    )

    queries = [f"Explain concept {i}" for i in range(20)]

    result = await agent.run_batch(queries, batch_config=config)

    print(f"Processed {len(queries)} queries")
    print(f"Success rate: {result.success_rate:.1%}")
    print(f"Total cost: ${result.total_cost:.4f}")


async def example_handle_errors():
    """Handle batch errors gracefully"""
    print("\n=== Example 3: Error Handling ===\n")

    agent = CascadeAgent()

    queries = [
        "Valid query 1",
        "",  # Empty query (will fail)
        "Valid query 2",
        None,  # Invalid (will fail)
        "Valid query 3"
    ]

    result = await agent.run_batch(queries)

    for i, (cascade_result, error) in enumerate(zip(result.results, result.errors)):
        if cascade_result:
            print(f"âœ“ Query {i}: Success (cost: ${cascade_result.cost:.4f})")
        else:
            print(f"âœ— Query {i}: Failed ({error})")


async def main():
    """Run all examples"""
    await example_simple_batch()
    await example_custom_config()
    await example_handle_errors()


if __name__ == "__main__":
    asyncio.run(main())
```

### Performance Benchmarks

#### 6. `benchmarks/batch_benchmark.py`
**Purpose**: Benchmark batch performance
**Lines**: ~200 lines
**Priority**: P1

**Benchmarks**:
- 10 queries (LiteLLM vs Sequential)
- 50 queries (LiteLLM vs Sequential)
- 100 queries (LiteLLM vs Sequential)
- Cost comparison
- Time comparison

### Success Criteria

âœ… **Code Complete**:
- All 3 files created (`batch_config.py`, `batch.py`, `agent.py` modifications)
- All code follows CascadeFlow conventions
- Type hints throughout
- Comprehensive docstrings

âœ… **Tests Pass**:
- 15+ unit tests passing
- 5+ integration tests passing
- 2+ E2E tests passing
- Test coverage >85%

âœ… **Performance**:
- 10 queries in <5 seconds (LiteLLM)
- 10 queries in <15 seconds (sequential fallback)
- 50 queries in <20 seconds (LiteLLM)
- Cost tracking accurate (within 1% error)
- Quality validation works correctly

âœ… **Documentation**:
- Example code working
- Docstrings complete
- Type hints correct

âœ… **Integration**:
- Works with existing CascadeAgent
- Works with cost tracking
- Works with quality validation
- Backwards compatible

### Validation Checklist

Before marking Milestone 1 complete:
- [ ] All files created
- [ ] All tests passing
- [ ] All benchmarks passing
- [ ] Example code working
- [ ] Documentation complete
- [ ] Code committed to GitHub
- [ ] No regressions (v0.2.0 tests still pass)

---

## ðŸŽ¯ Milestone 2: Async Streaming Integration

### Goal
Complete CascadeAgent integration for streaming responses with LiteLLM + non-streaming fallback.

### Duration
2-3 days (Days 5-7 of implementation)

### Dependencies
- None (can run in parallel with M1 if needed)

### Files to Modify

#### 1. `cascadeflow/streaming.py` (new file)
**Purpose**: Streaming configuration and data structures
**Lines**: ~150 lines
**Priority**: P0

```python
"""Streaming support for CascadeFlow."""

from dataclasses import dataclass
from typing import AsyncIterator, Optional, Dict, Any
from enum import Enum


class StreamMode(str, Enum):
    """Streaming mode"""
    STREAM = "stream"          # Real streaming (token by token)
    BUFFER = "buffer"          # Buffer and send all at once
    AUTO = "auto"              # Auto-detect best mode


@dataclass
class StreamConfig:
    """Configuration for streaming"""

    mode: StreamMode = StreamMode.AUTO
    """Streaming mode"""

    buffer_size: int = 1
    """Buffer size (number of tokens before yielding)"""

    include_metadata: bool = True
    """Include metadata in stream chunks"""

    track_cost: bool = True
    """Track cost during streaming"""

    validate_quality_on_complete: bool = True
    """Validate quality on final content"""


@dataclass
class CascadeStreamChunk:
    """
    Single chunk from streaming cascade.

    Attributes:
        model_name: Model that generated this chunk
        content: Partial content (or full if final)
        is_final: Whether this is the last chunk
        cost: Cost so far (if track_cost=True)
        metadata: Additional metadata
    """

    model_name: str
    content: str
    is_final: bool = False
    cost: Optional[float] = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
```

#### 2. `cascadeflow/agent.py` (add .stream() method)
**Purpose**: Add streaming method to CascadeAgent
**Lines**: ~200 lines added
**Priority**: P0

```python
# Add to CascadeAgent class

from .streaming import StreamConfig, StreamMode, CascadeStreamChunk
from typing import AsyncIterator

class CascadeAgent:
    # ... existing code ...

    async def stream(
        self,
        query: str,
        stream_config: Optional[StreamConfig] = None,
        **kwargs
    ) -> AsyncIterator[CascadeStreamChunk]:
        """
        Stream response from cascade.

        Features:
        - Real streaming (token by token) when supported
        - Automatic fallback to buffered mode
        - Cost tracking during streaming
        - Quality validation on final content
        - Cascade routing with streaming support

        Args:
            query: Query string
            stream_config: Streaming configuration
            **kwargs: Additional arguments

        Yields:
            CascadeStreamChunk objects

        Example:
            async for chunk in agent.stream("Explain Python"):
                print(chunk.content, end="", flush=True)
                if chunk.is_final:
                    print(f"\nCost: ${chunk.cost:.4f}")
                    print(f"Model: {chunk.model_name}")
        """
        if stream_config is None:
            stream_config = StreamConfig()

        # Choose streaming mode
        mode = self._choose_stream_mode(stream_config.mode)

        if mode == StreamMode.STREAM:
            # Real streaming
            async for chunk in self._stream_real(query, stream_config, **kwargs):
                yield chunk
        else:
            # Buffered mode (fallback)
            async for chunk in self._stream_buffered(query, stream_config, **kwargs):
                yield chunk

    def _choose_stream_mode(self, mode: StreamMode) -> StreamMode:
        """Choose streaming mode based on provider support"""
        if mode == StreamMode.AUTO:
            # Check if current provider supports streaming
            # For now, always try streaming if provider supports it
            return StreamMode.STREAM
        return mode

    async def _stream_real(
        self,
        query: str,
        config: StreamConfig,
        **kwargs
    ) -> AsyncIterator[CascadeStreamChunk]:
        """Real streaming implementation"""

        # Route query through cascade to find best model
        selected_model = await self._select_model_for_query(query)

        # Get provider for selected model
        provider = self._get_provider_for_model(selected_model)

        # Check if provider supports streaming
        if not hasattr(provider, 'stream'):
            # Fallback to buffered
            async for chunk in self._stream_buffered(query, config, **kwargs):
                yield chunk
            return

        # Stream from provider
        accumulated_content = ""
        accumulated_cost = 0.0

        try:
            async for token in provider.stream(
                prompt=query,
                model=selected_model,
                **kwargs
            ):
                accumulated_content += token

                # Track cost (approximate during streaming)
                if config.track_cost:
                    accumulated_cost = self._estimate_streaming_cost(
                        accumulated_content,
                        selected_model
                    )

                # Yield chunk
                yield CascadeStreamChunk(
                    model_name=selected_model,
                    content=token,
                    is_final=False,
                    cost=accumulated_cost if config.track_cost else None,
                    metadata={"mode": "stream"} if config.include_metadata else {}
                )

            # Final chunk
            final_cost = accumulated_cost
            if config.track_cost:
                # Get exact cost after completion
                final_cost = await self._calculate_exact_cost(
                    accumulated_content,
                    selected_model
                )

            # Validate quality if requested
            quality_score = None
            if config.validate_quality_on_complete:
                quality_score = await self._validate_quality(
                    query,
                    accumulated_content
                )

            yield CascadeStreamChunk(
                model_name=selected_model,
                content="",
                is_final=True,
                cost=final_cost if config.track_cost else None,
                metadata={
                    "mode": "stream",
                    "quality_score": quality_score
                } if config.include_metadata else {}
            )

        except Exception as e:
            # Error during streaming, fallback to buffered
            async for chunk in self._stream_buffered(query, config, **kwargs):
                yield chunk

    async def _stream_buffered(
        self,
        query: str,
        config: StreamConfig,
        **kwargs
    ) -> AsyncIterator[CascadeStreamChunk]:
        """Buffered streaming (fallback)"""

        # Run normal query
        result = await self.run(query, **kwargs)

        # Yield single chunk with full content
        yield CascadeStreamChunk(
            model_name=result.model_name,
            content=result.content,
            is_final=True,
            cost=result.cost if config.track_cost else None,
            metadata={
                "mode": "buffered",
                "quality_score": result.quality_score
            } if config.include_metadata else {}
        )
```

### Tests to Create

#### 3. `tests/test_streaming.py`
**Purpose**: Comprehensive streaming tests
**Lines**: ~450 lines
**Priority**: P0
**Tests**: 12+ unit tests, 4+ integration tests, 2+ E2E tests

**Test Categories**:
1. **Unit Tests** (12+):
   - StreamConfig creation
   - StreamMode selection
   - Chunk creation
   - Cost tracking during streaming
   - Quality validation on final chunk
   - Buffered mode fallback
   - Error handling during streaming
   - Stream cancellation
   - Metadata in chunks
   - Buffer size handling
   - Multiple concurrent streams
   - Memory efficiency

2. **Integration Tests** (4+):
   - Stream with LiteLLM
   - Stream without LiteLLM (fallback to buffered)
   - Stream with cascade routing
   - Stream with quality validation

3. **E2E Tests** (2+):
   - Real-world streaming: Long response
   - Real-world streaming: Multiple queries concurrently

### Example to Create

#### 4. `examples/streaming_example.py`
**Purpose**: Working streaming example
**Lines**: ~150 lines
**Priority**: P0

### Success Criteria

âœ… **Code Complete**:
- `streaming.py` created
- `agent.py` modified with `.stream()` method
- Type hints and docstrings complete

âœ… **Tests Pass**:
- 12+ unit tests passing
- 4+ integration tests passing
- 2+ E2E tests passing

âœ… **Performance**:
- First token in <500ms
- Smooth streaming (no buffering delays)
- Cost tracking accurate

âœ… **Integration**:
- Works with existing cascade routing
- Works with quality validation
- Graceful fallback to buffered mode

### Validation Checklist

Before marking Milestone 2 complete:
- [ ] All files created/modified
- [ ] All tests passing
- [ ] First token <500ms validated
- [ ] Example code working
- [ ] Documentation complete
- [ ] Code committed to GitHub
- [ ] No regressions

---

## ðŸŽ¯ Milestone 3: User Profile Foundation

### Goal
Implement TierConfig, UserProfile, UserProfileManager, and CascadeAgent integration for multi-dimensional user management.

### Duration
4-5 days (Days 8-12 of implementation)

### Dependencies
- None (independent of M1 and M2)

### Files to Create

**Module Structure**:
```
cascadeflow/profiles/
â”œâ”€â”€ __init__.py          (exports)
â”œâ”€â”€ tier_config.py       (TierConfig, TierLevel, TIER_PRESETS)
â”œâ”€â”€ user_profile.py      (UserProfile)
â”œâ”€â”€ profile_manager.py   (UserProfileManager)
â””â”€â”€ presets.py          (Tier preset definitions)
```

#### 1. `cascadeflow/profiles/tier_config.py`
**Lines**: ~250 lines
**Priority**: P0
**Details**: Complete implementation in V0.2.1_IMPLEMENTATION_PLAN.md

#### 2. `cascadeflow/profiles/user_profile.py`
**Lines**: ~200 lines
**Priority**: P0
**Details**: Complete implementation in V0.2.1_IMPLEMENTATION_PLAN.md

#### 3. `cascadeflow/profiles/profile_manager.py`
**Lines**: ~300 lines
**Priority**: P0
**Details**: Complete implementation in V0.2.1_IMPLEMENTATION_PLAN.md

#### 4. `cascadeflow/agent.py` (add `.from_profile()` method)
**Lines**: ~80 lines added
**Priority**: P0
**Details**: Complete implementation in V0.2.1_IMPLEMENTATION_PLAN.md

### Tests to Create

#### 5. `tests/test_user_profiles.py`
**Purpose**: Comprehensive profile tests
**Lines**: ~700 lines
**Priority**: P0
**Tests**: 20+ unit tests, 8+ integration tests, 3+ E2E tests

### Examples to Create

#### 6. `examples/user_profile_simple.py`
**Lines**: ~80 lines

#### 7. `examples/user_profile_advanced.py`
**Lines**: ~150 lines

### Success Criteria

âœ… **Code Complete**:
- All files in `cascadeflow/profiles/` module
- CascadeAgent.from_profile() implemented

âœ… **Tests Pass**:
- 20+ unit tests passing
- 8+ integration tests passing
- 3+ E2E tests passing

âœ… **Performance**:
- Profile creation <1ms (from preset)
- Profile get (cached) <1ms
- Profile get (DB load) <10ms
- Bulk creation: 1000 profiles in <100ms

âœ… **Integration**:
- Works with CascadeAgent
- Backwards compatible (v0.2.0 code still works)

### Validation Checklist

Before marking Milestone 3 complete:
- [ ] All files created
- [ ] All tests passing
- [ ] All performance benchmarks met
- [ ] Examples working
- [ ] Documentation complete
- [ ] Backwards compatibility verified
- [ ] Code committed to GitHub

---

## ðŸŽ¯ Milestone 4: Rate Limiting

### Goal
Per-user, per-tier rate limiting with sliding window algorithm.

### Duration
2-3 days (Days 13-15 of implementation)

### Dependencies
- **M3 (User Profiles)** - Rate limits come from UserProfile

### Files to Create

#### 1. `cascadeflow/limits/rate_limiter.py`
**Lines**: ~250 lines
**Priority**: P0
**Details**: Complete implementation in V0.2.1_IMPLEMENTATION_PLAN.md

#### 2. `cascadeflow/agent.py` (integrate rate limiting)
**Lines**: ~40 lines added
**Priority**: P0

### Tests to Create

#### 3. `tests/test_rate_limiting.py`
**Lines**: ~400 lines
**Tests**: 10+ unit tests, 5+ integration tests, 2+ E2E tests

### Example to Create

#### 4. `examples/rate_limiting_example.py`
**Lines**: ~100 lines

### Success Criteria

âœ… **Code Complete**:
- `rate_limiter.py` created
- Integrated with CascadeAgent
- Integrated with UserProfile

âœ… **Tests Pass**:
- 10+ unit tests passing
- Accurate sliding window (within 1% error)
- Per-user isolation works

âœ… **Performance**:
- <1ms overhead per request

### Validation Checklist

Before marking Milestone 4 complete:
- [ ] All files created
- [ ] All tests passing
- [ ] Performance validated
- [ ] Example working
- [ ] Code committed to GitHub

---

## ðŸŽ¯ Milestone 5: Basic Guardrails

### Goal
Content moderation and PII detection with LiteLLM + regex fallback.

### Duration
3-4 days (Days 16-19 of implementation)

### Dependencies
- **M3 (User Profiles)** - Guardrail settings come from UserProfile

### Files to Create

#### 1. `cascadeflow/guardrails/content_moderation.py`
**Lines**: ~300 lines
**Priority**: P0
**Details**: Complete implementation in V0.2.1_IMPLEMENTATION_PLAN.md

#### 2. `cascadeflow/guardrails/pii_detection.py`
**Lines**: ~250 lines
**Priority**: P0
**Details**: Complete implementation in V0.2.1_IMPLEMENTATION_PLAN.md

#### 3. `cascadeflow/agent.py` (integrate guardrails)
**Lines**: ~60 lines added
**Priority**: P0

### Tests to Create

#### 4. `tests/test_guardrails.py`
**Lines**: ~550 lines
**Tests**: 15+ unit tests, 6+ integration tests, 3+ E2E tests

### Example to Create

#### 5. `examples/guardrails_example.py`
**Lines**: ~180 lines

### Success Criteria

âœ… **Code Complete**:
- ContentModerator implemented
- PIIDetector implemented
- Integrated with CascadeAgent

âœ… **Tests Pass**:
- 15+ unit tests passing
- LiteLLM moderation works
- Regex fallback works

âœ… **Performance**:
- <100ms overhead (LiteLLM)
- <10ms overhead (regex)

### Validation Checklist

Before marking Milestone 5 complete:
- [ ] All files created
- [ ] All tests passing
- [ ] LiteLLM and regex modes both tested
- [ ] Example working
- [ ] Code committed to GitHub

---

## ðŸ“ˆ Progress Tracking

| Milestone | Status | Progress | ETA |
|-----------|--------|----------|-----|
| M1: Batch | â³ Not Started | 0% | Day 4 |
| M2: Streaming | â³ Not Started | 0% | Day 7 |
| M3: Profiles | â³ Not Started | 0% | Day 12 |
| M4: Rate Limiting | â³ Not Started | 0% | Day 15 |
| M5: Guardrails | â³ Not Started | 0% | Day 19 |

**Legend**:
- â³ Not Started
- ðŸ”¨ In Progress
- âœ… Complete

---

## ðŸš€ Daily Execution Plan

### Days 1-4: Milestone 1 (Batch Completion)
- **Day 1**: Create batch_config.py, batch.py (core logic)
- **Day 2**: Add agent.run_batch(), write unit tests
- **Day 3**: Write integration tests, E2E tests
- **Day 4**: Create example, run benchmarks, commit to GitHub

### Days 5-7: Milestone 2 (Async Streaming)
- **Day 5**: Create streaming.py, add agent.stream() (core logic)
- **Day 6**: Write tests (unit, integration, E2E)
- **Day 7**: Create example, validate performance, commit to GitHub

### Days 8-12: Milestone 3 (User Profiles)
- **Day 8**: Create tier_config.py, user_profile.py
- **Day 9**: Create profile_manager.py
- **Day 10**: Add agent.from_profile(), write unit tests
- **Day 11**: Write integration and E2E tests
- **Day 12**: Create examples, run benchmarks, commit to GitHub

### Days 13-15: Milestone 4 (Rate Limiting)
- **Day 13**: Create rate_limiter.py, integrate with agent
- **Day 14**: Write tests (unit, integration, E2E)
- **Day 15**: Create example, validate performance, commit to GitHub

### Days 16-19: Milestone 5 (Guardrails)
- **Day 16**: Create content_moderation.py
- **Day 17**: Create pii_detection.py, integrate with agent
- **Day 18**: Write tests (unit, integration, E2E)
- **Day 19**: Create example, validate, commit to GitHub

---

**Status**: âœ… MILESTONES DEFINED - READY TO START
**Next Action**: Begin Milestone 1 - Day 1 (batch_config.py, batch.py)
**Total Estimated Time**: 19 days (3-4 weeks)
