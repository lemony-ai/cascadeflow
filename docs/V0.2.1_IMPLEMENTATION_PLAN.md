# CascadeFlow v0.2.1 - Implementation Plan

**Date**: October 28, 2025
**Target Release**: WEEK 4-6 from v0.2.0 release
**Status**: ğŸ”¨ PLANNING COMPLETE - READY TO IMPLEMENT

---

## ğŸ¯ Executive Summary

v0.2.1 builds on v0.2.0's Presets 2.0 foundation by adding:
1. **Batch Completion** - Process multiple queries efficiently
2. **Async Streaming** - Complete CascadeAgent integration
3. **User Profile System** - Multi-dimensional user management (foundation)
4. **Rate Limiting** - Per-user, per-tier request controls
5. **Basic Guardrails** - Content moderation and PII detection

**Key Design Principles**:
- âœ… **LiteLLM + Fallback** - All features work with and without LiteLLM
- âœ… **Optional but Powerful** - Simple by default, advanced when needed
- âœ… **Backwards Compatible** - No breaking changes from v0.2.0
- âœ… **Production-Ready** - Handles thousands of users

---

## ğŸ“Š What's Being Added

### Priority 1: Batch & Streaming (WEEK 4)
| Feature | Status | LiteLLM Integration | Fallback | Impact |
|---------|--------|---------------------|----------|--------|
| **Batch Completion** | âŒ New | âœ… Yes (native) | âœ… Sequential | High efficiency |
| **Async Streaming** | âš ï¸ Partial â†’ âœ… Complete | âœ… Yes (native) | âœ… Non-streaming | Better UX |

### Priority 2: User Profiles Foundation (WEEK 5)
| Feature | Status | Dependencies | Impact |
|---------|--------|--------------|--------|
| **TierConfig** | âŒ New | None | Foundation for profiles |
| **UserProfile** | âŒ New | TierConfig | Multi-dimensional mgmt |
| **UserProfileManager** | âŒ New | UserProfile | Scales to thousands |
| **CascadeAgent.from_profile()** | âŒ New | All above | Easy integration |

### Priority 3: Limits & Guardrails (WEEK 6)
| Feature | Status | LiteLLM Integration | Fallback | Impact |
|---------|--------|---------------------|----------|--------|
| **Rate Limiting** | âŒ New | âŒ No (build ourselves) | N/A | Production-ready |
| **Content Moderation** | âŒ New | âœ… Optional (LLM-based) | âœ… Regex-based | Safety |
| **PII Detection** | âŒ New | âœ… Optional (LLM-based) | âœ… Regex-based | Compliance |

---

## ğŸ—ï¸ Architecture Overview

### System Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CascadeAgent API                      â”‚
â”‚  .from_profile() | .run() | .run_batch() | .stream()   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  User Profile System                    â”‚
â”‚  UserProfile | TierConfig | UserProfileManager          â”‚
â”‚  (7 dimensions: tier, limits, prefs, guardrails, ...)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Batch   â”‚    â”‚ Streamingâ”‚    â”‚  Limits  â”‚
    â”‚ Processorâ”‚    â”‚  Handler â”‚    â”‚ Enforcer â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                â”‚                â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  LiteLLM Integration Layer     â”‚
           â”‚  (with graceful fallbacks)     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Existing v0.2.0 Foundation    â”‚
           â”‚  (Presets, Quality, Routing)   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Implementation Milestones

### Milestone 1: Batch Completion (3-4 days)
**Goal**: Process multiple queries efficiently with LiteLLM + fallback

**Files to Create**:
- `cascadeflow/batch.py` - Batch processing logic
- `cascadeflow/batch_config.py` - Batch configuration options
- `tests/test_batch.py` - Comprehensive batch tests
- `examples/batch_usage.py` - Working example

**Key Features**:
```python
# LiteLLM native batch (preferred)
results = await agent.run_batch(
    queries=["query1", "query2", "query3"],
    batch_config=BatchConfig(
        batch_size=10,
        max_parallel=3,
        timeout_per_query=30.0
    )
)

# Fallback: Sequential processing (no LiteLLM)
# Automatically used if LiteLLM batch not available
```

**LiteLLM Integration**:
```python
try:
    from litellm import batch_completion
    HAS_LITELLM_BATCH = True
except ImportError:
    HAS_LITELLM_BATCH = False

async def run_batch(queries: List[str], **kwargs):
    if HAS_LITELLM_BATCH:
        # Use LiteLLM native batch (efficient)
        return await _run_batch_litellm(queries, **kwargs)
    else:
        # Fallback: Sequential with concurrency control
        return await _run_batch_fallback(queries, **kwargs)
```

**Tests Required**:
- âœ… Batch with LiteLLM (10 queries)
- âœ… Batch without LiteLLM (10 queries)
- âœ… Batch with errors (retry logic)
- âœ… Batch with timeout
- âœ… Batch with cost tracking
- âœ… Batch with quality validation

**Success Criteria**:
- All tests pass
- 10 queries in <5 seconds (LiteLLM)
- 10 queries in <15 seconds (fallback)
- Cost tracking accurate
- Quality validation works

---

### Milestone 2: Async Streaming Integration (2-3 days)
**Goal**: Complete CascadeAgent integration for streaming responses

**Files to Modify**:
- `cascadeflow/agent.py` - Add `.stream()` method
- `cascadeflow/cascade.py` - Integrate streaming with cascade logic
- `tests/test_streaming.py` - Comprehensive streaming tests
- `examples/streaming_usage.py` - Working example

**Current Status**:
```python
# âœ… Base streaming exists in providers/base.py:815
async def stream(self, prompt: str, model: str, **kwargs) -> AsyncIterator[str]:
    """Stream response token by token"""
```

**What's Missing**:
```python
# âŒ CascadeAgent.stream() not implemented
# Need to add:

async def stream(
    self,
    query: str,
    stream_config: Optional[StreamConfig] = None
) -> AsyncIterator[CascadeStreamChunk]:
    """
    Stream response from cascade.

    Returns:
        AsyncIterator yielding:
        - model_name: str
        - content: str (partial)
        - is_final: bool
        - metadata: dict
    """
```

**LiteLLM Integration**:
```python
# LiteLLM native streaming (preferred)
async for chunk in litellm.acompletion(stream=True, ...):
    yield CascadeStreamChunk(
        model_name=chunk.model,
        content=chunk.choices[0].delta.content,
        is_final=False
    )

# Fallback: Non-streaming (buffer full response)
result = await self.run(query)
yield CascadeStreamChunk(
    model_name=result.model_name,
    content=result.content,
    is_final=True
)
```

**Tests Required**:
- âœ… Stream with LiteLLM
- âœ… Stream without LiteLLM (fallback)
- âœ… Stream with cascade routing
- âœ… Stream with quality validation
- âœ… Stream with cost tracking
- âœ… Stream cancellation

**Success Criteria**:
- All tests pass
- First token in <500ms
- Smooth streaming (no buffering)
- Quality validation on final content
- Cost tracking accurate

---

### Milestone 3: User Profile Foundation (4-5 days)
**Goal**: Implement TierConfig, UserProfile, UserProfileManager, CascadeAgent integration

**Files to Create**:
- `cascadeflow/profiles/__init__.py` - Module exports
- `cascadeflow/profiles/tier_config.py` - Tier configuration
- `cascadeflow/profiles/user_profile.py` - UserProfile data structure
- `cascadeflow/profiles/profile_manager.py` - UserProfileManager
- `cascadeflow/profiles/presets.py` - Tier presets (FREE, PRO, etc.)
- `tests/test_user_profiles.py` - Comprehensive profile tests
- `examples/user_profile_usage.py` - Working example

**Data Structures** (from USER_PROFILE_SYSTEM_DESIGN.md):

```python
# cascadeflow/profiles/tier_config.py
from dataclasses import dataclass
from enum import Enum
from typing import Optional, List

class TierLevel(str, Enum):
    """Predefined tier levels"""
    FREE = "free"
    STARTER = "starter"
    PRO = "pro"
    BUSINESS = "business"
    ENTERPRISE = "enterprise"

@dataclass
class TierConfig:
    """Tier configuration (one dimension of UserProfile)"""
    name: str

    # Budget limits
    daily_budget: Optional[float] = None
    weekly_budget: Optional[float] = None
    monthly_budget: Optional[float] = None

    # Rate limits
    requests_per_hour: Optional[int] = None
    requests_per_day: Optional[int] = None
    tokens_per_minute: Optional[int] = None

    # Feature flags
    enable_streaming: bool = True
    enable_batch: bool = False
    enable_embeddings: bool = False

    # Quality settings
    min_quality: float = 0.60
    target_quality: float = 0.80

    # Model access
    allowed_models: Optional[List[str]] = None
    blocked_models: Optional[List[str]] = None

    # Support level
    support_priority: str = "community"  # community, priority, dedicated

    @classmethod
    def from_preset(cls, tier: TierLevel) -> 'TierConfig':
        """Create TierConfig from predefined preset"""
        return TIER_PRESETS[tier]

# Predefined tier presets
TIER_PRESETS = {
    TierLevel.FREE: TierConfig(
        name="free",
        daily_budget=0.10,
        requests_per_hour=10,
        requests_per_day=100,
        enable_streaming=False,
        enable_batch=False,
        enable_embeddings=False,
        min_quality=0.60,
        target_quality=0.70,
        support_priority="community"
    ),
    TierLevel.STARTER: TierConfig(
        name="starter",
        daily_budget=1.00,
        requests_per_hour=100,
        requests_per_day=1000,
        enable_streaming=True,
        enable_batch=False,
        enable_embeddings=False,
        min_quality=0.70,
        target_quality=0.80,
        support_priority="community"
    ),
    TierLevel.PRO: TierConfig(
        name="pro",
        daily_budget=10.00,
        requests_per_hour=1000,
        requests_per_day=10000,
        tokens_per_minute=100000,
        enable_streaming=True,
        enable_batch=True,
        enable_embeddings=True,
        min_quality=0.75,
        target_quality=0.85,
        allowed_models=None,  # All models
        support_priority="priority"
    ),
    TierLevel.BUSINESS: TierConfig(
        name="business",
        daily_budget=50.00,
        requests_per_hour=5000,
        requests_per_day=50000,
        tokens_per_minute=500000,
        enable_streaming=True,
        enable_batch=True,
        enable_embeddings=True,
        min_quality=0.80,
        target_quality=0.90,
        support_priority="priority"
    ),
    TierLevel.ENTERPRISE: TierConfig(
        name="enterprise",
        daily_budget=None,  # Unlimited
        requests_per_hour=None,  # Unlimited
        requests_per_day=None,  # Unlimited
        tokens_per_minute=None,  # Unlimited
        enable_streaming=True,
        enable_batch=True,
        enable_embeddings=True,
        min_quality=0.85,
        target_quality=0.95,
        support_priority="dedicated"
    )
}
```

```python
# cascadeflow/profiles/user_profile.py
from dataclasses import dataclass, field
from typing import Optional
from datetime import datetime

@dataclass
class UserProfile:
    """
    Complete user profile for CascadeFlow.

    v0.2.1 Foundation: Focuses on tier, limits, basic preferences.
    v0.2.2+: Will add quality_preferences, advanced guardrails.
    v0.3.0+: Will add organization, workspace, compliance.
    """

    # 1. Identity (Who)
    user_id: str
    created_at: datetime = field(default_factory=datetime.utcnow)

    # 2. Tier (Subscription Level) - ONE subcategory
    tier: TierConfig = field(default_factory=lambda: TierConfig.from_preset(TierLevel.FREE))

    # 3. Limits (What They Can Do) - Inherited from tier by default
    # Can override tier defaults
    custom_daily_budget: Optional[float] = None
    custom_requests_per_hour: Optional[int] = None

    # 4. Preferences (How They Want It) - v0.2.1 basic, v0.2.2 advanced
    preferred_models: Optional[List[str]] = None

    # 5. Guardrails (Safety & Compliance) - v0.2.1 basic flags
    enable_content_moderation: bool = False
    enable_pii_detection: bool = False

    # 6. Organization (Multi-Tenant) - v0.3.0
    # organization: Optional[OrganizationConfig] = None  # Coming in v0.3.0

    # 7. Telemetry (Observability) - v0.2.1 basic
    metadata: dict = field(default_factory=dict)

    @classmethod
    def from_tier(cls, tier: TierLevel, user_id: str, **kwargs) -> 'UserProfile':
        """
        Simple factory: Create profile from tier preset.

        Example:
            profile = UserProfile.from_tier(TierLevel.PRO, user_id="user_123")
        """
        tier_config = TierConfig.from_preset(tier)
        return cls(user_id=user_id, tier=tier_config, **kwargs)

    def get_daily_budget(self) -> Optional[float]:
        """Get effective daily budget (custom or tier default)"""
        return self.custom_daily_budget if self.custom_daily_budget is not None else self.tier.daily_budget

    def get_requests_per_hour(self) -> Optional[int]:
        """Get effective rate limit (custom or tier default)"""
        return self.custom_requests_per_hour if self.custom_requests_per_hour is not None else self.tier.requests_per_hour

    def to_dict(self) -> dict:
        """Serialize to dict for storage"""
        return {
            "user_id": self.user_id,
            "created_at": self.created_at.isoformat(),
            "tier": {
                "name": self.tier.name,
                "daily_budget": self.tier.daily_budget,
                "requests_per_hour": self.tier.requests_per_hour,
                # ... all tier fields
            },
            "custom_daily_budget": self.custom_daily_budget,
            "custom_requests_per_hour": self.custom_requests_per_hour,
            "preferred_models": self.preferred_models,
            "enable_content_moderation": self.enable_content_moderation,
            "enable_pii_detection": self.enable_pii_detection,
            "metadata": self.metadata
        }

    @classmethod
    def from_dict(cls, data: dict) -> 'UserProfile':
        """Deserialize from dict"""
        # Implementation here
        pass
```

```python
# cascadeflow/profiles/profile_manager.py
from typing import Dict, Optional, Callable, Awaitable
import asyncio
from datetime import datetime, timedelta

class UserProfileManager:
    """
    Manage user profiles at scale (thousands of users).

    Features:
    - In-memory caching (configurable TTL)
    - Database integration (via callback)
    - Bulk operations
    - Tier upgrades/downgrades
    """

    def __init__(
        self,
        cache_ttl_seconds: int = 300,  # 5 minutes
        load_callback: Optional[Callable[[str], Awaitable[Optional[UserProfile]]]] = None,
        save_callback: Optional[Callable[[UserProfile], Awaitable[None]]] = None
    ):
        """
        Initialize profile manager.

        Args:
            cache_ttl_seconds: How long to cache profiles in memory
            load_callback: Async function to load profile from database
            save_callback: Async function to save profile to database
        """
        self._cache: Dict[str, tuple[UserProfile, datetime]] = {}
        self._cache_ttl = timedelta(seconds=cache_ttl_seconds)
        self._load_callback = load_callback
        self._save_callback = save_callback
        self._lock = asyncio.Lock()

    async def get_profile(self, user_id: str) -> UserProfile:
        """
        Get user profile (from cache or load).

        Fast path: Cached profile (microseconds)
        Slow path: Load from database (milliseconds)
        Default path: Create free tier profile (microseconds)
        """
        # Check cache
        if user_id in self._cache:
            profile, cached_at = self._cache[user_id]
            if datetime.utcnow() - cached_at < self._cache_ttl:
                return profile

        # Load from database
        async with self._lock:
            if self._load_callback:
                profile = await self._load_callback(user_id)
                if profile:
                    self._cache[user_id] = (profile, datetime.utcnow())
                    return profile

        # Default: Create free tier profile
        profile = UserProfile.from_tier(TierLevel.FREE, user_id=user_id)
        self._cache[user_id] = (profile, datetime.utcnow())
        return profile

    async def save_profile(self, profile: UserProfile) -> None:
        """Save profile to database and cache"""
        self._cache[profile.user_id] = (profile, datetime.utcnow())
        if self._save_callback:
            await self._save_callback(profile)

    async def update_tier(self, user_id: str, new_tier: TierLevel) -> UserProfile:
        """Upgrade/downgrade user tier"""
        profile = await self.get_profile(user_id)
        profile.tier = TierConfig.from_preset(new_tier)
        await self.save_profile(profile)
        return profile

    def invalidate_cache(self, user_id: str) -> None:
        """Invalidate cached profile (e.g., after tier change)"""
        if user_id in self._cache:
            del self._cache[user_id]

    def create_bulk(self, user_data: List[dict]) -> List[UserProfile]:
        """Create multiple profiles efficiently"""
        profiles = []
        for data in user_data:
            tier = TierLevel(data.get("tier", "free"))
            profile = UserProfile.from_tier(tier, user_id=data["user_id"])
            profiles.append(profile)
            self._cache[profile.user_id] = (profile, datetime.utcnow())
        return profiles
```

**CascadeAgent Integration**:

```python
# cascadeflow/agent.py (add new method)

class CascadeAgent:
    # ... existing methods ...

    @classmethod
    def from_profile(cls, profile: UserProfile, **kwargs) -> 'CascadeAgent':
        """
        Create CascadeAgent from UserProfile.

        This is the recommended way to create agents in production SaaS apps.

        Example:
            profile = UserProfile.from_tier(TierLevel.PRO, user_id="user_123")
            agent = CascadeAgent.from_profile(profile)
            result = await agent.run("Your query")

        The agent will automatically:
        - Enforce tier limits (budget, rate limits)
        - Enable/disable features (streaming, batch, embeddings)
        - Apply quality targets
        - Enable guardrails if configured
        """
        # Convert UserProfile to CascadeAgent config
        config = {
            "user_id": profile.user_id,
            "daily_budget": profile.get_daily_budget(),
            "requests_per_hour": profile.get_requests_per_hour(),
            "min_quality": profile.tier.min_quality,
            "target_quality": profile.tier.target_quality,
            "enable_streaming": profile.tier.enable_streaming,
            "enable_batch": profile.tier.enable_batch,
            "allowed_models": profile.tier.allowed_models,
            "blocked_models": profile.tier.blocked_models,
            "preferred_models": profile.preferred_models,
            "enable_content_moderation": profile.enable_content_moderation,
            "enable_pii_detection": profile.enable_pii_detection,
            **kwargs
        }
        return cls(**config)
```

**Tests Required**:
- âœ… Create profile from tier preset (all 5 tiers)
- âœ… Create profile with custom overrides
- âœ… ProfileManager caching (hit/miss)
- âœ… ProfileManager bulk creation (1000 users)
- âœ… ProfileManager tier upgrade
- âœ… CascadeAgent.from_profile() with FREE tier
- âœ… CascadeAgent.from_profile() with PRO tier
- âœ… CascadeAgent.from_profile() with ENTERPRISE tier
- âœ… Serialization/deserialization (to_dict, from_dict)

**Success Criteria**:
- All tests pass
- Profile creation <1ms (from preset)
- ProfileManager get_profile <1ms (cached)
- ProfileManager get_profile <10ms (database load)
- Bulk creation: 1000 profiles in <100ms
- CascadeAgent.from_profile() works correctly
- Backwards compatible (v0.2.0 code still works)

---

### Milestone 4: Rate Limiting (2-3 days)
**Goal**: Per-user, per-tier rate limiting with sliding window

**Files to Create**:
- `cascadeflow/limits/__init__.py` - Module exports
- `cascadeflow/limits/rate_limiter.py` - Rate limiting logic
- `cascadeflow/limits/sliding_window.py` - Sliding window implementation
- `tests/test_rate_limiting.py` - Comprehensive rate limit tests
- `examples/rate_limiting_usage.py` - Working example

**Why NOT Use LiteLLM Proxy Rate Limiting**:
- âŒ LiteLLM proxy rate limiting is global, not per-user/per-tier
- âŒ Requires proxy infrastructure (we don't want to manage that)
- âŒ Not cascade-aware (can't do graceful degradation)

**Our Implementation**:
```python
# cascadeflow/limits/rate_limiter.py
from typing import Dict, Optional
from datetime import datetime, timedelta
import asyncio

class RateLimiter:
    """
    Per-user rate limiter with sliding window.

    Features:
    - Sliding window (more accurate than fixed window)
    - Per-user tracking
    - Per-tier limits
    - Async-safe (thread-safe)
    - Graceful degradation (warn vs block)
    """

    def __init__(self):
        self._user_requests: Dict[str, list[datetime]] = {}
        self._lock = asyncio.Lock()

    async def check_rate_limit(
        self,
        user_id: str,
        limit: int,
        window_seconds: int = 3600,  # 1 hour
        mode: str = "block"  # "block" or "warn"
    ) -> tuple[bool, Optional[str]]:
        """
        Check if user is within rate limit.

        Returns:
            (allowed: bool, message: Optional[str])

        Examples:
            allowed, msg = await limiter.check_rate_limit("user_123", limit=100, window_seconds=3600)
            if not allowed:
                raise RateLimitExceeded(msg)
        """
        async with self._lock:
            now = datetime.utcnow()
            window_start = now - timedelta(seconds=window_seconds)

            # Get user's request history
            if user_id not in self._user_requests:
                self._user_requests[user_id] = []

            # Remove old requests (outside window)
            self._user_requests[user_id] = [
                req_time for req_time in self._user_requests[user_id]
                if req_time > window_start
            ]

            # Check limit
            current_count = len(self._user_requests[user_id])

            if current_count >= limit:
                if mode == "block":
                    oldest = self._user_requests[user_id][0]
                    retry_after = (oldest + timedelta(seconds=window_seconds) - now).total_seconds()
                    return False, f"Rate limit exceeded. Retry after {retry_after:.0f} seconds."
                elif mode == "warn":
                    # Log warning but allow (for gradual rollout)
                    return True, f"Warning: Rate limit exceeded ({current_count}/{limit})"

            # Record this request
            self._user_requests[user_id].append(now)
            return True, None
```

**Integration with CascadeAgent**:
```python
# cascadeflow/agent.py (add rate limiting)

class CascadeAgent:
    def __init__(self, ..., rate_limiter: Optional[RateLimiter] = None):
        self.rate_limiter = rate_limiter or RateLimiter()
        # ... existing init ...

    async def run(self, query: str, **kwargs):
        # Check rate limit before processing
        if self.profile:
            limit = self.profile.get_requests_per_hour()
            if limit:
                allowed, msg = await self.rate_limiter.check_rate_limit(
                    user_id=self.profile.user_id,
                    limit=limit,
                    window_seconds=3600,
                    mode="block"
                )
                if not allowed:
                    raise RateLimitExceeded(msg)

        # Continue with normal processing
        return await self._run_internal(query, **kwargs)
```

**Tests Required**:
- âœ… Rate limit: 10 requests in 1 hour (FREE tier)
- âœ… Rate limit: 100 requests in 1 hour (STARTER tier)
- âœ… Rate limit: 1000 requests in 1 hour (PRO tier)
- âœ… Rate limit: Sliding window accuracy
- âœ… Rate limit: Multiple users (isolation)
- âœ… Rate limit: Warn mode (soft limit)
- âœ… Rate limit: Block mode (hard limit)
- âœ… Rate limit: Retry-after header

**Success Criteria**:
- All tests pass
- Accurate sliding window (within 1% error)
- Per-user isolation (no cross-user interference)
- <1ms overhead per request
- Memory efficient (automatic cleanup)

---

### Milestone 5: Basic Guardrails (3-4 days)
**Goal**: Content moderation and PII detection with LiteLLM + fallback

**Files to Create**:
- `cascadeflow/guardrails/__init__.py` - Module exports
- `cascadeflow/guardrails/content_moderation.py` - Content moderation
- `cascadeflow/guardrails/pii_detection.py` - PII detection
- `cascadeflow/guardrails/config.py` - Guardrail configuration
- `tests/test_guardrails.py` - Comprehensive guardrail tests
- `examples/guardrails_usage.py` - Working example

**Why Use LiteLLM for Guardrails (Optional)**:
- âœ… LiteLLM supports OpenAI Moderation API natively
- âœ… Can use cheap models (GPT-3.5) for PII detection
- âœ… Unified API for multiple providers

**Fallback Strategy**:
- âœ… Regex-based content moderation (if no LiteLLM)
- âœ… Regex-based PII detection (if no LiteLLM)

**Implementation**:

```python
# cascadeflow/guardrails/content_moderation.py
from typing import Optional, List, Dict
import re

class ContentModerator:
    """
    Content moderation with LiteLLM + fallback.

    Detects:
    - Hate speech
    - Violence
    - Sexual content
    - Self-harm
    """

    def __init__(self, use_litellm: bool = True):
        self.use_litellm = use_litellm
        self._init_regex_patterns()

    def _init_regex_patterns(self):
        """Initialize regex patterns for fallback"""
        self.patterns = {
            "hate": [
                r"\b(hate|racist|sexist|homophobic)\b",
                # ... more patterns
            ],
            "violence": [
                r"\b(kill|murder|attack|hurt)\b",
                # ... more patterns
            ],
            # ... other categories
        }

    async def check(self, text: str) -> Dict[str, any]:
        """
        Check text for policy violations.

        Returns:
            {
                "flagged": bool,
                "categories": {
                    "hate": bool,
                    "violence": bool,
                    ...
                },
                "action": "allow" | "warn" | "block",
                "message": Optional[str]
            }
        """
        if self.use_litellm:
            try:
                return await self._check_litellm(text)
            except Exception as e:
                # Fallback to regex
                return self._check_regex(text)
        else:
            return self._check_regex(text)

    async def _check_litellm(self, text: str) -> Dict:
        """Use LiteLLM OpenAI Moderation API"""
        try:
            import litellm
            response = await litellm.amoderation(input=text)
            result = response.results[0]

            return {
                "flagged": result.flagged,
                "categories": {
                    "hate": result.categories.hate,
                    "violence": result.categories.violence,
                    "sexual": result.categories.sexual,
                    "self_harm": result.categories.self_harm
                },
                "action": "block" if result.flagged else "allow",
                "message": "Content policy violation" if result.flagged else None
            }
        except ImportError:
            # LiteLLM not installed, fallback
            return self._check_regex(text)

    def _check_regex(self, text: str) -> Dict:
        """Fallback: Regex-based moderation"""
        flagged_categories = {}

        for category, patterns in self.patterns.items():
            flagged_categories[category] = any(
                re.search(pattern, text, re.IGNORECASE)
                for pattern in patterns
            )

        flagged = any(flagged_categories.values())

        return {
            "flagged": flagged,
            "categories": flagged_categories,
            "action": "warn" if flagged else "allow",  # Regex is less accurate, warn only
            "message": "Potential content policy violation (regex detection)" if flagged else None
        }
```

```python
# cascadeflow/guardrails/pii_detection.py
from typing import Dict, List
import re

class PIIDetector:
    """
    PII detection with LiteLLM + fallback.

    Detects:
    - Email addresses
    - Phone numbers
    - Credit card numbers
    - Social Security Numbers
    - IP addresses
    """

    def __init__(self, use_litellm: bool = True, action: str = "redact"):
        """
        Initialize PII detector.

        Args:
            use_litellm: Use LiteLLM for detection (more accurate)
            action: "redact" | "warn" | "block"
        """
        self.use_litellm = use_litellm
        self.action = action
        self._init_regex_patterns()

    def _init_regex_patterns(self):
        """Initialize regex patterns for PII detection"""
        self.patterns = {
            "email": r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",
            "phone": r"\b(\+?1[-.]?)?\(?\d{3}\)?[-.]?\d{3}[-.]?\d{4}\b",
            "credit_card": r"\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b",
            "ssn": r"\b\d{3}-\d{2}-\d{4}\b",
            "ip_address": r"\b(?:\d{1,3}\.){3}\d{1,3}\b"
        }

    async def check(self, text: str) -> Dict[str, any]:
        """
        Check text for PII.

        Returns:
            {
                "pii_found": bool,
                "types": ["email", "phone", ...],
                "redacted_text": str,
                "action": "allow" | "warn" | "block",
                "message": Optional[str]
            }
        """
        if self.use_litellm:
            try:
                return await self._check_litellm(text)
            except Exception:
                return self._check_regex(text)
        else:
            return self._check_regex(text)

    async def _check_litellm(self, text: str) -> Dict:
        """Use LiteLLM with cheap model for PII detection"""
        try:
            import litellm

            prompt = f"""Detect PII in the following text. Return JSON:
{{"pii_found": bool, "types": ["email", "phone", ...], "redacted_text": "text with PII redacted"}}

Text: {text}"""

            response = await litellm.acompletion(
                model="gpt-3.5-turbo",  # Cheap model
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)

            return {
                "pii_found": result["pii_found"],
                "types": result["types"],
                "redacted_text": result["redacted_text"],
                "action": self.action if result["pii_found"] else "allow",
                "message": f"PII detected: {', '.join(result['types'])}" if result["pii_found"] else None
            }
        except ImportError:
            return self._check_regex(text)

    def _check_regex(self, text: str) -> Dict:
        """Fallback: Regex-based PII detection"""
        pii_types = []
        redacted_text = text

        for pii_type, pattern in self.patterns.items():
            matches = re.finditer(pattern, text)
            for match in matches:
                pii_types.append(pii_type)
                redacted_text = redacted_text.replace(match.group(0), f"[REDACTED_{pii_type.upper()}]")

        return {
            "pii_found": len(pii_types) > 0,
            "types": list(set(pii_types)),
            "redacted_text": redacted_text,
            "action": self.action if pii_types else "allow",
            "message": f"PII detected: {', '.join(set(pii_types))}" if pii_types else None
        }
```

**Integration with CascadeAgent**:
```python
# cascadeflow/agent.py (add guardrails)

class CascadeAgent:
    def __init__(
        self,
        ...,
        content_moderator: Optional[ContentModerator] = None,
        pii_detector: Optional[PIIDetector] = None
    ):
        self.content_moderator = content_moderator
        self.pii_detector = pii_detector
        # ... existing init ...

    async def run(self, query: str, **kwargs):
        # Check content moderation (if enabled)
        if self.profile and self.profile.enable_content_moderation:
            if not self.content_moderator:
                self.content_moderator = ContentModerator()

            mod_result = await self.content_moderator.check(query)
            if mod_result["action"] == "block":
                raise ContentPolicyViolation(mod_result["message"])

        # Check PII (if enabled)
        if self.profile and self.profile.enable_pii_detection:
            if not self.pii_detector:
                self.pii_detector = PIIDetector(action="redact")

            pii_result = await self.pii_detector.check(query)
            if pii_result["pii_found"]:
                if pii_result["action"] == "redact":
                    query = pii_result["redacted_text"]
                elif pii_result["action"] == "block":
                    raise PIIDetected(pii_result["message"])

        # Continue with normal processing
        return await self._run_internal(query, **kwargs)
```

**Tests Required**:
- âœ… Content moderation: Hate speech (LiteLLM)
- âœ… Content moderation: Violence (LiteLLM)
- âœ… Content moderation: Fallback (regex)
- âœ… PII detection: Email (LiteLLM)
- âœ… PII detection: Phone (LiteLLM)
- âœ… PII detection: Credit card (regex fallback)
- âœ… PII detection: Redaction
- âœ… PII detection: Block mode
- âœ… Guardrails with UserProfile integration

**Success Criteria**:
- All tests pass
- LiteLLM moderation works (when available)
- Regex fallback works (when LiteLLM unavailable)
- Redaction accurate (no PII leakage)
- <100ms overhead per request (LiteLLM)
- <10ms overhead per request (regex)

---

## ğŸ§ª Testing Strategy

### Per-Milestone Testing
Each milestone must pass ALL tests before moving to next milestone.

**Test Categories**:
1. **Unit Tests** - Individual components (85%+ coverage)
2. **Integration Tests** - Multiple components working together
3. **End-to-End Tests** - Full user workflows
4. **Performance Tests** - Latency, throughput, memory
5. **Regression Tests** - v0.2.0 features still work

### Test Requirements

**Milestone 1: Batch Completion**
- 15+ unit tests
- 5+ integration tests
- 2+ E2E tests
- Performance: 10 queries in <5s (LiteLLM), <15s (fallback)

**Milestone 2: Async Streaming**
- 12+ unit tests
- 4+ integration tests
- 2+ E2E tests
- Performance: First token <500ms

**Milestone 3: User Profiles**
- 20+ unit tests
- 8+ integration tests
- 3+ E2E tests
- Performance: Profile creation <1ms, get_profile <10ms

**Milestone 4: Rate Limiting**
- 10+ unit tests
- 5+ integration tests
- 2+ E2E tests
- Performance: <1ms overhead per request

**Milestone 5: Guardrails**
- 15+ unit tests
- 6+ integration tests
- 3+ E2E tests
- Performance: <100ms overhead (LiteLLM), <10ms (regex)

**Total v0.2.1 Tests**: 72+ unit tests, 28+ integration tests, 12+ E2E tests

---

## ğŸ“ˆ Success Metrics

### Feature Completeness
- âœ… Batch completion: 100% (with LiteLLM + fallback)
- âœ… Async streaming: 100% (CascadeAgent integration)
- âœ… User profiles: 100% (foundation, 7 dimensions planned)
- âœ… Rate limiting: 100% (per-user, per-tier)
- âœ… Guardrails: 80% (basic content moderation + PII)

### Performance Targets
| Metric | Target | Validation |
|--------|--------|------------|
| Batch (10 queries, LiteLLM) | <5s | Benchmark |
| Batch (10 queries, fallback) | <15s | Benchmark |
| Streaming first token | <500ms | Benchmark |
| Profile creation | <1ms | Unit test |
| Profile get (cached) | <1ms | Unit test |
| Profile get (DB load) | <10ms | Integration test |
| Rate limit check | <1ms | Unit test |
| Content moderation (LiteLLM) | <100ms | Integration test |
| Content moderation (regex) | <10ms | Unit test |
| PII detection (LiteLLM) | <100ms | Integration test |
| PII detection (regex) | <5ms | Unit test |

### Developer Experience
| Metric | Target | Validation |
|--------|--------|------------|
| Simple usage (1 line) | `UserProfile.from_tier()` | Example |
| Advanced usage complexity | <10 lines | Example |
| Backwards compatibility | 100% | v0.2.0 tests pass |
| Documentation coverage | 95%+ | All features documented |
| Example coverage | 100% | All features have examples |

### Production Readiness
| Metric | Target | Validation |
|--------|--------|------------|
| Test coverage | 85%+ | pytest-cov |
| Memory efficiency | <10MB per 1000 users | Load test |
| Scale | 10,000 users | Load test |
| Error handling | 100% graceful | Error tests |
| Monitoring | Full telemetry | Metrics |

---

## ğŸ”„ Backwards Compatibility

### v0.2.0 Code Still Works (Zero Breaking Changes)

```python
# v0.2.0 code (still works in v0.2.1)
from cascadeflow import CascadeAgent
from cascadeflow.telemetry import CostTracker, BudgetConfig

tracker = CostTracker(
    user_budgets={'user_123': BudgetConfig(daily=1.00)}
)

agent = CascadeAgent(cost_tracker=tracker)
result = await agent.run("Your query")  # âœ… Works unchanged
```

```python
# v0.2.1 new way (recommended)
from cascadeflow import CascadeAgent, UserProfile, TierLevel

profile = UserProfile.from_tier(TierLevel.PRO, user_id="user_123")
agent = CascadeAgent.from_profile(profile)
result = await agent.run("Your query")  # âœ… New, simplified API
```

**Migration Path**:
1. v0.2.0 code works unchanged (no migration required)
2. Gradually adopt UserProfile (optional, recommended)
3. Advanced features available when needed

---

## ğŸ“¦ Dependencies

### Required (No Changes)
All v0.2.0 dependencies remain unchanged:
- `litellm>=1.0.0` - Provider abstraction, cost calculation
- `fastembed>=0.2.0` - Embeddings
- `diskcache>=5.6.0` - Caching
- `pydantic>=2.0.0` - Data validation

### Optional (New for v0.2.1)
No new required dependencies. All new features work with existing dependencies.

**Optional enhancements**:
- LiteLLM batch completion (already in `litellm>=1.0.0`)
- OpenAI Moderation API (via LiteLLM, optional)

---

## ğŸš€ Deployment Plan

### Week 4: Batch & Streaming
- **Days 1-2**: Implement Milestone 1 (Batch Completion)
- **Days 3-4**: Implement Milestone 2 (Async Streaming)
- **Day 5**: Testing, validation, commit to GitHub

### Week 5: User Profiles
- **Days 1-3**: Implement Milestone 3 (User Profile Foundation)
- **Days 4-5**: Testing, validation, commit to GitHub

### Week 6: Limits & Guardrails
- **Days 1-2**: Implement Milestone 4 (Rate Limiting)
- **Days 3-4**: Implement Milestone 5 (Basic Guardrails)
- **Day 5**: Final testing, validation, commit to GitHub

### Release Day (End of Week 6)
- Final review
- Documentation update
- PyPI publish
- GitHub release announcement

---

## ğŸ“š Documentation Plan

### New Documentation to Create
1. **User Profiles Guide** (`docs/guides/user_profiles.md`)
   - Simple usage with tier presets
   - Advanced usage with custom configs
   - Production best practices
   - Multi-tenant setup

2. **Batch Processing Guide** (`docs/guides/batch_processing.md`)
   - When to use batch
   - Performance comparison
   - Cost optimization

3. **Streaming Guide Update** (`docs/guides/streaming.md`)
   - New CascadeAgent.stream() API
   - Real-time use cases
   - Performance tips

4. **Rate Limiting Guide** (`docs/guides/rate_limiting.md`)
   - Per-user, per-tier limits
   - Graceful degradation
   - Monitoring

5. **Guardrails Guide** (`docs/guides/guardrails.md`)
   - Content moderation setup
   - PII detection and redaction
   - Compliance requirements

### Examples to Create
1. `examples/batch_processing.py` - Batch completion example
2. `examples/streaming_example.py` - Async streaming example
3. `examples/user_profile_simple.py` - Simple tier usage
4. `examples/user_profile_advanced.py` - Advanced multi-tenant
5. `examples/rate_limiting_example.py` - Rate limiting setup
6. `examples/guardrails_example.py` - Content moderation + PII

### Migration Guide Update
Update `docs/MIGRATION_GUIDE_V0.2.0.md` with:
- v0.2.0 â†’ v0.2.1 migration section
- UserProfile adoption guide
- New features overview

---

## âš ï¸ Risks & Mitigation

### Risk 1: LiteLLM Batch API Changes
**Probability**: Low
**Impact**: Medium
**Mitigation**: Comprehensive fallback implementation (sequential processing)

### Risk 2: Performance Regression
**Probability**: Medium
**Impact**: High
**Mitigation**: Extensive benchmarking, performance tests, monitoring

### Risk 3: Memory Usage with Thousands of Users
**Probability**: Medium
**Impact**: Medium
**Mitigation**: Cache TTL, automatic cleanup, memory profiling

### Risk 4: Guardrails Accuracy (Regex Fallback)
**Probability**: High
**Impact**: Medium
**Mitigation**: Warn mode for regex, recommend LiteLLM for production

### Risk 5: Rate Limiting Edge Cases
**Probability**: Medium
**Impact**: Medium
**Mitigation**: Comprehensive tests, sliding window implementation

---

## ğŸ¯ Definition of Done

### Milestone-Level DoD
For each milestone to be considered "done":
- âœ… All code implemented
- âœ… All tests passing (unit, integration, E2E)
- âœ… Performance benchmarks meet targets
- âœ… Documentation written
- âœ… Example code working
- âœ… Code review complete
- âœ… Committed to GitHub (feature branch)

### Release-Level DoD
For v0.2.1 to be released:
- âœ… All 5 milestones complete
- âœ… 100+ tests passing
- âœ… v0.2.0 regression tests passing (backwards compatibility)
- âœ… Performance benchmarks meet all targets
- âœ… Documentation complete (5 guides + 6 examples)
- âœ… Migration guide updated
- âœ… CHANGELOG.md updated
- âœ… GitHub release notes prepared
- âœ… PyPI package ready

---

## ğŸ“ Next Steps

### Immediate (Today)
1. âœ… **Plan complete** - This document
2. âœ… **Validate plan** - Review all milestones
3. âœ… **Create todo list** - Track all tasks
4. ğŸ”¨ **Start Milestone 1** - Batch completion implementation

### This Week (Week 4)
1. ğŸ”¨ **Complete Milestone 1** - Batch completion
2. ğŸ”¨ **Complete Milestone 2** - Async streaming
3. âœ… **Commit to GitHub** - feature/cost-control-quality-v2 branch

### Next 2 Weeks (Week 5-6)
1. ğŸ”¨ **Complete Milestone 3** - User profiles
2. ğŸ”¨ **Complete Milestone 4** - Rate limiting
3. ğŸ”¨ **Complete Milestone 5** - Guardrails
4. âœ… **Release v0.2.1** - PyPI publish

---

## ğŸ“Š Implementation Checklist

### Milestone 1: Batch Completion â³
- [ ] Create `cascadeflow/batch.py`
- [ ] Create `cascadeflow/batch_config.py`
- [ ] Implement LiteLLM batch integration
- [ ] Implement sequential fallback
- [ ] Add CascadeAgent.run_batch() method
- [ ] Write 15+ unit tests
- [ ] Write 5+ integration tests
- [ ] Write 2+ E2E tests
- [ ] Create `examples/batch_processing.py`
- [ ] Benchmark: 10 queries <5s (LiteLLM)
- [ ] Commit to GitHub

### Milestone 2: Async Streaming â³
- [ ] Modify `cascadeflow/agent.py` (add .stream())
- [ ] Integrate with cascade routing
- [ ] Implement quality validation on final chunk
- [ ] Add cost tracking for streaming
- [ ] Write 12+ unit tests
- [ ] Write 4+ integration tests
- [ ] Write 2+ E2E tests
- [ ] Create `examples/streaming_example.py`
- [ ] Benchmark: First token <500ms
- [ ] Commit to GitHub

### Milestone 3: User Profiles â³
- [ ] Create `cascadeflow/profiles/` module
- [ ] Implement `TierConfig` with 5 presets
- [ ] Implement `UserProfile` data structure
- [ ] Implement `UserProfileManager` with caching
- [ ] Add `CascadeAgent.from_profile()` method
- [ ] Write 20+ unit tests
- [ ] Write 8+ integration tests
- [ ] Write 3+ E2E tests
- [ ] Create `examples/user_profile_simple.py`
- [ ] Create `examples/user_profile_advanced.py`
- [ ] Benchmark: Profile creation <1ms
- [ ] Commit to GitHub

### Milestone 4: Rate Limiting â³
- [ ] Create `cascadeflow/limits/` module
- [ ] Implement `RateLimiter` (sliding window)
- [ ] Integrate with `CascadeAgent`
- [ ] Add UserProfile integration
- [ ] Write 10+ unit tests
- [ ] Write 5+ integration tests
- [ ] Write 2+ E2E tests
- [ ] Create `examples/rate_limiting_example.py`
- [ ] Benchmark: <1ms overhead
- [ ] Commit to GitHub

### Milestone 5: Guardrails â³
- [ ] Create `cascadeflow/guardrails/` module
- [ ] Implement `ContentModerator` (LiteLLM + regex)
- [ ] Implement `PIIDetector` (LiteLLM + regex)
- [ ] Integrate with `CascadeAgent`
- [ ] Add UserProfile integration
- [ ] Write 15+ unit tests
- [ ] Write 6+ integration tests
- [ ] Write 3+ E2E tests
- [ ] Create `examples/guardrails_example.py`
- [ ] Benchmark: <100ms overhead (LiteLLM)
- [ ] Commit to GitHub

### Documentation â³
- [ ] Create `docs/guides/user_profiles.md`
- [ ] Create `docs/guides/batch_processing.md`
- [ ] Update `docs/guides/streaming.md`
- [ ] Create `docs/guides/rate_limiting.md`
- [ ] Create `docs/guides/guardrails.md`
- [ ] Update `docs/MIGRATION_GUIDE_V0.2.0.md`
- [ ] Update `CHANGELOG.md`
- [ ] Create GitHub release notes

### Release â³
- [ ] All milestones complete
- [ ] All tests passing (100+)
- [ ] All benchmarks passing
- [ ] All documentation complete
- [ ] All examples working
- [ ] Final review
- [ ] Merge to main
- [ ] Tag v0.2.1
- [ ] Publish to PyPI
- [ ] GitHub release announcement

---

**Status**: âœ… PLAN COMPLETE - READY TO START IMPLEMENTATION
**Next Action**: Start Milestone 1 - Batch Completion
**Estimated Completion**: 3 weeks (18 working days)
**Total New Features**: 5 major features
**Total Tests**: 100+ tests
**Total Documentation**: 5 guides + 6 examples

---

**ğŸ‰ Let's build v0.2.1! ğŸš€**
