# TypeScript ML Implementation Plan
## Bringing Feature Parity with Python

**Goal:** Implement ML semantic detection in TypeScript to achieve feature parity with Python

**Status:** Ready for implementation
**Priority:** High
**Estimated Effort:** 12-16 hours

---

## Executive Summary

This plan brings TypeScript to feature parity with Python by implementing ML-based semantic domain detection using Transformers.js (Xenova/bge-small-en-v1.5). This will:

- âœ… Add ML semantic detection (84-87% confidence like Python)
- âœ… Add semantic validation (cosine similarity)
- âœ… Maintain graceful fallback to rule-based detection
- âœ… Work in Node.js, browser, and edge environments
- âœ… Update all documentation and dependencies

---

## Part 1: Research Findings

### Embedding Library Selection: **Transformers.js** âœ…

**Why Transformers.js:**
1. **Official Hugging Face library** - Well-maintained, actively developed
2. **Same model as Python** - Xenova/bge-small-en-v1.5 (ONNX-converted BAAI/bge-small-en-v1.5)
3. **Browser + Node.js support** - Works everywhere TypeScript runs
4. **No server required** - Runs locally, privacy-friendly
5. **ONNX-optimized** - Fast inference like Python's FastEmbed
6. **384 dimensions** - Exact match with Python implementation

**NPM Package:** `@xenova/transformers` (previously `@huggingface/transformers`)

**Model:** `Xenova/bge-small-en-v1.5`
- Size: ~40MB (same as Python)
- Dimensions: 384 (same as Python)
- Performance: Similar to Python FastEmbed

**Rejected Alternatives:**
- âŒ ONNX.js alone - Lower-level, requires more work
- âŒ TensorFlow.js - Larger bundle size, slower inference
- âŒ OpenAI embeddings API - Requires API calls, costs money, not local

---

## Part 2: Implementation Architecture

### New Files to Create

```
packages/
â”œâ”€â”€ ml/                                    # NEW: ML package
â”‚   â”œâ”€â”€ package.json                       # Package config
â”‚   â”œâ”€â”€ tsconfig.json                      # TypeScript config
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ index.ts                       # Exports
â”‚   â”‚   â”œâ”€â”€ embedding.ts                   # UnifiedEmbeddingService (port from Python)
â”‚   â”‚   â”œâ”€â”€ semantic.ts                    # Semantic utilities
â”‚   â”‚   â””â”€â”€ types.ts                       # ML-specific types
â”‚   â”œâ”€â”€ README.md                          # ML package docs
â”‚   â””â”€â”€ dist/                              # Built output
â”‚
â””â”€â”€ core/
    â””â”€â”€ src/
        â”œâ”€â”€ ml/                            # NEW: ML integration in core
        â”‚   â”œâ”€â”€ detector.ts                # SemanticDomainDetector
        â”‚   â””â”€â”€ validator.ts               # SemanticValidator
        â”œâ”€â”€ types.ts                       # UPDATE: Add ValidationMethod.SEMANTIC
        â”œâ”€â”€ validators.ts                  # UPDATE: Add semantic validation
        â”œâ”€â”€ config.ts                      # UPDATE: Add enableSemanticDetection
        â””â”€â”€ agent.ts                       # UPDATE: Use ML when available
```

### Architecture Decisions

1. **Separate `@cascadeflow/ml` Package** (Like Python's `cascadeflow[ml]`)
   - Optional dependency for core
   - Can be installed separately: `npm install @cascadeflow/ml`
   - Keeps core package small for users who don't need ML

2. **Lazy Loading** (Like Python)
   - Model only loads when first needed
   - Graceful degradation if package not installed

3. **Same API as Python** (Maximum compatibility)
   - `UnifiedEmbeddingService` class
   - `is_available` property
   - `embed()`, `embed_batch()`, `similarity()` methods
   - `EmbeddingCache` for request-scoped caching

4. **Browser + Node.js Support**
   - Use dynamic imports for Transformers.js
   - Works in edge functions (Vercel, Cloudflare Workers)
   - Model auto-downloads on first use

---

## Part 3: Implementation Steps

### Step 1: Create `@cascadeflow/ml` Package (2-3 hours)

**1.1: Package Structure**

Create `packages/ml/package.json`:
```json
{
  "name": "@cascadeflow/ml",
  "version": "0.1.1",
  "description": "ML semantic detection for CascadeFlow TypeScript",
  "main": "./dist/index.js",
  "module": "./dist/index.mjs",
  "types": "./dist/index.d.ts",
  "exports": {
    ".": {
      "import": "./dist/index.mjs",
      "require": "./dist/index.js",
      "types": "./dist/index.d.ts"
    }
  },
  "files": ["dist", "README.md"],
  "scripts": {
    "build": "tsup src/index.ts --format cjs,esm --dts --clean",
    "dev": "tsup src/index.ts --format cjs,esm --dts --watch",
    "test": "vitest run",
    "typecheck": "tsc --noEmit"
  },
  "keywords": [
    "ai",
    "ml",
    "embeddings",
    "semantic-search",
    "transformers",
    "bge",
    "cascadeflow"
  ],
  "dependencies": {
    "@xenova/transformers": "^2.17.0"
  },
  "peerDependencies": {
    "@cascadeflow/core": "^0.1.1"
  },
  "peerDependenciesMeta": {
    "@cascadeflow/core": {
      "optional": false
    }
  },
  "devDependencies": {
    "@types/node": "^20.10.0",
    "tsup": "^8.0.1",
    "typescript": "^5.3.3",
    "vitest": "^1.0.4"
  },
  "engines": {
    "node": ">=18.0.0"
  }
}
```

**1.2: UnifiedEmbeddingService (Port from Python)**

Create `packages/ml/src/embedding.ts`:
```typescript
/**
 * Unified Embedding Service for CascadeFlow TypeScript
 *
 * Port of cascadeflow/ml/embedding.py to TypeScript
 * Uses Transformers.js with Xenova/bge-small-en-v1.5 (same model as Python)
 */

import { pipeline, Pipeline } from '@xenova/transformers';

export interface EmbeddingVector {
  data: Float32Array;
  dimensions: number;
}

export class UnifiedEmbeddingService {
  private modelName: string;
  private embedder: Pipeline | null = null;
  private _isAvailable: boolean | null = null;
  private initializeAttempted: boolean = false;

  constructor(modelName: string = 'Xenova/bge-small-en-v1.5') {
    this.modelName = modelName;
  }

  /**
   * Check if embedding service is available
   * Lazy initialization on first access
   */
  get isAvailable(): boolean {
    if (this._isAvailable === null && !this.initializeAttempted) {
      // Trigger lazy initialization
      this.lazyInitialize();
    }
    return this._isAvailable || false;
  }

  /**
   * Lazy initialization - only loads model when first needed
   * Defers ~200-500ms model load time until first use
   */
  private async lazyInitialize(): Promise<void> {
    if (this.initializeAttempted) {
      return;
    }

    this.initializeAttempted = true;

    try {
      console.log(`Loading embedding model: ${this.modelName}`);

      // Load the feature extraction pipeline
      this.embedder = await pipeline('feature-extraction', this.modelName, {
        quantized: true,  // Use quantized model for speed
      });

      this._isAvailable = true;
      console.log('Embedding service initialized successfully');

    } catch (error) {
      console.warn(
        'Transformers.js not available. Install with: npm install @xenova/transformers',
        error
      );
      this._isAvailable = false;
    }
  }

  /**
   * Get embedding for a single text
   *
   * @param text Text to embed
   * @returns 384-dimensional embedding vector, or null if service unavailable
   */
  async embed(text: string): Promise<EmbeddingVector | null> {
    // Ensure initialization
    await this.lazyInitialize();

    if (!this._isAvailable || !this.embedder) {
      return null;
    }

    try {
      // Generate embedding
      const output = await this.embedder(text, {
        pooling: 'mean',
        normalize: true,
      });

      // Extract Float32Array from tensor
      const embedding = output.data as Float32Array;

      return {
        data: embedding,
        dimensions: embedding.length,  // Should be 384
      };

    } catch (error) {
      console.error('Error generating embedding:', error);
      return null;
    }
  }

  /**
   * Get embeddings for multiple texts (batching for efficiency)
   * Batching is ~30% faster than individual calls
   *
   * @param texts List of texts to embed
   * @returns List of embedding vectors, or null if service unavailable
   */
  async embedBatch(texts: string[]): Promise<EmbeddingVector[] | null> {
    // Ensure initialization
    await this.lazyInitialize();

    if (!this._isAvailable || !this.embedder) {
      return null;
    }

    try {
      const embeddings: EmbeddingVector[] = [];

      // Process each text (Transformers.js doesn't support true batching yet)
      for (const text of texts) {
        const output = await this.embedder(text, {
          pooling: 'mean',
          normalize: true,
        });

        const embedding = output.data as Float32Array;
        embeddings.push({
          data: embedding,
          dimensions: embedding.length,
        });
      }

      return embeddings;

    } catch (error) {
      console.error('Error generating batch embeddings:', error);
      return null;
    }
  }

  /**
   * Compute cosine similarity between two texts
   *
   * @param text1 First text
   * @param text2 Second text
   * @returns Similarity score [0.0, 1.0], or null if service unavailable
   */
  async similarity(text1: string, text2: string): Promise<number | null> {
    // Use batch embedding for efficiency
    const embeddings = await this.embedBatch([text1, text2]);
    if (!embeddings || embeddings.length !== 2) {
      return null;
    }

    return this.cosineSimilarity(embeddings[0].data, embeddings[1].data);
  }

  /**
   * Compute cosine similarity between two vectors
   *
   * @param vec1 First embedding vector
   * @param vec2 Second embedding vector
   * @returns Similarity score [0.0, 1.0]
   */
  private cosineSimilarity(vec1: Float32Array, vec2: Float32Array): number {
    if (vec1.length !== vec2.length) {
      throw new Error('Vectors must have same dimensions');
    }

    // Compute dot product
    let dotProduct = 0;
    let norm1 = 0;
    let norm2 = 0;

    for (let i = 0; i < vec1.length; i++) {
      dotProduct += vec1[i] * vec2[i];
      norm1 += vec1[i] * vec1[i];
      norm2 += vec2[i] * vec2[i];
    }

    // Normalize
    const magnitude = Math.sqrt(norm1) * Math.sqrt(norm2);
    if (magnitude === 0) {
      return 0;
    }

    const similarity = dotProduct / magnitude;

    // Clamp to [0, 1] (cosine can be [-1, 1], but we only care about positive)
    return Math.max(0, Math.min(1, similarity));
  }
}

/**
 * Request-scoped cache for embeddings
 * Reduces latency by 50% when same text embedded multiple times
 */
export class EmbeddingCache {
  private embedder: UnifiedEmbeddingService;
  private cache: Map<string, EmbeddingVector> = new Map();

  constructor(embedder: UnifiedEmbeddingService) {
    this.embedder = embedder;
  }

  /**
   * Get embedding from cache or compute if not cached
   */
  async getOrEmbed(text: string): Promise<EmbeddingVector | null> {
    if (this.cache.has(text)) {
      return this.cache.get(text)!;
    }

    const embedding = await this.embedder.embed(text);
    if (embedding) {
      this.cache.set(text, embedding);
    }

    return embedding;
  }

  /**
   * Compute similarity with caching
   */
  async similarity(text1: string, text2: string): Promise<number | null> {
    const emb1 = await this.getOrEmbed(text1);
    const emb2 = await this.getOrEmbed(text2);

    if (!emb1 || !emb2) {
      return null;
    }

    return this.embedder['cosineSimilarity'](emb1.data, emb2.data);
  }

  /**
   * Clear the cache (e.g., at end of request)
   */
  clear(): void {
    this.cache.clear();
  }

  /**
   * Get number of cached embeddings
   */
  cacheSize(): number {
    return this.cache.size;
  }

  /**
   * Get cache statistics
   */
  cacheInfo(): { size: number; texts: string[] } {
    return {
      size: this.cache.size,
      texts: Array.from(this.cache.keys()).slice(0, 5),  // First 5 for debugging
    };
  }
}
```

**1.3: Export from Package**

Create `packages/ml/src/index.ts`:
```typescript
/**
 * @cascadeflow/ml - ML semantic detection for CascadeFlow TypeScript
 *
 * Optional package providing ML-based domain detection and semantic validation
 *
 * Install: npm install @cascadeflow/ml
 *
 * @example
 * ```typescript
 * import { CascadeAgent } from '@cascadeflow/core';
 * import '@cascadeflow/ml';  // Opt-in to ML features
 *
 * const agent = new CascadeAgent({
 *   models: [...],
 *   enableSemanticDetection: true  // Now uses ML
 * });
 * ```
 */

export {
  UnifiedEmbeddingService,
  EmbeddingCache,
  type EmbeddingVector,
} from './embedding';

export {
  computeSemanticSimilarity,
  detectDomainSemanticly,
} from './semantic';

export type {
  SemanticDetectionResult,
  DomainExemplar,
} from './types';
```

**Validation:**
- [ ] Package builds successfully (`pnpm build`)
- [ ] UnifiedEmbeddingService loads model correctly
- [ ] Embeddings generated are 384 dimensions
- [ ] Cosine similarity works correctly
- [ ] Graceful fallback when package not installed

---

### Step 2: Add Semantic Validation to Core (1-2 hours)

**2.1: Update ValidationMethod Enum**

Edit `packages/core/src/types.ts`:
```typescript
/**
 * Validation methods for cascade steps
 */
export enum ValidationMethod {
  SYNTAX_CHECK = 'syntax_check',
  QUALITY_CHECK = 'quality_check',
  FULL_QUALITY = 'full_quality',
  SEMANTIC = 'semantic',  // NEW: ML-based semantic validation
}
```

**2.2: Add Semantic Validator**

Create `packages/core/src/ml/validator.ts`:
```typescript
/**
 * Semantic validation using ML embeddings
 * Requires @cascadeflow/ml package
 */

import { ValidationMethod } from '../types';

export interface SemanticValidationResult {
  passed: boolean;
  confidence: number;
  similarity: number;
  method: 'semantic' | 'fallback';
}

export class SemanticValidator {
  private embedder: any = null;  // UnifiedEmbeddingService from @cascadeflow/ml

  constructor() {
    // Try to load @cascadeflow/ml package
    this.tryLoadMLPackage();
  }

  private async tryLoadMLPackage(): Promise<void> {
    try {
      const mlPackage = await import('@cascadeflow/ml');
      this.embedder = new mlPackage.UnifiedEmbeddingService();
    } catch (error) {
      // ML package not installed - will use fallback
      console.warn(
        'Semantic validation requires @cascadeflow/ml package. ' +
        'Install with: npm install @cascadeflow/ml'
      );
    }
  }

  /**
   * Validate response semantically matches query
   *
   * @param query Original user query
   * @param response Model response
   * @param threshold Minimum similarity threshold
   * @returns Validation result with similarity score
   */
  async validate(
    query: string,
    response: string,
    threshold: number = 0.75
  ): Promise<SemanticValidationResult> {
    // Check if ML is available
    if (!this.embedder || !this.embedder.isAvailable) {
      // Fallback to basic validation
      return this.fallbackValidation(query, response, threshold);
    }

    try {
      // Compute semantic similarity
      const similarity = await this.embedder.similarity(query, response);

      if (similarity === null) {
        return this.fallbackValidation(query, response, threshold);
      }

      return {
        passed: similarity >= threshold,
        confidence: similarity,
        similarity,
        method: 'semantic',
      };

    } catch (error) {
      console.error('Error in semantic validation:', error);
      return this.fallbackValidation(query, response, threshold);
    }
  }

  /**
   * Fallback validation when ML unavailable
   * Uses basic keyword overlap
   */
  private fallbackValidation(
    query: string,
    response: string,
    threshold: number
  ): SemanticValidationResult {
    // Simple keyword overlap as fallback
    const queryTokens = new Set(
      query.toLowerCase().match(/\b\w+\b/g) || []
    );
    const responseTokens = new Set(
      response.toLowerCase().match(/\b\w+\b/g) || []
    );

    const intersection = new Set(
      [...queryTokens].filter(x => responseTokens.has(x))
    );

    const similarity = queryTokens.size > 0
      ? intersection.size / queryTokens.size
      : 0;

    return {
      passed: similarity >= (threshold * 0.7),  // Lower threshold for fallback
      confidence: similarity * 0.8,  // Indicate lower confidence
      similarity,
      method: 'fallback',
    };
  }
}
```

**Validation:**
- [ ] Semantic validation works with ML package installed
- [ ] Gracefully falls back without ML package
- [ ] Similarity scores match Python implementation
- [ ] Threshold checking works correctly

---

### Step 3: Update Domain Detection with ML (2-3 hours)

**3.1: Add Semantic Domain Detector**

Create `packages/core/src/ml/detector.ts`:
```typescript
/**
 * Semantic domain detection using ML embeddings
 * Requires @cascadeflow/ml package
 */

import { Domain, DOMAIN_EXEMPLARS } from '../domains';  // Assume we port from Python

export interface DomainDetectionResult {
  domain: Domain;
  confidence: number;
  method: 'semantic' | 'rule-based';
  allScores?: Record<Domain, number>;
}

export class SemanticDomainDetector {
  private embedder: any = null;  // UnifiedEmbeddingService from @cascadeflow/ml
  private cache: any = null;  // EmbeddingCache

  constructor() {
    this.tryLoadMLPackage();
  }

  private async tryLoadMLPackage(): Promise<void> {
    try {
      const mlPackage = await import('@cascadeflow/ml');
      this.embedder = new mlPackage.UnifiedEmbeddingService();
      this.cache = new mlPackage.EmbeddingCache(this.embedder);
    } catch (error) {
      // ML package not installed - will use rule-based
      console.warn(
        'ML domain detection requires @cascadeflow/ml package. ' +
        'Using rule-based detection.'
      );
    }
  }

  /**
   * Detect domain using semantic similarity to exemplars
   * Falls back to rule-based if ML unavailable
   *
   * @param query User query
   * @param ruleBasedResult Fallback result from rule-based detection
   * @returns Detection result with confidence score
   */
  async detect(
    query: string,
    ruleBasedResult: DomainDetectionResult
  ): Promise<DomainDetectionResult> {
    // Check if ML is available
    if (!this.embedder || !this.embedder.isAvailable) {
      return ruleBasedResult;
    }

    try {
      // Get query embedding
      const queryEmbedding = await this.cache.getOrEmbed(query);
      if (!queryEmbedding) {
        return ruleBasedResult;
      }

      // Compute similarity to each domain's exemplars
      const domainScores: Record<string, number> = {};

      for (const [domain, exemplars] of Object.entries(DOMAIN_EXEMPLARS)) {
        let maxSimilarity = 0;

        // Find highest similarity to any exemplar
        for (const exemplar of exemplars) {
          const similarity = await this.cache.similarity(query, exemplar);
          if (similarity !== null && similarity > maxSimilarity) {
            maxSimilarity = similarity;
          }
        }

        domainScores[domain] = maxSimilarity;
      }

      // Find domain with highest score
      const sortedDomains = Object.entries(domainScores)
        .sort((a, b) => b[1] - a[1]);

      const [bestDomain, bestScore] = sortedDomains[0];

      // Use ML result if confidence is high enough (>= 80%)
      if (bestScore >= 0.80) {
        return {
          domain: bestDomain as Domain,
          confidence: bestScore,
          method: 'semantic',
          allScores: domainScores as Record<Domain, number>,
        };
      }

      // Otherwise fall back to rule-based
      return ruleBasedResult;

    } catch (error) {
      console.error('Error in semantic domain detection:', error);
      return ruleBasedResult;
    }
  }
}
```

**3.2: Update CascadeAgent to Use ML**

Edit `packages/core/src/agent.ts`:
```typescript
import { SemanticDomainDetector } from './ml/detector';
import { SemanticValidator } from './ml/validator';

export class CascadeAgent {
  private semanticDetector: SemanticDomainDetector | null = null;
  private semanticValidator: SemanticValidator | null = null;

  constructor(config: CascadeAgentConfig) {
    // ... existing code ...

    // Initialize ML components if requested
    if (config.enableSemanticDetection) {
      this.semanticDetector = new SemanticDomainDetector();
      this.semanticValidator = new SemanticValidator();
    }
  }

  private async detectDomain(query: string): Promise<DomainDetectionResult> {
    // Rule-based detection (always runs first)
    const ruleBasedResult = this.detectDomainRuleBased(query);

    // Try ML detection if enabled
    if (this.semanticDetector) {
      return await this.semanticDetector.detect(query, ruleBasedResult);
    }

    return ruleBasedResult;
  }

  // ... rest of implementation ...
}
```

**Validation:**
- [ ] ML detection works when enabled
- [ ] Falls back to rule-based when ML unavailable
- [ ] Confidence scores match Python (84-87%)
- [ ] Integration with CascadeAgent works correctly

---

### Step 4: Update Configuration (1 hour)

**4.1: Add enableSemanticDetection to Config**

Edit `packages/core/src/config.ts`:
```typescript
export interface CascadeAgentConfig {
  models: ModelConfig[];

  /**
   * Enable ML-based semantic detection (requires @cascadeflow/ml)
   *
   * When enabled and @cascadeflow/ml is installed:
   * - Domain detection uses semantic similarity (84-87% confidence)
   * - Validation can use semantic similarity scoring
   * - Automatically falls back to rule-based if ML unavailable
   *
   * @default false
   */
  enableSemanticDetection?: boolean;

  // ... existing config options ...
}
```

**Validation:**
- [ ] Config type checking works
- [ ] Default is false (backward compatible)
- [ ] Documentation clear

---

### Step 5: Update Python Dependencies (30 min)

**5.1: Update requirements.txt**

Edit `requirements.txt`:
```txt
# CascadeFlow Production Requirements
# Minimal core dependencies only

# Core dependencies
pydantic>=2.0.0
httpx>=0.25.0
tiktoken>=0.5.0
```

**5.2: Update pyproject.toml**

Edit `pyproject.toml` (add ML extra):
```toml
[project.optional-dependencies]
# ... existing providers ...

# ML/Semantic detection (UPDATED)
ml = [
    "fastembed>=0.2.0",
    "numpy>=1.24.0",
]

# All features (UPDATED)
all = [
    "openai>=1.0.0",
    "anthropic>=0.8.0",
    "groq>=0.4.0",
    "huggingface-hub>=0.19.0",
    "together>=0.2.0",
    "vllm>=0.2.0",
    "fastembed>=0.2.0",
    "numpy>=1.24.0",
]
```

**5.3: Create requirements-dev.txt**

Create `requirements-dev.txt`:
```txt
# Development dependencies

# Testing
pytest>=7.4.0
pytest-asyncio>=0.21.0
pytest-cov>=4.1.0
pytest-mock>=3.12.0

# Code quality
black>=23.0.0
ruff>=0.1.0
mypy>=1.5.0
isort>=5.12.0
pre-commit>=3.5.0

# Documentation
mkdocs>=1.5.0
mkdocs-material>=9.4.0
mkdocstrings[python]>=0.23.0

# ML (optional for dev)
fastembed>=0.2.0
numpy>=1.24.0

# Terminal output
rich>=13.0.0
```

**Validation:**
- [ ] Python installs correctly with `pip install cascadeflow`
- [ ] ML extra works: `pip install cascadeflow[ml]`
- [ ] Dev dependencies install correctly

---

### Step 6: Update TypeScript Package.json Files (30 min)

**6.1: Update packages/core/package.json**

Add @cascadeflow/ml as optional peer dependency:
```json
{
  "peerDependencies": {
    "openai": "^4.0.0",
    "@anthropic-ai/sdk": "^0.30.0",
    "groq-sdk": "^0.5.0",
    "@huggingface/inference": "^2.8.0",
    "@cascadeflow/ml": "^0.1.1"
  },
  "peerDependenciesMeta": {
    "@cascadeflow/ml": {
      "optional": true
    }
  }
}
```

**6.2: Update root package.json**

Add workspace for ml package:
```json
{
  "workspaces": [
    "packages/core",
    "packages/ml"
  ]
}
```

**Validation:**
- [ ] pnpm install works correctly
- [ ] packages/ml builds independently
- [ ] packages/core builds with optional ml dependency

---

### Step 7: Update n8n Integration (1-2 hours)

**7.1: Update n8n Node Definition**

Edit `packages/integrations/n8n/nodes/CascadeFlow/CascadeFlow.node.ts`:

Add new parameter:
```typescript
{
  displayName: 'Enable ML Detection',
  name: 'enableSemanticDetection',
  type: 'boolean',
  default: false,
  description: 'Enable ML-based semantic domain detection (requires @cascadeflow/ml)',
},
```

Update node execution:
```typescript
const agent = new CascadeAgent({
  models: modelConfigs,
  enableSemanticDetection: this.getNodeParameter('enableSemanticDetection', 0) as boolean,
});
```

**7.2: Update n8n Package Dependencies**

Edit `packages/integrations/n8n/package.json`:
```json
{
  "peerDependencies": {
    "n8n-workflow": "*",
    "@cascadeflow/core": "^0.1.1",
    "@cascadeflow/ml": "^0.1.1"
  },
  "peerDependenciesMeta": {
    "@cascadeflow/ml": {
      "optional": true
    }
  }
}
```

**7.3: Update n8n README**

Edit `packages/integrations/n8n/README.md`:

Add ML installation instructions:
```markdown
### Optional: ML-based Domain Detection

For improved domain detection accuracy (84-87% confidence):

```bash
npm install @cascadeflow/ml
```

Then enable "Enable ML Detection" in the node configuration.
```

**Validation:**
- [ ] n8n node loads correctly
- [ ] ML detection toggle works
- [ ] Falls back gracefully if @cascadeflow/ml not installed
- [ ] Works in n8n v1+ environment

---

### Step 8: Update README.md Quickstart (30 min)

**8.1: Update TypeScript ML Snippet**

Edit existing TypeScript ML snippet (already in README):

REMOVE this text:
```markdown
> **Note:** ML semantic detection is currently available in Python only. TypeScript support is planned for a future release.
```

REPLACE with actual working code:
```markdown
**Step 1:** Install the optional ML package:

```bash
npm install @cascadeflow/ml  # Adds semantic similarity detection via Transformers.js
```

**Step 2:** Enable semantic detection in your agent:

```typescript
import { CascadeAgent, ModelConfig } from '@cascadeflow/core';

// Enable ML-based semantic detection (optional parameter)
const agent = new CascadeAgent({
  models: [
    { name: 'gpt-4o-mini', provider: 'openai', cost: 0.00015 },
    { name: 'gpt-4o', provider: 'openai', cost: 0.00625 },
  ],
  enableSemanticDetection: true  // Optional: Uses ML for domain detection
});

// ML semantic detection is now active for all queries
const result = await agent.run('Calculate the eigenvalues of matrix [[1,2],[3,4]]');

// Check which detection method was used
console.log(`Domain: ${result.metadata.domainDetected}`);
console.log(`Method: ${result.metadata.detectionMethod}`);  // 'semantic' or 'rule-based'
console.log(`Confidence: ${(result.metadata.domainConfidence * 100).toFixed(1)}%`);
```

**What you get:**
- ðŸŽ¯ 84-87% confidence on complex domains (MATH, CODE, DATA, STRUCTURED)
- ðŸ”„ Automatic fallback to rule-based if ML dependencies unavailable
- ðŸ“ˆ Improved routing accuracy for specialized queries
- ðŸš€ Works seamlessly with your existing cascade setup

**Note:** If `enableSemanticDetection=true` but @cascadeflow/ml is not installed, CascadeFlow automatically falls back to rule-based detection without errors.
```

**8.2: Update Feature Parity Table**

Update `docs/PYTHON_VS_TYPESCRIPT_FEATURES.md` to show parity achieved.

**Validation:**
- [ ] README examples work correctly
- [ ] Installation instructions clear
- [ ] Feature table updated

---

### Step 9: Testing (2-3 hours)

**9.1: Unit Tests for @cascadeflow/ml**

Create `packages/ml/src/__tests__/embedding.test.ts`:
```typescript
import { describe, it, expect, beforeAll } from 'vitest';
import { UnifiedEmbeddingService, EmbeddingCache } from '../embedding';

describe('UnifiedEmbeddingService', () => {
  let embedder: UnifiedEmbeddingService;

  beforeAll(() => {
    embedder = new UnifiedEmbeddingService();
  });

  it('should check if service is available', () => {
    expect(typeof embedder.isAvailable).toBe('boolean');
  });

  it('should generate embeddings', async () => {
    const embedding = await embedder.embed('Hello world');

    expect(embedding).not.toBeNull();
    expect(embedding?.dimensions).toBe(384);
    expect(embedding?.data).toBeInstanceOf(Float32Array);
  });

  it('should compute similarity', async () => {
    const sim1 = await embedder.similarity('cat', 'kitten');
    const sim2 = await embedder.similarity('cat', 'car');

    expect(sim1).toBeGreaterThan(sim2);
    expect(sim1).toBeGreaterThan(0.5);
  });

  it('should handle batch embeddings', async () => {
    const texts = ['Hello', 'World', 'Test'];
    const embeddings = await embedder.embedBatch(texts);

    expect(embeddings).not.toBeNull();
    expect(embeddings?.length).toBe(3);
    expect(embeddings?.[0].dimensions).toBe(384);
  });
});

describe('EmbeddingCache', () => {
  let embedder: UnifiedEmbeddingService;
  let cache: EmbeddingCache;

  beforeAll(() => {
    embedder = new UnifiedEmbeddingService();
    cache = new EmbeddingCache(embedder);
  });

  it('should cache embeddings', async () => {
    const text = 'Test caching';

    const emb1 = await cache.getOrEmbed(text);
    const emb2 = await cache.getOrEmbed(text);  // Should be cached

    expect(emb1).toBe(emb2);  // Same object reference
    expect(cache.cacheSize()).toBe(1);
  });

  it('should clear cache', async () => {
    await cache.getOrEmbed('Test');
    expect(cache.cacheSize()).toBeGreaterThan(0);

    cache.clear();
    expect(cache.cacheSize()).toBe(0);
  });
});
```

**9.2: Integration Tests for Core**

Create `packages/core/src/__tests__/ml-integration.test.ts`:
```typescript
import { describe, it, expect } from 'vitest';
import { CascadeAgent } from '../agent';

describe('ML Integration', () => {
  it('should work without @cascadeflow/ml installed', async () => {
    const agent = new CascadeAgent({
      models: [
        { name: 'gpt-4o-mini', provider: 'openai', cost: 0.00015 },
      ],
      enableSemanticDetection: true,  // Should fall back gracefully
    });

    // Should not throw, even if ML unavailable
    expect(agent).toBeDefined();
  });

  it('should use ML detection when available', async () => {
    // This test requires @cascadeflow/ml to be installed
    const agent = new CascadeAgent({
      models: [
        { name: 'gpt-4o-mini', provider: 'openai', cost: 0.00015 },
      ],
      enableSemanticDetection: true,
    });

    // Test query
    const result = await agent.run('Calculate 2 + 2');

    // Check metadata includes detection method
    expect(result.metadata.detectionMethod).toBeDefined();
    expect(['semantic', 'rule-based']).toContain(result.metadata.detectionMethod);
  });
});
```

**9.3: Python Tests**

Run existing Python tests to ensure nothing broke:
```bash
pytest tests/ -v
```

**9.4: Manual Testing Checklist**

Test scenarios:
- [ ] TypeScript with ML package installed
- [ ] TypeScript without ML package (graceful fallback)
- [ ] Python with ML extra (`pip install cascadeflow[ml]`)
- [ ] Python without ML extra (graceful fallback)
- [ ] n8n integration with ML enabled
- [ ] n8n integration with ML disabled
- [ ] Browser environment (Vercel Edge, Cloudflare Workers)
- [ ] Node.js environment
- [ ] Domain detection accuracy matches Python

**Validation:**
- [ ] All tests pass
- [ ] No regressions in existing functionality
- [ ] ML detection matches Python performance
- [ ] Graceful fallback works everywhere

---

### Step 10: Documentation (1 hour)

**10.1: Create ML Package README**

Create `packages/ml/README.md`:
```markdown
# @cascadeflow/ml

ML-based semantic detection for CascadeFlow TypeScript.

Brings TypeScript to feature parity with Python's ML capabilities.

## Features

- ðŸŽ¯ 84-87% domain detection confidence (matches Python)
- ðŸ§  Semantic validation using cosine similarity
- ðŸš€ Works in Node.js, browser, and edge environments
- ðŸ“¦ Same BGE-small-en-v1.5 model as Python
- ðŸ”„ Automatic fallback to rule-based detection

## Installation

```bash
npm install @cascadeflow/ml
```

## Usage

```typescript
import { CascadeAgent } from '@cascadeflow/core';

const agent = new CascadeAgent({
  models: [...],
  enableSemanticDetection: true  // Enables ML detection
});

const result = await agent.run('Calculate eigenvalues');
console.log(result.metadata.detectionMethod);  // 'semantic'
```

## How It Works

Uses Transformers.js with Xenova/bge-small-en-v1.5 (ONNX-optimized):
- Model size: ~40MB
- Embedding dimensions: 384
- Inference time: ~20-50ms per embedding
- Works offline after first download

## API

See main documentation at https://docs.lemony.ai/cascadeflow
```

**10.2: Update Main Documentation**

Update docs to reflect TypeScript ML support is now available.

**10.3: Update CHANGELOG**

Add entry for v0.2.0:
```markdown
## [0.2.0] - 2025-XX-XX

### Added
- ðŸŽ‰ **TypeScript ML Support**: Feature parity with Python
  - New `@cascadeflow/ml` package with Transformers.js
  - ML-based semantic domain detection (84-87% confidence)
  - Semantic validation using cosine similarity
  - Automatic fallback to rule-based detection
  - Works in Node.js, browser, and edge environments
- Updated n8n integration with ML support
- New `enableSemanticDetection` config option

### Changed
- Updated Python dependencies (FastEmbed >=0.2.0)
- Updated TypeScript dependencies (added @xenova/transformers)
- Updated documentation with TypeScript ML examples

### Fixed
- Feature parity between Python and TypeScript implementations
```

**Validation:**
- [ ] All documentation accurate
- [ ] Code examples tested and working
- [ ] CHANGELOG complete

---

## Part 4: Validation & Testing Protocol

### Pre-Implementation Checklist

- [ ] Python embedding service works correctly (baseline)
- [ ] Python domain detection achieves 84-87% confidence
- [ ] Python graceful fallback works
- [ ] Current TypeScript tests pass

### Post-Implementation Checklist

#### TypeScript ML Package (@cascadeflow/ml)
- [ ] Package builds successfully
- [ ] Model loads in Node.js environment
- [ ] Model loads in browser environment (if applicable)
- [ ] Embeddings are 384 dimensions
- [ ] Cosine similarity calculations correct
- [ ] EmbeddingCache works correctly
- [ ] Graceful fallback when model unavailable
- [ ] Performance acceptable (<50ms per embedding)

#### Core Integration
- [ ] SemanticDomainDetector works with ML package
- [ ] Falls back to rule-based without ML package
- [ ] SemanticValidator works with ML package
- [ ] Falls back to basic validation without ML package
- [ ] CascadeAgent config accepts `enableSemanticDetection`
- [ ] Domain detection achieves 84-87% confidence (matches Python)
- [ ] Metadata includes detection method and confidence

#### Dependencies & Packaging
- [ ] Python `pip install cascadeflow[ml]` works
- [ ] TypeScript `npm install @cascadeflow/ml` works
- [ ] No breaking changes to existing APIs
- [ ] Backward compatible with existing code
- [ ] All peer dependencies correctly marked as optional

#### n8n Integration
- [ ] n8n node loads correctly
- [ ] ML detection toggle works
- [ ] Works with and without @cascadeflow/ml installed
- [ ] No errors in n8n logs

#### Documentation
- [ ] README TypeScript ML section updated
- [ ] Python vs TypeScript feature table updated
- [ ] Installation instructions clear
- [ ] Code examples tested and working
- [ ] ML package README complete
- [ ] CHANGELOG updated

#### Testing
- [ ] All Python tests pass
- [ ] All TypeScript tests pass
- [ ] ML package unit tests pass
- [ ] Core integration tests pass
- [ ] Manual testing completed for all scenarios
- [ ] Performance benchmarks acceptable

### Rollback Plan

If issues arise:

1. **Revert Python changes:**
   ```bash
   git checkout -- pyproject.toml requirements.txt requirements-dev.txt
   ```

2. **Remove TypeScript ML package:**
   ```bash
   rm -rf packages/ml
   git checkout -- packages/core/package.json
   ```

3. **Revert README changes:**
   ```bash
   git checkout -- README.md
   ```

4. **Revert n8n integration:**
   ```bash
   git checkout -- packages/integrations/n8n/
   ```

---

## Part 5: Timeline & Effort Estimate

| Step | Task | Time | Total |
|------|------|------|-------|
| 1 | Create @cascadeflow/ml package | 2-3h | 2-3h |
| 2 | Add semantic validation to core | 1-2h | 3-5h |
| 3 | Update domain detection with ML | 2-3h | 5-8h |
| 4 | Update configuration | 1h | 6-9h |
| 5 | Update Python dependencies | 30m | 6.5-9.5h |
| 6 | Update TypeScript package.json | 30m | 7-10h |
| 7 | Update n8n integration | 1-2h | 8-12h |
| 8 | Update README.md quickstart | 30m | 8.5-12.5h |
| 9 | Testing | 2-3h | 10.5-15.5h |
| 10 | Documentation | 1h | 11.5-16.5h |

**Total Estimated Time:** 12-16 hours

**Recommended Approach:**
- Implement in order (steps 1-10)
- Test after each step before proceeding
- Commit frequently with descriptive messages
- Create PR after step 6 for early feedback

---

## Part 6: Success Criteria

### Must Have (MVP)
1. âœ… @cascadeflow/ml package builds and works
2. âœ… Domain detection achieves 84-87% confidence (matches Python)
3. âœ… Graceful fallback to rule-based when ML unavailable
4. âœ… Works in Node.js environment
5. âœ… No breaking changes to existing APIs
6. âœ… All existing tests pass
7. âœ… README updated with working examples
8. âœ… Python and TypeScript feature parity achieved

### Should Have
1. âœ… Works in browser environment
2. âœ… n8n integration updated
3. âœ… Comprehensive testing suite
4. âœ… Performance benchmarks documented
5. âœ… ML package documentation complete

### Nice to Have
1. âš ï¸ Works in edge functions (Vercel, Cloudflare Workers)
2. âš ï¸ Streaming support for embeddings
3. âš ï¸ Advanced caching strategies
4. âš ï¸ Model quantization options

---

## Part 7: Risk Assessment

### High Risk
- **Model loading in browser:** May be slow or fail
  - Mitigation: Test thoroughly, provide warnings
- **Bundle size:** 40MB model may be too large for some use cases
  - Mitigation: Make package optional, document size

### Medium Risk
- **Performance:** TypeScript may be slower than Python
  - Mitigation: Benchmark and optimize
- **Compatibility:** May not work in all edge environments
  - Mitigation: Document limitations

### Low Risk
- **API changes:** Breaking existing functionality
  - Mitigation: Thorough testing, backward compatibility
- **Dependencies:** Transformers.js version conflicts
  - Mitigation: Pin versions, test thoroughly

---

## Part 8: Next Steps

1. **Get approval** for this implementation plan
2. **Set up development branch:** `git checkout -b feat/typescript-ml-parity`
3. **Start with Step 1:** Create @cascadeflow/ml package
4. **Test incrementally:** Validate after each step
5. **Create PR** after Step 6 for early feedback
6. **Final testing** in Steps 9-10
7. **Merge** and release v0.2.0

---

## Questions & Clarifications

Before starting implementation:

1. **Browser support priority:** Should browser environment be MVP or nice-to-have?
2. **Model size acceptable:** Is 40MB model size acceptable for TypeScript users?
3. **Edge function support:** Is Vercel Edge/Cloudflare Workers support required for MVP?
4. **Version bumping:** Should this be v0.2.0 or v1.0.0?
5. **Release timeline:** Any target date for release?

---

**Status:** âœ… Plan complete, ready for review and implementation

**Next Action:** Get approval to proceed with implementation
