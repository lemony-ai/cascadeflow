# CascadeFlow Examples

**Complete collection of examples** demonstrating CascadeFlow from basics to production deployment.

## ğŸ†• NEW in v0.2.0: Presets 2.0

**One-line agent initialization!** Start here if you're new to v0.2.0:

### Quick Start with Presets 2.0 (Recommended) â­

```bash
# 1. Set API keys
export OPENAI_API_KEY="sk-..."
export GROQ_API_KEY="gsk_..."  # optional but recommended

# 2. Run the new quickstart example
python examples/quickstart_v2.py
```

**New v0.2.0 Examples:**
- **[`quickstart_v2.py`](quickstart_v2.py)** - One-line initialization with `get_balanced_agent()`
- **[`migration_example.py`](migration_example.py)** - Side-by-side v0.1.x vs v0.2.0 comparison
- **[`preset_comparison.py`](preset_comparison.py)** - Compare all 5 presets (cost, speed, quality)

**Documentation:**
- [Presets 2.0 Guide](../docs/guides/presets.md) - Complete preset reference
- [Migration Guide](../docs/MIGRATION_GUIDE_V0.2.0.md) - v0.1.x â†’ v0.2.0 upgrade
- [Release Notes](../docs/RELEASE_NOTES_V0.2.0.md) - What's new in v0.2.0

**Quick Comparison:**

| Approach | Code | Setup Time | Features |
|----------|------|------------|----------|
| **v0.2.0 Presets** | 1 line | <1 min | Automatic detection, production-ready |
| **Manual Config** | 20+ lines | ~10 min | Full control, advanced use cases |

**Choose Presets 2.0 if:**
- âœ… You want the simplest setup
- âœ… You're starting a new project
- âœ… You want production-ready defaults
- âœ… You want 80-90% cost savings automatically

**Choose Manual Config if:**
- âœ… You need custom models
- âœ… You need fine-grained control
- âœ… You have advanced routing logic

---

## ğŸ“‹ Table of Contents

1. [Quick Start](#-quick-start)
    - [Prerequisites](#prerequisites)
    - [Supported Providers](#supported-providers)
    - [Run Your First Example](#run-your-first-example)
2. [Available Examples](#-available-examples)
    - **Core Examples**
        - [1. Basic Usage](#1-basic-usage--start-here)
        - [2. Streaming Text](#2-streaming-text-responses-)
        - [3. Streaming Tools](#3-streaming-with-tool-calling-ï¸)
        - [4. Tool Execution](#4-tool-execution-)
        - [5. Cost Tracking](#5-cost-tracking-)
        - [6. Multi-Provider](#6-multi-provider-cascade-)
    - **Advanced Examples**
        - [7. FastAPI Integration](#7-fastapi-integration-)
        - [8. Custom Cascade](#8-custom-cascade-strategies-)
        - [9. Custom Validation](#9-custom-validation-)
        - [10. Production Patterns](#10-production-patterns-)
3. [Complete Documentation](#-complete-documentation)
4. [Learning Path](#-learning-path)
5. [Running Examples](#-running-examples)
6. [Troubleshooting](#-troubleshooting)
7. [Tips for Learning](#-tips-for-learning)
8. [Contributing](#-contributing-examples)
9. [Need Help?](#-need-help)

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Install CascadeFlow with all dependencies
pip install cascadeflow[all]

# Set API keys (choose your providers)
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GROQ_API_KEY="gsk_..."
```

### Supported Providers

CascadeFlow supports **7 providers** with varying capabilities:

| Provider | Streaming | Tool Calling | Logprobs | Best For | Setup |
|----------|-----------|--------------|----------|----------|-------|
| **OpenAI** | âœ… | âœ… | âœ… | Production, best quality | `export OPENAI_API_KEY="sk-..."` |
| **Anthropic** | âœ… | âœ… | âŒ | Long context, safety | `export ANTHROPIC_API_KEY="sk-ant-..."` |
| **Groq** | âœ… | âœ… | âœ… | Speed, low latency | `export GROQ_API_KEY="gsk_..."` |
| **Together** | âœ… | âœ… | âœ… | Open models, flexibility | `export TOGETHER_API_KEY="..."` |
| **Ollama** | âœ… | âœ… | âŒ | Local/offline, privacy | No key needed (local) |
| **vLLM** | âœ… | âœ… | âœ… | Self-hosted, control | Configure endpoint URL |
| **HuggingFace** | âœ… | âœ…* | âŒ | Model variety, testing | `export HF_TOKEN="hf_..."` |

**Legend:**
- âœ… = Fully supported
- âŒ = Not supported (uses confidence estimation fallback)
- \* = Tool calling support varies by model and endpoint type

**Installation Options:**
```bash
# All providers
pip install cascadeflow[all]

# Specific providers
pip install cascadeflow[openai]
pip install cascadeflow[anthropic]
pip install cascadeflow[groq]
```

**Provider Comparison:**

| Feature | OpenAI | Anthropic | Groq | Together | Ollama | vLLM | HuggingFace |
|---------|--------|-----------|------|----------|--------|------|-------------|
| **Cost** | $$ | $$$ | $ | $$ | Free | Free* | $-$$ |
| **Speed** | Fast | Medium | Very Fast | Fast | Fast | Very Fast | Varies |
| **Quality** | Excellent | Excellent | Good | Good-Excellent | Varies | Varies | Varies |
| **Context** | 128K | 200K | 32K | 32-128K | Varies | Varies | Varies |
| **Best Models** | GPT-4o, GPT-4o-mini | Claude 3.5 Sonnet | Llama 3, Mixtral | Llama 3, Qwen | Any Ollama model | Any vLLM model | Depends on endpoint |

\* vLLM is free to run but requires infrastructure costs

**Quick Setup Examples:**
```python
from cascadeflow import CascadeAgent, ModelConfig

# OpenAI (most common)
agent = CascadeAgent(models=[
    ModelConfig("gpt-4o-mini", "openai", cost=0.00015),
    ModelConfig("gpt-4o", "openai", cost=0.00625),
])

# Mix providers for best results
agent = CascadeAgent(models=[
    ModelConfig("llama-3.1-8b", "groq", cost=0.00005),      # Cheap & fast
    ModelConfig("claude-3-5-sonnet", "anthropic", cost=0.003),  # Quality
])

# Local with Ollama (no API key needed)
agent = CascadeAgent(models=[
    ModelConfig("llama3", "ollama", cost=0.0),
    ModelConfig("codellama", "ollama", cost=0.0),
])

# Auto-discover Ollama models
from cascadeflow.providers.ollama import OllamaProvider
provider = OllamaProvider()
models = await provider.list_models()
# Returns: ['llama3.2:1b', 'mistral:7b', ...]

# Auto-discover vLLM models
from cascadeflow.providers.vllm import VLLMProvider
provider = VLLMProvider(base_url="http://localhost:8000/v1")
models = await provider.list_models()
# Returns: ['meta-llama/Llama-3.2-3B-Instruct', ...]
```

**ğŸ’¡ Auto-Discovery**: Both Ollama and vLLM support automatic model discovery via `list_models()` - no need to hardcode model names! See [Provider Guide](../docs/guides/providers.md) for details.

### Run Your First Example

```bash
# From repository root
python examples/basic_usage.py
```

---

## ğŸ“š Available Examples

### Core Examples

#### 1. Basic Usage â­ **START HERE**

**File**: [`basic_usage.py`](basic_usage.py) (~200 lines)  
**Documentation**: [Quick Start Guide](../docs/guides/quickstart.md)

The simplest way to get started with CascadeFlow.

**What it demonstrates:**
- Two-tier cascade (GPT-4o-mini â†’ GPT-4o)
- Automatic quality-based routing
- Token-based cost tracking and savings calculation
- 8 test queries from trivial to expert complexity
- When drafts are accepted vs rejected
- Real-world cost optimization patterns

**Requirements:**
- OpenAI API key

**Expected savings:** 40-60% vs all-GPT-4o (varies by query mix)

**Run it:**
```bash
export OPENAI_API_KEY="sk-..."
python examples/basic_usage.py
```

**Output preview:**
```
Query 1/8: What color is the sky?
   ğŸ’š Model: gpt-4o-mini only
   ğŸ’° Cost: $0.000014
   âœ… Draft Accepted: Verifier skipped

Query 6/8: Explain quantum entanglement...
   ğŸ’šğŸ’› Models: gpt-4o-mini + gpt-4o
   ğŸ’° Cost: $0.005006
   âŒ Draft Rejected: Both models used

ğŸ’° TOTAL SAVINGS: 45% reduction
```

<details>
<summary><strong>Common Questions</strong></summary>

**Q: Why are my savings different from the example?**  
A: Savings depend on your query mix (simple vs complex), response lengths, draft acceptance rates, and which models you're comparing.

**Q: What does "Draft Accepted" mean?**  
A: The cheap model passed quality checks, so the expensive model was NOT called. **This saves money!**

**Q: What does "Draft Rejected" mean?**  
A: The cheap model failed quality checks, so BOTH models were called. This costs more but ensures quality.

**Q: How are costs calculated?**  
A: CascadeFlow uses token-based pricing - input tokens + output tokens, with different rates per provider/model.
</details>

---

### Streaming Examples

#### 2. Streaming Text Responses ğŸŒŠ

**File**: [`streaming_text.py`](streaming_text.py) (~250 lines)  
**Documentation**: [Streaming Guide](../docs/guides/streaming.md)

Real-time streaming with cascade visualization.

**What it demonstrates:**
- Real-time text streaming (character-by-character)
- Cascade transitions in action (see routing decisions)
- Quality validation during streaming
- Visual feedback (colors, indicators)
- Performance metrics (latency, costs)

**Requirements:**
- OpenAI API key
- Terminal with ANSI color support

**Run it:**
```bash
export OPENAI_API_KEY="sk-..."
python examples/streaming_text.py
```

**Output preview:**
```
ğŸŒŠ Streaming Query: What color is the sky?

ğŸ’š gpt-4o-mini: The sky appears blue during...
âœ… DRAFT ACCEPTED in 234ms
ğŸ’° Cost: $0.000018

ğŸŒŠ Streaming Query: Explain quantum mechanics...

ğŸ’š gpt-4o-mini: Quantum mechanics is...
âŒ DRAFT REJECTED (confidence: 0.65)
â†» Cascading to gpt-4o...

ğŸ’› gpt-4o: Quantum mechanics describes...
âœ… VERIFIER COMPLETE in 876ms
ğŸ’° Cost: $0.004523
```

<details>
<summary><strong>Important Notes</strong></summary>

- **Streaming requires 2+ models** (cascade must be enabled)
- Check `if agent.text_streaming_manager:` before streaming
- Use `agent.stream_events(query)` (not internal methods)
- Handle `CHUNK`, `DRAFT_DECISION`, `SWITCH`, `COMPLETE` events
</details>

---

#### 3. Streaming with Tool Calling ğŸ› ï¸

**File**: [`streaming_tools.py`](streaming_tools.py) (~300 lines)  
**Documentation**: [Streaming Guide - Tools](../docs/guides/streaming.md#tool-streaming)

Watch tool calls form in real-time as JSON arrives.

**What it demonstrates:**
- Real-time tool call detection
- Progressive JSON parsing (see arguments as they arrive)
- Universal tool format (works with all providers)
- Event-based streaming architecture
- Tool call validation

**Requirements:**
- OpenAI API key

**Run it:**
```bash
export OPENAI_API_KEY="sk-..."
python examples/streaming_tools.py
```

**Output preview:**
```
ğŸ”§ Tool Streaming Query: What's the weather in Tokyo?

ğŸ› ï¸ [get_weather] called:
   ğŸ“ Arguments streaming in...
   {"location": "Tokyo", "unit": "celsius"}
   âœ… Tool call complete

ğŸ’¬ Model response: The current temperature...
```

<details>
<summary><strong>Critical Notes</strong></summary>

**This example shows tool call STREAMING only (detection + parsing), NOT execution.**

- âœ… Use **universal tool format**: `{"name": "...", "description": "...", "parameters": {...}}`
- âŒ Don't use OpenAI format: `{"type": "function", "function": {...}}`
- ğŸ“š For actual tool **EXECUTION**, see next example (`tool_execution.py`)
- ğŸ”§ Streaming â‰  Execution (two separate concepts)
</details>

---

#### 4. Tool Execution ğŸ¯

**File**: [`tool_execution.py`](tool_execution.py) (~400 lines)  
**Documentation**: [Tools Guide](../docs/guides/tools.md)

Complete tool execution workflow with `ToolExecutor`.

**What it demonstrates:**
- Creating executable `ToolConfig` objects
- Using `ToolExecutor` to run tool calls
- Feeding results back to the model
- Multi-turn conversations with tools
- Complete request/response workflows
- Error handling and retries

**Requirements:**
- OpenAI API key
- Requests library (for weather API)

**Run it:**
```bash
export OPENAI_API_KEY="sk-..."
python examples/tool_execution.py
```

**Output preview:**
```
ğŸ”§ Tool Execution: What's the weather in Tokyo and should I bring an umbrella?

ğŸ› ï¸ Executing tool: get_weather
   ğŸ“ Location: Tokyo
   â˜€ï¸ Result: 24Â°C, Clear

ğŸ› ï¸ Executing tool: get_recommendation
   â˜‚ï¸ Result: No umbrella needed

ğŸ’¬ Final Answer: The weather in Tokyo is 24Â°C and clear. You don't need an umbrella!
ğŸ’° Total Cost: $0.003456
```

<details>
<summary><strong>Key Concepts</strong></summary>

**Streaming vs Execution (Important!):**
1. **Streaming** (`streaming_tools.py`): Watch tool calls form, but don't execute
2. **Execution** (this example): Actually run the tool functions and get results

**Complete Workflow:**
```python
# 1. Define tools with executable functions
tools = [ToolConfig(
    name="get_weather",
    function=get_weather_impl,  # Actual Python function
    ...
)]

# 2. Create executor
executor = ToolExecutor(tools)

# 3. Get model response with tool calls
result = await agent.run(query, tools=tools)

# 4. Execute tools
tool_results = await executor.execute(result.tool_calls)

# 5. Feed results back for final answer
final = await agent.run(query, tool_results=tool_results)
```
</details>

---

#### 5. Cost Tracking ğŸ’°

**File**: [`cost_tracking.py`](cost_tracking.py) (~320 lines)  
**Documentation**: [Cost Tracking Guide](../docs/guides/cost_tracking.md)

Track costs in real-time and manage budgets effectively.

**What it demonstrates:**
- **Real-time cost tracking** across multiple queries
- **Budget management** with automatic warnings (80% threshold)
- **Per-model and per-provider analysis** for cost optimization
- **Metadata extraction** from result objects (working with result.metadata)
- **Integration with telemetry** (CostTracker + MetricsCollector)
- **Cost optimization insights** to identify expensive patterns

**Requirements:**
- OpenAI API key

**Run it:**
```bash
export OPENAI_API_KEY="sk-..."
python examples/cost_tracking.py
```

**Output preview:**
```
ğŸ’° CascadeFlow Cost Tracking

âœ“ Cost tracker initialized
  Budget limit: $1.00
  Warn threshold: 80%

Running queries with cost tracking...

Query 1/5: What is Python?...
  ğŸ’° Cost: $0.000234
  ğŸ¯ Model: gpt-4o-mini (draft accepted)
  âœ… Saved cost by using cheap model!

Query 3/5: What are the health benefits of green tea?...
  ğŸ’° Cost: $0.000189
  ğŸ¯ Model: gpt-4o-mini (draft accepted)
  âœ… Saved cost by using cheap model!

Query 5/5: Explain machine learning in detail...
  ğŸ’° Cost: $0.004156
  ğŸ¯ Model: gpt-4o (after cascade)
  ğŸ”„ Draft rejected, used verifier for quality

============================================================
COST TRACKER SUMMARY
============================================================
Total Cost:        $0.023456
Total Queries:     5
Budget Used:       2.3%
Budget Remaining:  $0.976544

BY MODEL:
  gpt-4o-mini:    $0.003234 (13.8%)
  gpt-4o:         $0.020222 (86.2%)

BY PROVIDER:
  openai:         $0.023456 (100.0%)
============================================================

METRICS SUMMARY
============================================================
Total Queries:     5
Cascaded Queries:  2
Cascade Rate:      40.0%
Avg Latency:       856ms
Total Cost:        $0.023456
============================================================
```

<details>
<summary><strong>Key Concepts</strong></summary>

**Cost Tracking Components:**
- **CostTracker**: Monitors costs across queries, enforces budgets (warnings at threshold)
- **MetricsCollector**: Aggregates comprehensive statistics (costs, latency, cascade rate)
- **CostCalculator**: Stateless calculations for detailed savings analysis (advanced use)

**Working with Result Metadata:**
```python
# Extract costs safely from result.metadata
total_cost = getattr(result, 'total_cost', 0) or 0
total_tokens = result.metadata.get('total_tokens', 0) or 0
draft_cost = result.metadata.get('draft_cost', 0)
verifier_cost = result.metadata.get('verifier_cost', 0)
cascaded = result.metadata.get('cascaded', False)
draft_accepted = result.metadata.get('draft_accepted', False)
```

**Integration Pattern:**
```python
# 1. Setup tracker with budget
tracker = CostTracker(budget_limit=1.0, warn_threshold=0.8)

# 2. Run query
result = await agent.run(query)

# 3. Extract metadata and track costs
total_cost = getattr(result, 'total_cost', 0) or 0
total_tokens = result.metadata.get('total_tokens', 0) or 0

tracker.add_cost(
    model=result.metadata.get('draft_model') or result.model_used,
    provider="openai",
    tokens=total_tokens,
    cost=total_cost,
    metadata={'cascaded': result.metadata.get('cascaded', False)}
)

# 4. View summary
tracker.print_summary()
```

**Budget Alerts:**
- Warning at 80% of budget (default, configurable)
- Per-query tracking with metadata
- Budget remaining calculations
- No hard stops - warnings only for flexibility
</details>

---

#### 6. Multi-Provider Cascade ğŸŒ

**File**: [`multi_provider.py`](multi_provider.py) (~400 lines)
**Documentation**: [Providers Guide](../docs/guides/providers.md)

Mix and match models from different providers.

**What it demonstrates:**
- Using models from multiple providers in one cascade
- Provider-specific optimizations
- Fallback strategies across providers
- Cost optimization across providers
- Handling provider-specific features

**Requirements:**
- At least 2 provider API keys (OpenAI, Anthropic, Groq, etc.)

**Run it:**
```bash
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
python examples/multi_provider.py
```

**Output preview:**
```
ğŸŒ Multi-Provider Cascade

Query 1: Simple question
  ğŸ’š Groq (llama-3-8b): Fast and cheap!
  ğŸ’° Cost: $0.000005
  âš¡ Latency: 123ms

Query 2: Complex analysis
  ğŸ’š OpenAI (gpt-4o-mini): Initial attempt...
  âŒ Draft rejected
  ğŸ’› Anthropic (claude-3-opus): High quality!
  ğŸ’° Cost: $0.015234
  âš¡ Latency: 2,456ms
```

---

### Advanced Examples

#### 7. FastAPI Integration ğŸš€

**File**: [`fastapi_integration.py`](fastapi_integration.py) (~450 lines)  
**Documentation**: [FastAPI Guide](../docs/guides/fastapi.md)

Production-ready REST API with streaming support.

**What it demonstrates:**
- RESTful API design with FastAPI
- Server-Sent Events (SSE) for streaming
- Request validation with Pydantic
- Error handling and health checks
- Cost tracking and monitoring
- Deployment patterns

**Requirements:**
- FastAPI, Uvicorn, SSE-Starlette

**Setup:**
```bash
pip install fastapi uvicorn sse-starlette
export OPENAI_API_KEY="sk-..."
```

**Run it:**
```bash
python examples/fastapi_integration.py
```

**Test endpoints:**
```bash
# Non-streaming query
curl -X POST "http://localhost:8000/api/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "What is AI?", "max_tokens": 100}'

# Streaming query (SSE)
curl "http://localhost:8000/api/query/stream?query=Explain%20AI"

# Health check
curl "http://localhost:8000/health"

# Statistics
curl "http://localhost:8000/api/stats"
```

**Features:**
- âœ… RESTful + Streaming endpoints
- âœ… Request/response validation
- âœ… Automatic API docs (Swagger/ReDoc)
- âœ… Health monitoring
- âœ… Production logging
- âœ… Docker deployment ready

---

#### 8. Custom Cascade Strategies ğŸ¯

**File**: [`custom_cascade.py`](custom_cascade.py) (~450 lines)  
**Documentation**: [Custom Cascade Guide](../docs/guides/custom_cascade.md)

Build domain-specific routing strategies.

**What it demonstrates:**
- **Domain-specific routing**: Detect query type, route to optimized models
- **Time-based routing**: Peak vs off-peak cost optimization
- **Budget-aware cascades**: Per-query cost constraints
- **Custom quality thresholds**: Different criticality levels
- **Adaptive routing**: Learn from historical performance

**Requirements:**
- OpenAI API key

**Run it:**
```bash
export OPENAI_API_KEY="sk-..."
python examples/custom_cascade.py
```

**Example patterns:**

```python
# 1. Domain-Specific Routing
class DomainRouter:
    def route(self, query: str) -> List[ModelConfig]:
        if "code" in query.lower():
            return [code_specialist_models]
        elif "medical" in query.lower():
            return [medical_specialist_models]
        else:
            return [general_models]

# 2. Time-Based Routing
class TimeRouter:
    def route(self, query: str) -> List[ModelConfig]:
        if is_peak_hours():
            return [cheap_fast_models]  # Save costs
        else:
            return [premium_models]  # Better quality

# 3. Budget-Aware Routing
class BudgetRouter:
    def route(self, query: str, budget: float) -> List[ModelConfig]:
        if budget < 0.001:
            return [free_models_only]
        elif budget < 0.01:
            return [cheap_models]
        else:
            return [all_models]
```

---

#### 9. Custom Validation ğŸ”

**File**: [`custom_validation.py`](custom_validation.py) (~400 lines)  
**Documentation**: [Custom Validation Guide](../docs/guides/custom_validation.md)

Build custom quality validation rules.

**What it demonstrates:**
- Custom validation logic for specific domains
- Compliance checking (PII, regulations, safety)
- Format validation (JSON, markdown, code)
- Content filtering and safety checks
- Multi-validator composition

**Requirements:**
- OpenAI API key

**Run it:**
```bash
export OPENAI_API_KEY="sk-..."
python examples/custom_validation.py
```

**Example validators:**

```python
# 1. Medical Compliance Validator
class MedicalValidator:
    def validate(self, response: str) -> bool:
        # Check for medical disclaimers
        # Verify no diagnosis claims
        # Ensure safety warnings
        return is_compliant

# 2. PII Detection Validator
class PIIValidator:
    def validate(self, response: str) -> bool:
        # Check for emails, phone numbers, SSNs
        return not contains_pii(response)

# 3. Code Quality Validator
class CodeValidator:
    def validate(self, response: str) -> bool:
        # Check for syntax errors
        # Verify security best practices
        return is_valid_code(response)

# 4. Format Validator
class FormatValidator:
    def validate(self, response: str, format_type: str) -> bool:
        validators = {
            "json": self._validate_json,
            "markdown": self._validate_markdown,
            "code": self._validate_code_block
        }
        return validators[format_type](response)

# 5. Composite Validator
class CompositeValidator:
    def __init__(self, validators: List[Validator]):
        self.validators = validators
    
    def validate(self, response: str) -> bool:
        return all(v.validate(response) for v in self.validators)
```

---

#### 10. Production Patterns ğŸ­

**File**: [`production_patterns.py`](production_patterns.py) (~500 lines)  
**Documentation**: [Production Guide](../docs/guides/production.md)

Enterprise-ready deployment patterns.

**What it demonstrates:**
- **Error handling**: Retry logic with exponential backoff
- **Rate limiting**: Request throttling and queuing
- **Circuit breakers**: Automatic failure detection
- **Caching**: Response caching for common queries
- **Monitoring**: Metrics, logging, alerting
- **Budget management**: Cost caps and limits
- **Health checks**: System status monitoring

**Requirements:**
- OpenAI API key
- Redis (optional, for caching)

**Run it:**
```bash
export OPENAI_API_KEY="sk-..."
python examples/production_patterns.py
```

**Example patterns:**

```python
# 1. Retry with Exponential Backoff
class RetryHandler:
    async def call_with_retry(self, func, max_retries=3):
        for attempt in range(max_retries):
            try:
                return await func()
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                await asyncio.sleep(2 ** attempt)

# 2. Circuit Breaker
class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failures = 0
        self.state = "closed"  # closed, open, half_open
        
    async def call(self, func):
        if self.state == "open":
            raise CircuitBreakerOpen()
        try:
            result = await func()
            self.failures = 0
            return result
        except Exception as e:
            self.failures += 1
            if self.failures >= self.failure_threshold:
                self.state = "open"
            raise

# 3. Rate Limiter
class RateLimiter:
    def __init__(self, max_requests=100, window=60):
        self.requests = deque()
        self.max_requests = max_requests
        self.window = window
        
    async def acquire(self):
        now = time.time()
        # Remove old requests
        while self.requests and self.requests[0] < now - self.window:
            self.requests.popleft()
        
        if len(self.requests) >= self.max_requests:
            raise RateLimitExceeded()
        
        self.requests.append(now)

# 4. Response Cache
class ResponseCache:
    def __init__(self, ttl=300):
        self.cache = {}
        self.ttl = ttl
    
    async def get_or_compute(self, key, func):
        if key in self.cache:
            value, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return value
        
        value = await func()
        self.cache[key] = (value, time.time())
        return value
```

---

#### 11. Edge Device Deployment ğŸ”Œ

**File**: [`edge_device.py`](edge_device.py) (~430 lines)
**Documentation**: [Edge Device Guide](../docs/guides/edge_device.md) **NEW**

Run CascadeFlow on edge AI devices (Nvidia Jetson, Raspberry Pi) with local inference.

**What it demonstrates:**
- **Local inference** with vLLM on edge device (privacy-first)
- **Automatic cascade** to cloud for complex queries
- **Zero-cost** local processing with cloud fallback
- **Real-time** latency optimization (<100ms locally)
- **Privacy** - 70-80% of queries stay on device

**Hardware Requirements:**
- Nvidia Jetson (Thor, Orin, Xavier) OR Raspberry Pi 5
- 8GB+ RAM (16GB recommended)
- GPU with CUDA support (for optimal performance)

**Software Requirements:**
- vLLM server running locally
- Anthropic API key for cloud fallback

**Setup:**
```bash
# Install vLLM on Jetson
pip3 install vllm

# Start local model server
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.2-3B-Instruct \
    --dtype half \
    --max-model-len 4096

# Set environment
export VLLM_BASE_URL="http://localhost:8000/v1"
export ANTHROPIC_API_KEY="sk-ant-..."

# Run example
python examples/edge_device.py
```

**Expected Results:**
- Simple queries: Processed locally (<100ms, $0 cost)
- Complex queries: Cascade to Claude (~800ms, small cost)
- 70-80% queries stay on device (privacy + cost savings)
- 20-30% cascade only when quality needed

**Use Cases:**
- Smart factories: Local vision + reasoning
- Healthcare devices: HIPAA-compliant processing
- Retail kiosks: Fast local responses
- Autonomous robots: Real-time control
- IoT gateways: Local aggregation

**Cost Analysis** (10k queries/month):
```
Without CascadeFlow (all cloud): $30.00/month
With CascadeFlow (edge-first):   $9.00/month
Savings: $21.00/month (70%) + Privacy + Lower Latency
```

---

## ğŸ“š Complete Documentation

### Available Guides

1. [`quickstart.md`](../docs/guides/quickstart.md) - Getting started (~500 lines)
2. [`streaming.md`](../docs/guides/streaming.md) - Streaming details (~650 lines)
3. [`tools.md`](../docs/guides/tools.md) - Tool execution (~700 lines)
4. [`cost_tracking.md`](../docs/guides/cost_tracking.md) - Cost management (~1,160 lines) **NEW**
5. [`providers.md`](../docs/guides/providers.md) - Multi-provider setup (~600 lines)
6. [`fastapi.md`](../docs/guides/fastapi.md) - API integration (~800 lines)
7. [`custom_cascade.md`](../docs/guides/custom_cascade.md) - Custom routing (~700 lines)
8. [`custom_validation.md`](../docs/guides/custom_validation.md) - Validation (~600 lines)
9. [`production.md`](../docs/guides/production.md) - Production deployment (~650 lines)
10. [`edge_device.md`](../docs/guides/edge_device.md) - Edge deployment (~600 lines) **NEW**

### Total Documentation

**ğŸ“Š ~10,280 lines of professional documentation** covering:
- âœ… Core features - 100%
- âœ… Advanced features - 100%
- âœ… Production patterns - 100%
- âœ… API integration - 100%
- âœ… Cost tracking & budgets - 100%

---

## ğŸ¯ Learning Path

### Step 1: Master the Basics (1 hour)
1. Run `basic_usage.py` - Understand core concepts
2. Read the code comments - Learn implementation patterns
3. Read [`quickstart.md`](../docs/guides/quickstart.md) - Deep dive into concepts

**Key Concepts:**
- How cascading works (draft â†’ verify)
- Token-based cost calculation
- Quality-based routing decisions
- When drafts are accepted vs rejected

---

### Step 2: Explore Streaming (30 mins)
1. Run `streaming_text.py` - See streaming in action
2. Run `streaming_tools.py` - Watch tool calls form
3. Read [`streaming.md`](../docs/guides/streaming.md) - Understand streaming architecture

**Key Concepts:**
- Real-time response generation
- Event-based streaming (CHUNK, SWITCH, COMPLETE)
- Visual cascade feedback
- Streaming requires 2+ models

---

### Step 3: Learn Tool Execution (45 mins)
1. Run `tool_execution.py` - Complete tool workflow
2. Study the `ToolExecutor` pattern
3. Read [`tools.md`](../docs/guides/tools.md) - Tool system details

**Key Concepts:**
- Streaming â‰  Execution (two separate steps)
- Universal tool format (not OpenAI format)
- Multi-turn conversations with tools
- Error handling and retries

---

### Step 4: Master Cost Tracking (30 mins) **NEW**
1. Run `cost_tracking.py` - See comprehensive cost tracking
2. Study the `CostTracker`, `MetricsCollector` pattern
3. Read [`cost_tracking.md`](../docs/guides/cost_tracking.md) - Cost management details

**Key Concepts:**
- Real-time cost monitoring across queries
- Budget limits with alerts (warn at 80%)
- Per-model and per-provider cost breakdowns
- Metadata extraction from results (result.metadata)
- Cost history with query-level metadata
- Advanced cost analysis and optimization

---

### Step 5: Advanced Patterns (1 hour)
1. Run `multi_provider.py` - Mix providers
2. Run `custom_cascade.py` - Custom routing strategies
3. Run `custom_validation.py` - Domain-specific validation
4. Read the corresponding guides

**Key Concepts:**
- Domain-specific routing
- Compliance validation
- Cost optimization strategies
- Quality control patterns

---

### Step 6: Production Deployment (1 hour)
1. Run `fastapi_integration.py` - API deployment
2. Run `production_patterns.py` - Enterprise patterns
3. Read [`production.md`](../docs/guides/production.md) - Deployment strategies

**Key Concepts:**
- Error handling and retries
- Rate limiting and circuit breakers
- Caching and monitoring
- Budget management

---

### Step 7: Customize for Your Use Case
- Modify models and providers
- Adjust quality thresholds
- Add custom validation rules
- Implement your own routing logic
- Set up cost tracking and budgets
- Deploy to production

---

## ğŸ› ï¸ Running Examples

### From Repository Root

```bash
# Activate virtual environment (recommended)
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate     # Windows

# Run any example
python examples/basic_usage.py
python examples/streaming_text.py
python examples/cost_tracking.py
python examples/fastapi_integration.py
```

### With Custom Configuration

```python
# Edit the example file to use your models
agent = CascadeAgent(models=[
    ModelConfig("your-model", "your-provider", cost=0.001),
    ModelConfig("your-verifier", "your-provider", cost=0.01),
])
```

### Running FastAPI Example

```bash
# Install FastAPI dependencies
pip install fastapi uvicorn sse-starlette

# Run the server
python examples/fastapi_integration.py

# In another terminal, test endpoints
curl -X POST "http://localhost:8000/api/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "What is AI?"}'
```

---

## ğŸ”§ Troubleshooting

<details>
<summary><strong>API key errors</strong></summary>

```bash
# Check if API key is set
echo $OPENAI_API_KEY

# Set it if missing
export OPENAI_API_KEY="sk-..."

# On Windows:
set OPENAI_API_KEY=sk-...
```
</details>

<details>
<summary><strong>Import errors</strong></summary>

```bash
# Install all dependencies
pip install cascadeflow[all]

# Or install specific providers
pip install cascadeflow[openai]
pip install cascadeflow[anthropic]
```
</details>

<details>
<summary><strong>Cost tracking shows zero costs</strong></summary>

```python
# Ensure models have cost configured
agent = CascadeAgent(models=[
    ModelConfig("gpt-4o-mini", "openai", cost=0.00015),  # â† Must set cost
    ModelConfig("gpt-4o", "openai", cost=0.00625),       # â† Must set cost
])

# Use safe extraction from result.metadata
total_cost = getattr(result, 'total_cost', 0) or 0
total_tokens = result.metadata.get('total_tokens', 0) or 0
```
</details>

<details>
<summary><strong>Budget warnings not showing</strong></summary>

```python
# Enable verbose mode
tracker = CostTracker(
    budget_limit=1.0,
    warn_threshold=0.8,
    verbose=True  # â† Enable this
)

# Or manually check budget
summary = tracker.get_summary()
if summary.get('budget_used_pct', 0) >= 80:
    print(f"âš ï¸ Warning: {summary['budget_used_pct']:.1f}% used")
```
</details>

<details>
<summary><strong>Streaming shows garbled output</strong></summary>

Your terminal may not support ANSI colors:
```bash
# Disable colors temporarily
TERM=dumb python examples/streaming_text.py
```
</details>

<details>
<summary><strong>Examples run but show errors</strong></summary>

```bash
# Check Python version (3.9+ required)
python --version

# Reinstall with all dependencies
pip install --upgrade cascadeflow[all]
```
</details>

---

## ğŸ’¡ Tips for Learning

### 1. Start Simple
Begin with `basic_usage.py` to understand core concepts before moving to advanced examples.

### 2. Read the Code
All examples are heavily commented with inline explanations. Read through to understand the patterns.

### 3. Understand Key Concepts

**Streaming vs Execution (Critical!):**
- `streaming_tools.py` - Watch tool calls form (streaming only, no execution)
- `tool_execution.py` - Actually execute tools (with `ToolExecutor`)
- Why separate? Gives you control over validation, security, and custom logic.

**Token-Based Pricing:**
- CascadeFlow uses actual token counts, not flat rates
- Input tokens cost less than output tokens
- Longer queries/responses = higher costs
- Provider pricing varies (OpenAI vs Anthropic vs Groq)

**Quality Validation:**
- Every response is validated for quality
- Draft accepted = cheap model only (saves money!)
- Draft rejected = both models called (ensures quality)
- Adjust thresholds based on your use case

**Cost Tracking:**
- Extract costs from `result.metadata` dictionary
- CostTracker monitors costs across queries
- MetricsCollector provides comprehensive analytics
- Budget limits with warnings prevent overspending
- Use safe extraction: `getattr()` and `.get()` with fallbacks

### 4. Modify and Experiment
- Change the models and providers
- Try different queries (simple to complex)
- Adjust quality thresholds
- Add custom validation rules
- Create your own tools
- Track costs and performance with CostTracker
- Set budget limits and observe alerts

### 5. Watch the Streaming Examples
Streaming shows you the cascade in action:
- See when drafts are accepted/rejected
- Watch cascade transitions happen
- Understand quality decisions in real-time
- Debug issues as they occur

### 6. Check Statistics
All examples show costs and performance metrics:
```python
result = await agent.run(query)

# Safe extraction from result
total_cost = getattr(result, 'total_cost', 0)
model_used = getattr(result, 'model_used', 'unknown')
latency_ms = getattr(result, 'latency_ms', 0)

# Extract from metadata
cascaded = result.metadata.get('cascaded', False)
draft_accepted = result.metadata.get('draft_accepted', False)
```

### 7. Monitor Costs with CostTracker
Use the telemetry system for comprehensive cost monitoring:
```python
from cascadeflow.telemetry import CostTracker

tracker = CostTracker(budget_limit=1.0, warn_threshold=0.8)

# Run queries and track
result = await agent.run(query)

# Safe extraction
total_cost = getattr(result, 'total_cost', 0) or 0
total_tokens = result.metadata.get('total_tokens', 0) or 0

tracker.add_cost(
    model=result.metadata.get('draft_model') or result.model_used,
    provider="openai",
    tokens=total_tokens,
    cost=total_cost
)

# View summary
tracker.print_summary()
```

### 8. Read the Full Guides
Complete documentation in `docs/guides/` explains concepts in depth:
- Theory and architecture
- Best practices
- Production deployment
- Advanced patterns
- Cost optimization

---

## ğŸ¤ Contributing Examples

Have a great use case? Contribute an example!

### Example Template

```python
"""
Your Example - Brief Description

What it demonstrates:
- Feature 1
- Feature 2
- Feature 3

Requirements:
- Dependency 1
- Dependency 2

Setup:
    pip install cascadeflow[all] dependency1 dependency2
    export API_KEY="..."
    python examples/your_example.py

Expected Results:
    Description of what users should see

Use Cases:
- Use case 1
- Use case 2

Documentation:
    docs/guides/your-example.md
"""

import asyncio
from cascadeflow import CascadeAgent, ModelConfig

async def main():
    """Your example implementation."""
    
    # 1. Clear introduction
    print("=" * 80)
    print("YOUR EXAMPLE TITLE")
    print("=" * 80)
    print()
    print("What this example demonstrates...")
    print()
    
    # 2. Configuration with comments
    agent = CascadeAgent(models=[
        ModelConfig(...),  # Explain each model
    ])
    
    # 3. Demonstration with output
    result = await agent.run(query)
    
    # 4. Statistics and analysis
    print(f"\nğŸ’° Cost: ${result.total_cost:.6f}")
    print(f"âš¡ Latency: {result.latency_ms}ms")
    
    # 5. Key takeaways
    print("\n" + "=" * 80)
    print("KEY TAKEAWAYS")
    print("=" * 80)
    print("- Takeaway 1")
    print("- Takeaway 2")

if __name__ == "__main__":
    asyncio.run(main())
```

### Contribution Process

1. **Create example**: `examples/your_example.py` (~200-500 lines)
2. **Write guide**: `docs/guides/your-example.md` (~500-800 lines)
3. **Update README**: Add section to this file
4. **Add tests**: `tests/test_your_example.py`
5. **Submit PR**: Include example + guide + tests

See [CONTRIBUTING.md](../CONTRIBUTING.md) for detailed guidelines.

---

## ğŸ“ Need Help?

### Documentation
- ğŸ“– **[Quick Start Guide](../docs/guides/quickstart.md)** - Core concepts
- ğŸŒŠ **[Streaming Guide](../docs/guides/streaming.md)** - Streaming details
- ğŸ› ï¸ **[Tools Guide](../docs/guides/tools.md)** - Tool execution
- ğŸ’° **[Cost Tracking Guide](../docs/guides/cost_tracking.md)** - Cost management
- ğŸŒ **[Providers Guide](../docs/guides/providers.md)** - Multi-provider setup
- ğŸš€ **[FastAPI Guide](../docs/guides/fastapi.md)** - API integration
- ğŸ¯ **[Custom Cascade Guide](../docs/guides/custom_cascade.md)** - Custom routing
- ğŸ” **[Custom Validation Guide](../docs/guides/custom_validation.md)** - Quality control
- ğŸ­ **[Production Guide](../docs/guides/production.md)** - Deployment
- ğŸ”Œ **[Edge Device Guide](../docs/guides/edge_device.md)** - Jetson/Pi deployment **NEW**

### Community
- ğŸ’¬ **[GitHub Discussions](https://github.com/lemony-ai/CascadeFlow/discussions)** - Ask questions
- ğŸ› **[GitHub Issues](https://github.com/lemony-ai/CascadeFlow/issues)** - Report bugs
- ğŸ’¡ **Questions**: Open an issue with the "question" label

---

## ğŸ¯ Summary

### âœ… Available Examples (11 total)

**Core:**
1. Basic Usage - Cost savings fundamentals
2. Streaming Text - Real-time responses
3. Streaming Tools - Tool call streaming
4. Tool Execution - Complete tool workflow
5. Cost Tracking - Budget management

**Advanced:**
6. Multi-Provider - Mix any providers
7. FastAPI Integration - REST API deployment
8. Custom Cascade - Domain-specific routing
9. Custom Validation - Quality control
10. Production Patterns - Enterprise deployment
11. Edge Device Deployment - Jetson/Pi with local inference **NEW**

### ğŸ“š Complete Documentation

- âœ… **11 examples** (~4,180 lines of code)
- âœ… **10 comprehensive guides** (~10,280 lines of docs)
- âœ… **~14,460 lines total** of professional documentation
- âœ… **100% feature coverage** (core + advanced + production + edge deployment)

### ğŸ”‘ Key Learnings

**Essential Concepts:**
- âœ… Token-based pricing (not flat rates)
- âœ… Streaming requires 2+ models (cascade enabled)
- âœ… Use universal tool format (not OpenAI format)
- âœ… Streaming â‰  Execution (two separate concepts)
- âœ… Check `agent.text_streaming_manager` availability
- âœ… Draft accepted = money saved, draft rejected = quality ensured
- âœ… Extract costs from result.metadata with safe fallbacks
- âœ… Use CostTracker for budget management (warnings, not hard stops)

**Production Ready:**
- âœ… Error handling and retry logic
- âœ… Rate limiting and circuit breakers
- âœ… Caching and monitoring
- âœ… Budget management with CostTracker
- âœ… Cost tracking with telemetry system
- âœ… API deployment (FastAPI + SSE)
- âœ… Multi-provider support

### ğŸš€ Quick Links

**Start here:** [`basic_usage.py`](basic_usage.py)
**Learn streaming:** [`streaming_text.py`](streaming_text.py)
**Master tools:** [`tool_execution.py`](tool_execution.py)
**Track costs:** [`cost_tracking.py`](cost_tracking.py)
**Deploy API:** [`fastapi_integration.py`](fastapi_integration.py)
**Edge devices:** [`edge_device.py`](edge_device.py) **NEW**
**Go production:** [`production_patterns.py`](production_patterns.py)

**Full documentation:** [`docs/guides/quickstart.md`](../docs/guides/quickstart.md)

---

**ğŸ’° Save 40-85% on AI costs with intelligent cascading!** ğŸš€

**ğŸ‰ Everything is production-ready and ready for GitHub launch!**