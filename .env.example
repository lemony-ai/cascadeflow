# ============================================
# CascadeFlow Provider API Keys & Configuration
# ============================================
# Copy this file to .env and fill in your API keys
# Local providers (Ollama, vLLM) can be used without API keys

# ============================================
# CLOUD PROVIDERS (Require API Keys)
# ============================================

# --------------------------------------------
# OpenAI - Industry-leading quality, most reliable, best for production
# --------------------------------------------
OPENAI_API_KEY=your_openai_api_key_here
# Get your key: https://platform.openai.com/api-keys
# Models: gpt-4, gpt-4-turbo, gpt-4o, gpt-4o-mini, gpt-3.5-turbo

# --------------------------------------------
# Anthropic Claude - Best for reasoning and analysis, strong safety features
# --------------------------------------------
ANTHROPIC_API_KEY=your_anthropic_api_key_here
# Get your key: https://console.anthropic.com/settings/keys
# Models: claude-3-opus, claude-3-5-sonnet, claude-3-sonnet, claude-3-haiku

# --------------------------------------------
# Groq - Fastest inference speed, ultra-low latency, free tier available
# --------------------------------------------
GROQ_API_KEY=your_groq_api_key_here
# Get your key: https://console.groq.com/keys
# Models: llama3-70b, llama3-8b, mixtral-8x7b, gemma-7b

# --------------------------------------------
# Together AI - Cost-effective, wide model selection, good for experimentation
# --------------------------------------------
TOGETHER_API_KEY=your_together_api_key_here
# Get your key: https://api.together.xyz/settings/api-keys
# Models: Llama-2, Mixtral, Code Llama, and many more

# --------------------------------------------
# Hugging Face - Open-source models, community-driven, flexible deployment
# --------------------------------------------
HF_TOKEN=your_huggingface_token_here
# Get your key: https://huggingface.co/settings/tokens
# Models: Mistral, Falcon, BLOOM, and thousands more

# Optional: If using Hugging Face Inference Endpoints
HF_INFERENCE_ENDPOINT_URL=https://your-endpoint.endpoints.huggingface.cloud
# Leave empty if using standard Hugging Face API

# --------------------------------------------
# Google (Vertex AI) - Enterprise integration, GCP ecosystem, Gemini models
# --------------------------------------------
GOOGLE_API_KEY=your_google_api_key_here
# Get your key: https://makersuite.google.com/app/apikey
# Models: gemini-pro, gemini-pro-vision, text-bison, chat-bison

# Optional: For Vertex AI (instead of AI Studio)
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
# GOOGLE_PROJECT_ID=your-gcp-project-id
# GOOGLE_REGION=us-central1

# --------------------------------------------
# Azure OpenAI - Enterprise compliance, HIPAA/SOC2, Microsoft ecosystem
# --------------------------------------------
AZURE_API_KEY=your_azure_api_key_here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT_NAME=your-deployment-name
AZURE_OPENAI_API_VERSION=2024-02-15-preview
# Get your key: https://portal.azure.com/ → Azure OpenAI → Keys and Endpoint
# Models: Same as OpenAI (gpt-4, gpt-3.5-turbo, etc.) but via Azure

# --------------------------------------------
# DeepSeek - Specialized code models, very cost-effective for coding tasks
# --------------------------------------------
DEEPSEEK_API_KEY=your_deepseek_api_key_here
# Get your key: https://platform.deepseek.com/api_keys
# Models: deepseek-coder, deepseek-chat

# ============================================
# LOCAL / SELF-HOSTED PROVIDERS (No API Keys Required)
# ============================================

# --------------------------------------------
# Ollama - Privacy-first, local deployment, no internet required
# --------------------------------------------
# Ollama can run on localhost or any network-accessible host

# For local Ollama (default)
OLLAMA_BASE_URL=http://localhost:11434
# Models: llama2, mistral, codellama, phi, gemma, etc.

# For Ollama on another machine in your network
# OLLAMA_BASE_URL=http://192.168.1.100:11434

# For Ollama on a remote server
# OLLAMA_BASE_URL=https://ollama.yourdomain.com
# If using authentication:
# OLLAMA_API_KEY=your_custom_auth_token

# How to install Ollama:
# 1. Download: https://ollama.ai/download
# 2. Install and run: ollama serve
# 3. Pull a model: ollama pull llama2
# 4. Test: curl http://localhost:11434/api/generate -d '{"model":"llama2","prompt":"Hello"}'

# --------------------------------------------
# vLLM - Self-hosted inference server, full control, optimized performance
# --------------------------------------------
# vLLM can run on localhost, your network, or a remote server

# For local vLLM (default)
VLLM_BASE_URL=http://localhost:8000
# Models: Any model supported by vLLM (Llama, Mistral, Mixtral, etc.)

# For vLLM on another machine in your network
# VLLM_BASE_URL=http://192.168.1.200:8000

# For vLLM on a remote server
# VLLM_BASE_URL=https://vllm.yourdomain.com
# If using authentication:
# VLLM_API_KEY=your_custom_auth_token

# Optional: vLLM model configuration
VLLM_MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.2
# Specify which model your vLLM server is serving

# How to install vLLM:
# 1. Install: pip install vllm
# 2. Start server: python -m vllm.entrypoints.openai.api_server \
#    --model mistralai/Mistral-7B-Instruct-v0.2 \
#    --port 8000
# 3. Test: curl http://localhost:8000/v1/models

# ============================================
# CONFIGURATION EXAMPLES
# ============================================

# Example 1: Local-only setup (no cloud providers)
# - Set OLLAMA_BASE_URL and VLLM_BASE_URL
# - Leave all API keys empty
# - Great for privacy-sensitive work

# Example 2: Hybrid setup (cloud + local)
# - Configure OpenAI/Anthropic for production quality
# - Configure Ollama for development and testing
# - Use cloud for critical tasks, local for everything else

# Example 3: Multi-cloud setup (all providers)
# - Configure all cloud providers for maximum flexibility
# - Use CascadeFlow's automatic fallback system
# - Let CascadeFlow pick the best provider for each task

# Example 4: Enterprise setup (Azure + local)
# - Configure Azure OpenAI for compliance
# - Configure vLLM for high-volume, low-cost inference
# - Keep sensitive data on-premises

# ============================================
# PROVIDER SELECTION GUIDE
# ============================================

# For production quality:     OpenAI, Anthropic
# For speed:                   Groq, vLLM
# For cost optimization:       Together, DeepSeek, Ollama
# For privacy/compliance:      Ollama, vLLM, Azure
# For experimentation:         HuggingFace, Together
# For code tasks:              DeepSeek, OpenAI (GPT-4)
# For reasoning:               Anthropic (Claude), OpenAI (GPT-4)
# For enterprise:              Azure, Google (Vertex AI)

# ============================================
# TESTING YOUR SETUP
# ============================================

# After configuring, test with:
# python examples/integrations/test_all_providers.py

# This will:
# - Check which providers are configured
# - Test cost calculations for each provider
# - Identify any missing API keys or configuration issues
# - Generate a status report

# ============================================
# SECURITY NOTES
# ============================================

# 1. Never commit .env file to version control
# 2. Keep API keys secure and rotate them regularly
# 3. Use environment-specific .env files (.env.dev, .env.prod)
# 4. For production, use a secrets manager (AWS Secrets Manager, HashiCorp Vault, etc.)
# 5. Limit API key permissions to only what's needed
# 6. Monitor API usage and set up billing alerts

# ============================================
# TROUBLESHOOTING
# ============================================

# Issue: "API key not found"
# Solution: Make sure .env file is in the project root and keys are set

# Issue: "Connection refused" for Ollama/vLLM
# Solution: Check if the service is running and BASE_URL is correct

# Issue: "Model not found"
# Solution: For Ollama, pull the model first: ollama pull <model>
#           For vLLM, make sure the model is loaded on the server

# Issue: "Rate limit exceeded"
# Solution: Use CascadeFlow's budget tracking to manage API usage
#           Or configure multiple providers for automatic fallback

# ============================================
# ADDITIONAL RESOURCES
# ============================================

# CascadeFlow Documentation: https://github.com/yourusername/cascadeflow
# OpenAI API: https://platform.openai.com/docs
# Anthropic API: https://docs.anthropic.com/
# Groq API: https://console.groq.com/docs
# Together API: https://docs.together.ai/
# Hugging Face: https://huggingface.co/docs
# Google AI: https://ai.google.dev/docs
# Azure OpenAI: https://learn.microsoft.com/azure/ai-services/openai/
# DeepSeek: https://platform.deepseek.com/docs
# Ollama: https://ollama.ai/docs
# vLLM: https://docs.vllm.ai/

# ============================================
# OPTIONAL CASCADEFLOW SETTINGS
# ============================================

# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Default quality threshold for responses (0.0 - 1.0)
CASCADEFLOW_DEFAULT_QUALITY_THRESHOLD=0.7
